[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "COMP/STAT112 Notebook",
    "section": "",
    "text": "Welcome\nWelcome to my online portfolio for COMP/STAT112 course taken at Macalester College. Please, use the side bar on the left for navigation.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "bw/bw-uni.html",
    "href": "bw/bw-uni.html",
    "title": "\n1  Univariate Viz\n",
    "section": "",
    "text": "This section showcases my best univariate visualizations, refined throughout the semester as I learned principles of effective design. The goal was to clearly communicate the distribution of a single variable using clean and professional plots.\nThe data comes Student Survey, available at hash-mac.github.io/stat112site-s25/data/survey.csv, which captures Macalester students’ outdoor preferences.\n\nCodesurvey %&gt;%\n  count(hangout) %&gt;%\n  ggplot(aes(x = fct_reorder(hangout, n), y = n)) +\n  geom_bar(stat = \"identity\", fill = \"seagreen\") +\n  geom_text(aes(label = n), hjust = 2, vjust = -0.5) +\n  labs(x = \"Location\", y = \"Number of Students\",\n       title = \"Where Students Prefer to Hang Out Outdoors\") +\n  coord_flip() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nCodeggplot(survey, aes(x = fav_temp_c)) +\n  geom_histogram(binwidth = 2, fill = \"coral\", color = \"white\") +\n  geom_vline(aes(xintercept = mean(fav_temp_c, na.rm = TRUE)), \n             linetype = \"dashed\", color = \"steelblue\", linewidth = 1) +\n  annotate(\"text\", \n           x = mean(survey$fav_temp_c, na.rm = TRUE) + 1.5, \n           y = 8, \n           label = \"Mean\", \n           color = \"steelblue\", \n           angle = 90, \n           vjust = -0.5, \n           size = 4) +\n  labs(\n    title = \"Distribution of Preferred Outdoor Temperature\",\n    x = \"Preferred Temperature (°C)\",\n    y = \"Count\"\n  ) +\n  coord_cartesian(xlim = c(-10, 40)) +\n  theme_minimal(base_size = 14)\n\n\n\n\n\n\n\n\nCodeggplot(survey, aes(x = fav_temp_c)) +\n  geom_density(fill = \"coral\", alpha = 0.5) +\n  geom_vline(aes(xintercept = mean(fav_temp_c, na.rm = TRUE)),\n             linetype = \"dashed\", color = \"steelblue\", linewidth = 1) +\n  annotate(\"text\",\n           x = mean(survey$fav_temp_c, na.rm = TRUE) + 0.5,\n           y = 0.055,\n           label = \"Mean\",\n           color = \"steelblue\",\n           angle = 90,\n           vjust = -0.5,\n           size = 4) +\n  geom_vline(aes(xintercept = median(fav_temp_c, na.rm = TRUE)),\n             linetype = \"dotted\", color = \"darkgreen\", linewidth = 1) +\n  annotate(\"text\",\n           x = median(survey$fav_temp_c, na.rm = TRUE) + 3,\n           y = 0.055,\n           label = \"Median\",\n           color = \"darkgreen\",\n           angle = 90,\n           vjust = -0.5,\n           size = 4) +\n  labs(\n    title = \"Density of Student Temperature Preferences\",\n    x = \"Preferred Temperature (°C)\",\n    y = \"Density\"\n  ) +\n  theme_minimal(base_size = 14)",
    "crumbs": [
      "Best Work",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Univariate Viz</span>"
    ]
  },
  {
    "objectID": "bw/bw-bi.html",
    "href": "bw/bw-bi.html",
    "title": "\n2  Bivariate Viz\n",
    "section": "",
    "text": "This section showcases my best bivariate visualization, developed and improved throughout the semester as I learned principles of effective data visualization and design. The goal was to clearly communicate the relationship between two variables using clean, informative, and professional plots.\nThe data comes from two sources: the 2020 U.S. county-level election results (election_2020_county.csv) and weather data from three locations (weather_3_locations.csv), both provided through the course website at mac-stat.github.io. These datasets allow for exploration of how weather patterns may relate to regional political preferences or other contextual variables.\n\nCodeggplot(elections, aes(x = historical, fill = winner_20)) +\n  geom_bar(position = \"fill\") +\n  scale_fill_manual(\n    name = \"2020 Winner\",\n    values = c(\"dem\" = \"#5DADE2\", \"repub\" = \"#F1948A\"),\n    labels = c(\"Democrat\", \"Republican\")\n  ) +\n  labs(\n    title = \"2020 Election Outcomes by Historical State Voting Patterns\",\n    x = \"Historical Voting Trend\",\n    y = \"Proportion of Counties\",\n    caption = \"Source: ICA\"\n  ) +\n  theme_minimal(base_size = 14)\n\n\n\n\n\n\n\n\nCodeggplot(weather, aes(x = temp3pm, fill = location)) +\n  geom_density(alpha = 0.5) +\n  scale_fill_brewer(palette = \"Set2\") +\n  labs(\n    title = \"Distribution of 3PM Temperatures by Location\",\n    x = \"Temperature at 3PM (°C)\",\n    y = \"Density\",\n    fill = \"Location\",\n    caption = \"Source: ICA\"\n  ) +\n  theme_minimal(base_size = 14) +\n  theme(legend.position = \"top\")",
    "crumbs": [
      "Best Work",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Bivariate Viz</span>"
    ]
  },
  {
    "objectID": "bw/bw-tri.html",
    "href": "bw/bw-tri.html",
    "title": "\n3  Trivariate Viz\n",
    "section": "",
    "text": "This section showcases my best trivariate visualization, created and refined over the semester as I developed a deeper understanding of effective visual communication. The goal was to clearly represent the relationship among three variables within a single plot using thoughtful encoding and design.\nThe data for the first plot comes from the SAT dataset (sat.csv), available at mac-stat.github.io. It includes information on average SAT scores, participation rates, and education-related funding across U.S. states, making it ideal for exploring how educational outcomes relate to policy and demographic factors.\nThe data for the second plot comes from Yahoo Finance and includes daily stock prices for Netflix and 13 affiliated companies from 2018 to 2022. After importing and cleaning the data, I normalized the adjusted closing prices to enable meaningful comparisons across firms with different stock price scales. In the final plot, Date is mapped to the x-axis, Normalized Price to the y-axis, and symbol (company name) is encoded by color.\n\nCodeggplot(education, aes(x = salary, y = expend, color = sat)) +\n  geom_point(size = 2.5) +\n  scale_color_gradient(low = \"lightblue\", high = \"darkblue\") +\n  labs(\n    title = \"Teacher Salary and School Expenditure by State\",\n    subtitle = \"Point color represents average SAT score\",\n    x = \"Estimated average annual teacher salary\",\n    y = \"Expenditure per pupil\",\n    color = \"SAT Score\",\n    caption = \"Source: ICA\"\n  ) +\n  theme_minimal(base_size = 14)\n\n\n\n\n\n\n\n\nCodeggplot(all_stocks, aes(x = Date, y = normalized, color = symbol)) +\n  geom_line(size = 0.4) +\n  labs(title = \"Normalized Stock Price Trends\",\n       subtitle = \"Netflix & Affiliated Companies from 2018 to 2022\",\n       caption = \"Source: Yahoo Finance\",\n       y = \"Normalized Price\",\n       x = \"Date\") +\n  scale_color_viridis_d(option = \"turbo\") +\n  theme_minimal()",
    "crumbs": [
      "Best Work",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Trivariate Viz</span>"
    ]
  },
  {
    "objectID": "bw/bw-quad.html",
    "href": "bw/bw-quad.html",
    "title": "\n4  Quadvariate Viz\n",
    "section": "",
    "text": "Use this file to generate a professional looking quadvariate visualization. The visualization will not perfect the first time but you are expected to improve on it throughout the semester especially after covering advanced topics such as effective viz.\n\nCodeggplot(weather, aes(y = temp3pm, x = temp9am, color = location, shape = raintoday)) + \n  geom_point()+\n  labs(\n    caption = \"Source: ICA\",\n    title = \"Relationship between temperatures at 9AM and at 3Pm for weather data in Australia\",\n    x = \"Temperature at 9AM\",\n    y = \"Temperature at 3PM\"\n  ) +\n  theme_minimal()\n\nWarning: Removed 69 rows containing missing values or values outside the scale range\n(`geom_point()`).",
    "crumbs": [
      "Best Work",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Quadvariate Viz</span>"
    ]
  },
  {
    "objectID": "bw/bw-spatial.html",
    "href": "bw/bw-spatial.html",
    "title": "\n5  Spatial Viz\n",
    "section": "",
    "text": "Use this file to generate a professional looking spatial visualization. The visualization will not perfect the first time but you are expected to improve on it throughout the semester especially after covering advanced topics such as effective viz.\n\nCodeleaflet(data = fave_places) |&gt;\n  addProviderTiles(\"USGS\") |&gt;\n  addCircles(weight = 10, opacity = 1, color = col2hex(\"yellow\")) |&gt;\n  addPolylines(\n    lng = ~longitude,\n    lat = ~latitude,\n    color = col2hex(\"green\")\n  )",
    "crumbs": [
      "Best Work",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Spatial Viz</span>"
    ]
  },
  {
    "objectID": "bw/Solo_project.html",
    "href": "bw/Solo_project.html",
    "title": "\n6  Mapping the Museums of the United Kingdom\n",
    "section": "",
    "text": "6.1 Intro\nMuseums are cultural landmarks, preserving the diverse history and heritage of the United Kingdom. This project explores the spatial distribution, types, and key characteristics of museums across England, Scotland, Wales, and Northern Ireland. By leveraging open data from TidyTuesday, I visualize the UK’s museum landscape and examine how different governance models, subject matters, and locations shape the museum sector. Through this analysis, I hope to contribute to a better understanding of the cultural infrastructure of the UK and highlight the important role museums play in connecting communities with their past.",
    "crumbs": [
      "Best Work",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Mapping the Museums of the United Kingdom</span>"
    ]
  },
  {
    "objectID": "bw/Solo_project.html#data-acquisition-and-preparation",
    "href": "bw/Solo_project.html#data-acquisition-and-preparation",
    "title": "\n6  Mapping the Museums of the United Kingdom\n",
    "section": "\n6.2 Data Acquisition and Preparation",
    "text": "6.2 Data Acquisition and Preparation\nFor this analysis, I’m using data from the TidyTuesday project in 2022 (https://github.com/rfordatascience/tidytuesday/tree/main/data/2022/2022-11-22), which is sourced from Museums UK, providing details on thousands of museums, their locations, categories, and governance.\n\n# Load required packages\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(sf)\n\nLinking to GEOS 3.11.0, GDAL 3.5.3, PROJ 9.1.0; sf_use_s2() is TRUE\n\nlibrary(ggplot2)\nlibrary(maps)\n\n\nAttaching package: 'maps'\n\nThe following object is masked from 'package:purrr':\n\n    map\n\nlibrary(rnaturalearth)\nlibrary(RColorBrewer)\nlibrary(mosaic)\n\nRegistered S3 method overwritten by 'mosaic':\n  method                           from   \n  fortify.SpatialPolygonsDataFrame ggplot2\n\nThe 'mosaic' package masks several functions from core packages in order to add \nadditional features.  The original behavior of these functions should not be affected by this.\n\nAttaching package: 'mosaic'\n\nThe following object is masked from 'package:Matrix':\n\n    mean\n\nThe following objects are masked from 'package:dplyr':\n\n    count, do, tally\n\nThe following object is masked from 'package:purrr':\n\n    cross\n\nThe following object is masked from 'package:ggplot2':\n\n    stat\n\nThe following objects are masked from 'package:stats':\n\n    binom.test, cor, cor.test, cov, fivenum, IQR, median, prop.test,\n    quantile, sd, t.test, var\n\nThe following objects are masked from 'package:base':\n\n    max, mean, min, prod, range, sample, sum\n\n\n\n# Import data\nmuseums &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2022/2022-11-22/museums.csv\") \n\nRows: 4191 Columns: 35\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (24): museum_id, Name_of_museum, Address_line_1, Address_line_2, Village...\ndbl (11): Latitude, Longitude, DOMUS_identifier, Area_Deprivation_index, Are...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nLet’s take a quick look at the data, it includes 35 variables – we may want to clean the data to keep the tidiness.\n\n# Inspect data\nhead(museums)\n\n# A tibble: 6 × 35\n  museum_id  Name_of_museum Address_line_1 Address_line_2 Village,_Town_or_Cit…¹\n  &lt;chr&gt;      &lt;chr&gt;          &lt;chr&gt;          &lt;chr&gt;          &lt;chr&gt;                 \n1 mm.New.1   Titanic Belfa… 1 Olympic Way  &lt;NA&gt;           Belfast               \n2 mm.aim.12… The Woodland … &lt;NA&gt;           Brokerswood    nr Westbury           \n3 mm.domus.… Warwickshire … Warwick Colle… Horticulture … Moreton Morrell       \n4 mm.aim.04… Battle Of Flo… La Robeline    Mont des Corv… St Ouen               \n5 mm.aim.04… Jet Age Museum 57 Crown Drive Bishops Cleeve Cheltenham            \n6 mm.aim.04… Jewish Milita… c/o Jewish Mu… Raymond Burto… London                \n# ℹ abbreviated name: ¹​`Village,_Town_or_City`\n# ℹ 30 more variables: Postcode &lt;chr&gt;, Latitude &lt;dbl&gt;, Longitude &lt;dbl&gt;,\n#   Admin_area &lt;chr&gt;, Accreditation &lt;chr&gt;, Governance &lt;chr&gt;, Size &lt;chr&gt;,\n#   Size_provenance &lt;chr&gt;, Subject_Matter &lt;chr&gt;, Year_opened &lt;chr&gt;,\n#   Year_closed &lt;chr&gt;, DOMUS_Subject_Matter &lt;chr&gt;, DOMUS_identifier &lt;dbl&gt;,\n#   Primary_provenance_of_data &lt;chr&gt;,\n#   Identifier_used_in_primary_data_source &lt;chr&gt;, …\n\n\n\n# data cleaning\nmuseums_clean &lt;- museums %&gt;% \n  select(Name_of_museum,Latitude,Longitude,Admin_area, Accreditation,Governance,Size,Subject_Matter,Year_closed) %&gt;% # keep only useful variables\n  filter(Year_closed == \"9999:9999\") %&gt;% # only look at the open museums\n  mutate(country = str_trim(str_split(Admin_area, \"/\", simplify = TRUE)[,2])) %&gt;% \n  mutate(subject = str_trim(str_split(Subject_Matter, \"-\", simplify = TRUE)[,1])) %&gt;% \n  mutate(gov_cat = str_trim(str_split(Governance, \"-\", simplify = TRUE)[,1])) %&gt;% \n  select(-Admin_area,-Subject_Matter,-Year_closed) # delete the variables that we've already organized\n\n\n# prpare the map background\nuk_countries &lt;- ne_states(country = \"United Kingdom\", returnclass = \"sf\")",
    "crumbs": [
      "Best Work",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Mapping the Museums of the United Kingdom</span>"
    ]
  },
  {
    "objectID": "bw/Solo_project.html#analysis",
    "href": "bw/Solo_project.html#analysis",
    "title": "\n6  Mapping the Museums of the United Kingdom\n",
    "section": "\n6.3 Analysis",
    "text": "6.3 Analysis\n\nggplot(museums_clean, aes(x = country, fill = Size)) +\n  geom_bar()  +\n  labs(x = \"UK Country\", y = \"Count\") +\n  theme_minimal()\n\n\n\n\n\n\nggplot(museums_clean , aes(x = Size, fill = country)) +\n  geom_bar(position = \"fill\") +\n  labs(x = \"Museum Size\", y = \"Count\") +\n  theme_minimal()\n\n\n\n\n\n\n\nFrom the 2 plots above, we can observe that most UK museums are located in England, which has by far the largest number of museums in every size category. Scotland has the second highest count, followed by Wales and Northern Ireland, with the Channel Islands and Isle of Man having very few. Across all countries, small museums are the most common, while “huge” museums are rare everywhere.\n\nggplot() +\n  geom_sf(data = uk_countries, fill = \"gray95\", color = \"gray70\") +\n  geom_point(\n    data = museums_clean,\n    aes(x = Longitude, y = Latitude, color = Size),\n    size = 1.1, alpha = 0.2\n  ) +\n  coord_sf(xlim = c(-8, 2), ylim = c(49, 61)) +\n  theme_void()\n\n\n\n\n\n\n\nThe map confirms our understanding about distribution of museums of different sizes – the majority of them are small. Now let’s shift to the the distribution of governance category by taking a quick look at the 2nd map.\n\nggplot() +\n  geom_sf(data = uk_countries, fill = \"gray95\", color = \"gray70\") +\n  geom_point(\n    data = museums_clean,\n    aes(x = Longitude, y = Latitude, color = gov_cat), \n    size = 1.1, alpha = 0.5\n  ) +\n  coord_sf(xlim = c(-8, 2), ylim = c(49, 61)) +\n  theme_void()\n\n\n\n\n\n\n\nWe can see that the independent governance dominates the UK museum landscape, with government museums making up much smaller shares. Museums are most densely clustered in southern England but are spread across the country, including remote regions. In the following steps we would analyze the museum’s governance composition combining with the subgroups with bar plot.\n\nggplot(museums_clean, aes(x = gov_cat, fill = Governance)) +\n  geom_bar() +\n  theme_minimal()\n\n\n\n\n\n\ngov_counts &lt;- museums_clean %&gt;%\n  count(Governance) %&gt;%\n  arrange(n) %&gt;%\n  mutate(Governance = factor(Governance, levels = Governance)) \n\n\nggplot(gov_counts, aes(x = Governance, y = n)) +\n  geom_col(fill = \"steelblue\") +\n  geom_text(aes(label = n), vjust = -0.3, size = 3) + \n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) + \n  labs(\n    x = \"Governance Type\",\n    y = \"Number of Museums\",\n    title = \"Number of Museums by Governance Type\"\n  )\n\n\n\n\n\n\n\nThe previous geographic map shows that museums are densely clustered in southern England, and independent museums dominate the landscape, particularly in England. This pattern is clearly reflected in both bar charts, too.\nMeanwhile, both bar charts show that “Independent-Not_for_profit” and “Independent-Private” are by far the most common categories, together making up the majority of museums. Local authority-run museums are the main form of government governance, but they are much less numerous than independent ones.\nNow, let’s focus on the most common governance types and the size composition within each type.\n\nggplot(museums_clean %&gt;% filter(Governance %in% c(\"Independent-National_Trust\", \"Independent-Private\",\"Government-Local_Authority\",\"Independent-Not_for_profit\")), aes(x = Governance, fill = Size)) +\n  geom_bar() +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) + \n  labs(\n    x = \"Governance Type\",\n    y = \"Number of Museums\",\n    title = \"Number of Museums by Governance Type\"\n  )\n\n\n\n\n\n\n\nOverall, small and medium-sized museums dominate each governance type, but government-managed museums are more likely to be medium or large compared to independents. This pattern suggests that independent museums are more numerous but generally smaller, while government-run museums, though fewer, tend to be larger institutions.\nThis distribution suggests the UK government prioritizes supporting fewer, larger museums with broad public impact. Meanwhile, the independent and nonprofit sector are encouraged to play a role in operating the many smaller, local museums, reflecting strong grassroots involvement and cultural diversity.",
    "crumbs": [
      "Best Work",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Mapping the Museums of the United Kingdom</span>"
    ]
  },
  {
    "objectID": "bw/Solo_project.html#conclusion",
    "href": "bw/Solo_project.html#conclusion",
    "title": "\n6  Mapping the Museums of the United Kingdom\n",
    "section": "\n6.4 Conclusion",
    "text": "6.4 Conclusion\nMy analysis highlights the rich and varied landscape of museums across the United Kingdom. The overwhelming presence of independent museums—especially small, not-for-profit, and private institutions—underscores the vital role of local communities and grassroots organizations in preserving and sharing culture. While England dominates in sheer number and variety, the contributions from Scotland, Wales, and Northern Ireland further illustrate the variety of regional heritage.\nGovernment involvement, while less extensive in number, is clearly focused on supporting larger institutions with wider public reach. This targeted approach allows the government to maintain and promote key national assets, while also enabling independent organizations to flourish at the local level – it has achieved its balance.\nOverall, museums remain crucial to connecting people with history, art, and science. The patterns revealed here point to a healthy ecosystem where both state and independent actors contribute tothe cultural heritage for future generations.",
    "crumbs": [
      "Best Work",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Mapping the Museums of the United Kingdom</span>"
    ]
  },
  {
    "objectID": "bw/Solo_project.html#reference",
    "href": "bw/Solo_project.html#reference",
    "title": "\n6  Mapping the Museums of the United Kingdom\n",
    "section": "\n6.5 Reference",
    "text": "6.5 Reference\nTidyTuesday. (2022). Museums Dataset. Retrieved from https://github.com/rfordatascience/tidytuesday/tree/main/data/2022/2022-11-22",
    "crumbs": [
      "Best Work",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Mapping the Museums of the United Kingdom</span>"
    ]
  },
  {
    "objectID": "bw/Exam_1.html",
    "href": "bw/Exam_1.html",
    "title": "\n7  Exam_1\n",
    "section": "",
    "text": "7.0.1 Data\nfood_consumption.csv",
    "crumbs": [
      "Best Work",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Exam_1</span>"
    ]
  },
  {
    "objectID": "bw/Exam_1.html#understand-data",
    "href": "bw/Exam_1.html#understand-data",
    "title": "\n7  Exam_1\n",
    "section": "\n7.1 Understand Data",
    "text": "7.1 Understand Data\n\n# check the types of variables\nglimpse(fc)\n\nRows: 1,430\nColumns: 4\n$ country       &lt;chr&gt; \"Argentina\", \"Argentina\", \"Argentina\", \"Argentina\", \"Arg…\n$ food_category &lt;chr&gt; \"Pork\", \"Poultry\", \"Beef\", \"Lamb & Goat\", \"Fish\", \"Eggs\"…\n$ consumption   &lt;dbl&gt; 10.51, 38.66, 55.48, 1.56, 4.36, 11.39, 195.08, 103.11, …\n$ co2_emmission &lt;dbl&gt; 37.20, 41.53, 1712.00, 54.63, 6.96, 10.46, 277.87, 19.66…\n\n# check the dimensions of the data\ndim(fc)\n\n[1] 1430    4\n\n# check the first 6 rows\nhead(fc)\n\n# A tibble: 6 × 4\n  country   food_category consumption co2_emmission\n  &lt;chr&gt;     &lt;chr&gt;               &lt;dbl&gt;         &lt;dbl&gt;\n1 Argentina Pork                10.5          37.2 \n2 Argentina Poultry             38.7          41.5 \n3 Argentina Beef                55.5        1712   \n4 Argentina Lamb & Goat          1.56         54.6 \n5 Argentina Fish                 4.36          6.96\n6 Argentina Eggs                11.4          10.5 \n\n# check the statistical summary of each variable\nsummary(fc)\n\n   country          food_category       consumption      co2_emmission    \n Length:1430        Length:1430        Min.   :  0.000   Min.   :   0.00  \n Class :character   Class :character   1st Qu.:  2.365   1st Qu.:   5.21  \n Mode  :character   Mode  :character   Median :  8.890   Median :  16.53  \n                                       Mean   : 28.110   Mean   :  74.38  \n                                       3rd Qu.: 28.133   3rd Qu.:  62.60  \n                                       Max.   :430.760   Max.   :1712.00  \n\n# check the column names\ncolSums(is.na(fc))\n\n      country food_category   consumption co2_emmission \n            0             0             0             0",
    "crumbs": [
      "Best Work",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Exam_1</span>"
    ]
  },
  {
    "objectID": "bw/Exam_2.html",
    "href": "bw/Exam_2.html",
    "title": "\n8  Exam_2\n",
    "section": "",
    "text": "8.0.1 Data\nfood_consumption.csv",
    "crumbs": [
      "Best Work",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Exam_2</span>"
    ]
  },
  {
    "objectID": "bw/Exam_2.html#research-question-which-5-countries-consume-the-most-food",
    "href": "bw/Exam_2.html#research-question-which-5-countries-consume-the-most-food",
    "title": "\n8  Exam_2\n",
    "section": "\n8.1 Research Question: Which 5 countries consume the most food?",
    "text": "8.1 Research Question: Which 5 countries consume the most food?\n\ntop_consumers &lt;- fc %&gt;%\n  group_by(country) %&gt;%\n  summarize(total_consumption = sum(consumption, na.rm = TRUE)) %&gt;%\n  arrange(desc(total_consumption)) %&gt;%\n  slice_head(n = 5)\n\ntop_consumers\n\n# A tibble: 5 × 2\n  country     total_consumption\n  &lt;chr&gt;                   &lt;dbl&gt;\n1 Finland                  640.\n2 Lithuania                555.\n3 Sweden                   550 \n4 Netherlands              534.\n5 Albania                  533.\n\n\nThe table clearly identifies the top five countries with the highest total food consumption (measured in kg/person/year). Finland ranks first with 639.8 kg/person/year, followed by Lithuania, Sweden, the Netherlands, and Albania.\n\nfc_country &lt;- fc %&gt;%\n  group_by(country) %&gt;%\n  summarise(\n    total_consumption = sum(consumption, na.rm = TRUE)\n  ) %&gt;%\n  arrange(desc(total_consumption)) %&gt;%\n  mutate(color_group = ifelse(row_number() &lt;= 5, \"Top 5\", \"Others\")) %&gt;%\n  slice(1:20) \n\n# Plot\nggplot(fc_country, aes(x = reorder(country, total_consumption), y = total_consumption, fill = color_group)) +\n  geom_col() +\n  scale_fill_manual(values = c(\"Top 5\" = \"red\", \"Others\" = \"steelblue\")) +\n  labs(\n    title = \"Top 20 Countries by Total Food Consumption\",\n    x = \"Country\",\n    y = \"Total Consumption (kg/person/year)\",\n    fill = NULL\n  ) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\nThe bar plot visualizes total food consumption across the top 20 countries and highlights the top five in red. Finland prominently leading the chart.",
    "crumbs": [
      "Best Work",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Exam_2</span>"
    ]
  },
  {
    "objectID": "bw/stock_market_fluctuation.html",
    "href": "bw/stock_market_fluctuation.html",
    "title": "\n9  Extra Work: Stock Market Fluctuation\n",
    "section": "",
    "text": "description: This section contains some extra work Phoebe did for fun. source: https://www.kaggle.com/datasets/zongaobian/netflix-stock-data-and-key-affiliated-companies?resource=download https://www.kaggle.com/datasets/kirolosatef/netflex-stock-dataset-with-twitter-sentiment?select=Ntweets.xlsx \n\nCodelibrary(tidymodels)\nlibrary(readr)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(zoo)\nlibrary(scales)\nlibrary(tibble)\nlibrary(stringr)\n\n\n\nCode# data imports\n\nWBD_stock&lt;-read_csv(\"/Users/piipan/Documents/GitHub/comp112/portfolio-PiiPan/data/ds solo project/affiliated companies/WBD_daily_data.csv\")\n\nRows: 4883 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl  (6): Open, High, Low, Close, Adj Close, Volume\ndate (1): Date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nCodeV_stock&lt;-read_csv(\"/Users/piipan/Documents/GitHub/comp112/portfolio-PiiPan/data/ds solo project/affiliated companies/V_daily_data.csv\")\n\nRows: 4205 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl  (6): Open, High, Low, Close, Adj Close, Volume\ndate (1): Date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nCodeUPS_stock&lt;-read_csv(\"/Users/piipan/Documents/GitHub/comp112/portfolio-PiiPan/data/ds solo project/affiliated companies/UPS_daily_data.csv\")\n\nRows: 6304 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl  (6): Open, High, Low, Close, Adj Close, Volume\ndate (1): Date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nCodeSONY_stock&lt;-read_csv(\"/Users/piipan/Documents/GitHub/comp112/portfolio-PiiPan/data/ds solo project/affiliated companies/SONY_daily_data.csv\")\n\nRows: 11085 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl  (6): Open, High, Low, Close, Adj Close, Volume\ndate (1): Date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nCodePARA_stock&lt;-read_csv(\"/Users/piipan/Documents/GitHub/comp112/portfolio-PiiPan/data/ds solo project/affiliated companies/PARA_daily_data.csv\")\n\nRows: 4779 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl  (6): Open, High, Low, Close, Adj Close, Volume\ndate (1): Date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nCodeNVDA_stock&lt;-read_csv(\"/Users/piipan/Documents/GitHub/comp112/portfolio-PiiPan/data/ds solo project/affiliated companies/NVDA_daily_data.csv\")\n\nRows: 6507 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl  (6): Open, High, Low, Close, Adj Close, Volume\ndate (1): Date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nCodeNFLX_stock&lt;-read_csv(\"/Users/piipan/Documents/GitHub/comp112/portfolio-PiiPan/data/ds solo project/affiliated companies/NFLX_daily_data.csv\")\n\nRows: 5670 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl  (6): Open, High, Low, Close, Adj Close, Volume\ndate (1): Date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nCodeMA_stock&lt;-read_csv(\"/Users/piipan/Documents/GitHub/comp112/portfolio-PiiPan/data/ds solo project/affiliated companies/MA_daily_data.csv\")\n\nRows: 4661 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl  (6): Open, High, Low, Close, Adj Close, Volume\ndate (1): Date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nCodeLUMN_stock&lt;-read_csv(\"/Users/piipan/Documents/GitHub/comp112/portfolio-PiiPan/data/ds solo project/affiliated companies/LUMN_daily_data.csv\")\n\nRows: 11085 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl  (6): Open, High, Low, Close, Adj Close, Volume\ndate (1): Date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nCodeINTC_stock&lt;-read_csv(\"/Users/piipan/Documents/GitHub/comp112/portfolio-PiiPan/data/ds solo project/affiliated companies/INTC_daily_data.csv\")\n\nRows: 11085 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl  (6): Open, High, Low, Close, Adj Close, Volume\ndate (1): Date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nCodeFDX_stock&lt;-read_csv(\"/Users/piipan/Documents/GitHub/comp112/portfolio-PiiPan/data/ds solo project/affiliated companies/FDX_daily_data.csv\")\n\nRows: 11085 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl  (6): Open, High, Low, Close, Adj Close, Volume\ndate (1): Date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nCodeCRM_stock&lt;-read_csv(\"/Users/piipan/Documents/GitHub/comp112/portfolio-PiiPan/data/ds solo project/affiliated companies/CRM_daily_data.csv\")\n\nRows: 5146 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl  (6): Open, High, Low, Close, Adj Close, Volume\ndate (1): Date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nCodeAMZN_stock&lt;-read_csv(\"/Users/piipan/Documents/GitHub/comp112/portfolio-PiiPan/data/ds solo project/affiliated companies/AMZN_daily_data.csv\")\n\nRows: 6932 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl  (6): Open, High, Low, Close, Adj Close, Volume\ndate (1): Date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nCodeAKAM_stock&lt;-read_csv(\"/Users/piipan/Documents/GitHub/comp112/portfolio-PiiPan/data/ds solo project/affiliated companies/AKAM_daily_data.csv\")\n\nRows: 6312 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl  (6): Open, High, Low, Close, Adj Close, Volume\ndate (1): Date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nCodedata_twitter_sentiment&lt;-read_csv(\"/Users/piipan/Documents/GitHub/comp112/portfolio-PiiPan/data/ds solo project/twitter sentiments/data_2018-2022.csv\")\n\nRows: 1137 Columns: 10\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl  (9): Open, High, Low, Close, Adj Close, Volume, P_mean, P_sum, twt_count\ndate (1): date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nCodeevent_df &lt;- tibble::tibble(\n  date = as.Date(c(\n    \"2020-11-12\",  # Sony PS5 launch\n    \"2021-05-17\" # WarnerMedia–Discovery merger news\n  )),\n  event = c(\n    \"Sony PS5 global launch (gaming boom)\",\n    \"WarnerMedia–Discovery merger announced\"\n  ),\n  affected_companies = c(\n    \"SONY\", # New Gaming Station launch\n    \"WBD\" #  Corporate structure change\n  )\n)\n\n\n\nCode# specify the time span\ndata_twitter_sentiment %&gt;%\n  summarize(\n    min_date = min(date),\n    max_date = max(date)\n  )\n\n# A tibble: 1 × 2\n  min_date   max_date  \n  &lt;date&gt;     &lt;date&gt;    \n1 2018-01-02 2022-07-08\n\n\n\nCode# data cleaning: adjust time span\nfilter_stock_data &lt;- function(data) {\n    data %&gt;% \n    mutate(Date = as.Date(Date)) %&gt;%\n    filter(Date &gt;= as.Date(\"2018-01-02\") & Date &lt;= as.Date(\"2022-07-08\"))\n}\n\nWBD_stock  &lt;- filter_stock_data(WBD_stock)\nV_stock    &lt;- filter_stock_data(V_stock)\nUPS_stock  &lt;- filter_stock_data(UPS_stock)\nSONY_stock &lt;- filter_stock_data(SONY_stock)\nPARA_stock &lt;- filter_stock_data(PARA_stock)\nNVDA_stock &lt;- filter_stock_data(NVDA_stock)\nNFLX_stock &lt;- filter_stock_data(NFLX_stock)\nMA_stock   &lt;- filter_stock_data(MA_stock)\nLUMN_stock &lt;- filter_stock_data(LUMN_stock)\nINTC_stock &lt;- filter_stock_data(INTC_stock)\nFDX_stock  &lt;- filter_stock_data(FDX_stock)\nCRM_stock  &lt;- filter_stock_data(CRM_stock)\nAMZN_stock &lt;- filter_stock_data(AMZN_stock)\nAKAM_stock &lt;- filter_stock_data(AKAM_stock)\n\n\n\nCode# data cleaning: combine the stock data\nnormalize_and_label &lt;- function(df, ticker) {\n  df %&gt;%\n    mutate(\n      Date = as.Date(Date),\n      normalized = `Adj Close` / first(`Adj Close`),\n      symbol = ticker\n    )\n}\n\nall_stocks &lt;- bind_rows(\n  normalize_and_label(WBD_stock,  \"WBD\"),\n  normalize_and_label(V_stock,    \"V\"),\n  normalize_and_label(UPS_stock,  \"UPS\"),\n  normalize_and_label(SONY_stock, \"SONY\"),\n  normalize_and_label(PARA_stock, \"PARA\"),\n  normalize_and_label(NVDA_stock, \"NVDA\"),\n  normalize_and_label(NFLX_stock, \"NFLX\"),\n  normalize_and_label(MA_stock,   \"MA\"),\n  normalize_and_label(LUMN_stock, \"LUMN\"),\n  normalize_and_label(INTC_stock, \"INTC\"),\n  normalize_and_label(FDX_stock,  \"FDX\"),\n  normalize_and_label(CRM_stock,  \"CRM\"),\n  normalize_and_label(AMZN_stock, \"AMZN\"),\n  normalize_and_label(AKAM_stock, \"AKAM\")\n)\n\n\n\nCode# data description\ncompany_info &lt;- tribble(\n  ~Company_Name,              ~Ticker, ~Sector,\n  \"Warner Bros. Discovery\",   \"WBD\",   \"Media & Entertainment\",\n  \"Visa Inc.\",                \"V\",     \"Financial Services\",\n  \"United Parcel Service\",    \"UPS\",   \"Logistics\",\n  \"Sony Group Corporation\",   \"SONY\",  \"Consumer Electronics & Entertainment\",\n  \"Paramount Global\",         \"PARA\",  \"Media & Entertainment\",\n  \"NVIDIA Corporation\",       \"NVDA\",  \"Semiconductors\",\n  \"Netflix Inc.\",             \"NFLX\",  \"Streaming Media\",\n  \"Mastercard Inc.\",          \"MA\",    \"Financial Services\",\n  \"Lumen Technologies\",       \"LUMN\",  \"Telecommunications\",\n  \"Intel Corporation\",        \"INTC\",  \"Semiconductors\",\n  \"FedEx Corporation\",        \"FDX\",   \"Logistics\",\n  \"Salesforce Inc.\",          \"CRM\",   \"Enterprise Software\",\n  \"Amazon.com Inc.\",          \"AMZN\",  \"E-Commerce & Cloud\",\n  \"Akamai Technologies\",      \"AKAM\",  \"Cloud & Cybersecurity\"\n)\n\ncompany_info\n\n# A tibble: 14 × 3\n   Company_Name           Ticker Sector                              \n   &lt;chr&gt;                  &lt;chr&gt;  &lt;chr&gt;                               \n 1 Warner Bros. Discovery WBD    Media & Entertainment               \n 2 Visa Inc.              V      Financial Services                  \n 3 United Parcel Service  UPS    Logistics                           \n 4 Sony Group Corporation SONY   Consumer Electronics & Entertainment\n 5 Paramount Global       PARA   Media & Entertainment               \n 6 NVIDIA Corporation     NVDA   Semiconductors                      \n 7 Netflix Inc.           NFLX   Streaming Media                     \n 8 Mastercard Inc.        MA     Financial Services                  \n 9 Lumen Technologies     LUMN   Telecommunications                  \n10 Intel Corporation      INTC   Semiconductors                      \n11 FedEx Corporation      FDX    Logistics                           \n12 Salesforce Inc.        CRM    Enterprise Software                 \n13 Amazon.com Inc.        AMZN   E-Commerce & Cloud                  \n14 Akamai Technologies    AKAM   Cloud & Cybersecurity               \n\n\n\nCodeggplot(all_stocks, aes(x = Date, y = normalized, color = symbol)) +\n  geom_line(size = 0.4) +\n  labs(title = \"Normalized Stock Price Trends (2018–2022)\",\n       y = \"Normalized Price\",\n       x = \"Date\") +\n  theme_minimal()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\nCodecalc_returns_volatility &lt;- function(stock, ticker) {\n  stock %&gt;%\n    arrange(Date) %&gt;%\n    mutate(\n      log_return = log(`Adj Close` / lag(`Adj Close`)),\n      vol_30d = rollapply(log_return, 30, sd, fill = NA, align = \"right\"),\n      symbol = ticker\n    )\n}\n\ntech_stock_data &lt;- bind_rows(\n  calc_returns_volatility(NFLX_stock, \"NFLX\"),\n  calc_returns_volatility(AMZN_stock, \"AMZN\"),\n  calc_returns_volatility(INTC_stock, \"INTC\")\n)\n\nggplot(tech_stock_data, aes(x = Date, y = vol_30d, color = symbol)) +\n  geom_line() +\n  labs(title = \"30-Day Rolling Volatility: NFLX vs AMZN vs INTC\",\n       y = \"Volatility\",\n       x = \"Date\") +\n  theme_minimal()\n\nWarning: Removed 90 rows containing missing values or values outside the scale range\n(`geom_line()`).\n\n\n\n\n\n\n\nCodeggplot(tech_stock_data, aes(x = Date, y = log_return, color = symbol,alpha =0.3)) +\n  geom_line() +\n  labs(title = \"log return: NFLX vs AMZN vs INTC\",\n       y = \"log return\",\n       x = \"Date\") +\n  theme_minimal()\n\nWarning: Removed 3 rows containing missing values or values outside the scale range\n(`geom_line()`).\n\n\n\n\n\n\n\n\n\nCodeggplot(data_twitter_sentiment, aes(x = date)) +\n  geom_line(aes(y = rescale(Volume, to = c(0,1),size = 0.2)), color = \"blue\") +\n  geom_line(aes(y = rescale(twt_count, to = c(0,1),size = 0.2)), color = \"pink\") +\n  labs(title = \"Netflix Stock Transaction Volume vs Twitter Counts (Rescaled)\",\n       y = \"Rescaled Value\",\n       x = \"Date\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nCodesony_event &lt;- event_df[1, ]\nsony_data &lt;- SONY_stock %&gt;%\n  mutate(Date = as.Date(Date)) %&gt;%\n  arrange(Date) %&gt;%\n  mutate(\n    log_return = log(`Adj Close` / lag(`Adj Close`)),\n    vol_30d = rollapply(log_return, 30, sd, fill = NA)\n  ) %&gt;%\n  filter(Date &gt;= sony_event$date - 10 & Date &lt;= sony_event$date + 10)\n\n# Close Price\nggplot(sony_data, aes(x = Date, y = Close)) +\n  geom_line(color = \"steelblue\") +\n  geom_vline(xintercept = as.numeric(sony_event$date), linetype = \"dashed\", color = \"red\") +\n  labs(title = \"SONY Close Price around PS5 Launch\", y = \"Close Price\")\n\n\n\n\n\n\nCode# Log Return\nggplot(sony_data, aes(x = Date, y = log_return)) +\n  geom_line(color = \"purple\") +\n  geom_vline(xintercept = as.numeric(sony_event$date), linetype = \"dashed\", color = \"red\") +\n  labs(title = \"SONY Log Return around PS5 Launch\", y = \"Log Return\")\n\n\n\n\n\n\nCode# Volatility\nggplot(sony_data, aes(x = Date, y = vol_30d)) +\n  geom_line(color = \"darkorange\") +\n  geom_vline(xintercept = as.numeric(sony_event$date), linetype = \"dashed\", color = \"red\") +\n  labs(title = \"SONY 30-Day Volatility around PS5 Launch\", y = \"Volatility\")\n\n\n\n\n\n\n\n\nCodewbd_event &lt;- event_df[2, ]\nwbd_data &lt;- WBD_stock %&gt;%\n  mutate(Date = as.Date(Date)) %&gt;%\n  arrange(Date) %&gt;%\n  mutate(\n    log_return = log(`Adj Close` / lag(`Adj Close`)),\n    vol_30d = rollapply(log_return, 30, sd, fill = NA, align = \"right\")\n  ) %&gt;%\n  filter(Date &gt;= wbd_event$date - 10 & Date &lt;= wbd_event$date + 10)\n\n# Close Price\nggplot(wbd_data, aes(x = Date, y = Close)) +\n  geom_line(color = \"steelblue\") +\n  geom_vline(xintercept = as.numeric(wbd_event$date), linetype = \"dashed\", color = \"red\") +\n  labs(title = \"WBD Close Price around Merger Announcement\", y = \"Close Price\")\n\n\n\n\n\n\nCode# Log Return\nggplot(wbd_data, aes(x = Date, y = log_return)) +\n  geom_line(color = \"purple\") +\n  geom_vline(xintercept = as.numeric(wbd_event$date), linetype = \"dashed\", color = \"red\") +\n  labs(title = \"WBD Log Return around Merger Announcement\", y = \"Log Return\")\n\n\n\n\n\n\nCode# Volatility\nggplot(wbd_data, aes(x = Date, y = vol_30d)) +\n  geom_line(color = \"darkorange\") +\n  geom_vline(xintercept = as.numeric(wbd_event$date), linetype = \"dashed\", color = \"red\") +\n  labs(title = \"WBD 30-Day Volatility around Merger Announcement\", y = \"Volatility\")",
    "crumbs": [
      "Best Work",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Extra Work: Stock Market Fluctuation</span>"
    ]
  },
  {
    "objectID": "ica/ica-uni.html#background",
    "href": "ica/ica-uni.html#background",
    "title": "\n12  Univariate Viz\n",
    "section": "\n12.1 Background",
    "text": "12.1 Background\nWe’re starting our unit on data visualization or data viz, thus skipping some steps in the data science workflow. Mainly, it’s tough to understand how our data should be prepared before we have a sense of what we want to do with this data!\n\nSource\n\n12.1.1 Importance of Visualizations\nEXAMPLE 1\nThe data below includes information on hiking trails in the 46 “high peaks” in the Adirondack mountains of Northeastern New York state. This includes data on the hike’s highest elevation (feet), vertical ascent (feet), length (miles), time in hours that it takes to complete, and difficulty rating. Open this data in a viewer, through the Environment tab or by typing View(hikes) in the console.\n\n# Import data\nhikes &lt;- read.csv(\"https://mac-stat.github.io/data/high_peaks.csv\")\n\n\nlibrary(ggplot2)\n\n\n\n\n\n\n\nDiscussion\n\n\n\n\nWhat is the pattern / trend of elevation of hiking trails?\nWhat is the relationship between a hike’s elevation and typical time it takes to summit / reach the top?\n\n\n# first look\nhead(hikes)\n\n# visualization\nggplot(hikes, aes(x = elevation)) + \n  geom_density() +\n  theme_minimal()\n\nggplot(hikes, aes(x = time, y = elevation)) + \n  geom_point() +\n  geom_smooth() +\n  theme_minimal()\n\n\n\nEXAMPLE 2\nLook at the plot below taken from a story reported by this New York Times article (html).\n\n\n\n\n\n\nDiscussion\n\n\n\nSuppose that the article tried telling the story without using data viz, What would that story be like?\n\n\n\nBenefits of Visualization\n\nUnderstand what we’re working with from\n\nscales & typical outcomes, to\noutliers, i.e. unusual cases, to\npatterns & relationships\n\n\nRefine research questions & inform next steps of our analysis.\nCommunicate our findings and tell a story.\n\n12.1.2 Components of Data Graphics\nEXAMPLE 3\nData viz is the process of mapping data to different plot components. For example, in the NYT example above, the research team mapped data like the following (but with many more rows!) to the plot.\n\n\nobservation\ndecade\nyear\ndate\nrelative temp\n\n\n\n1\n2020-30\n2023\n1/23\n1.2\n\n\n2\n1940-60\n1945\n3/45\n-0.05\n\n\n\n\n\n\n\n\n\nDiscussion\n\n\n\nWrite down step-by-step directions for using a data table like the one above to create the temperature visualization. A computer is your audience, thus be as precise as possible, but trust that the computer can find the exact numbers if you tell it where.\n\n\nCOMPONENTS OF GRAPHICS\nIn data viz, we essentially start with a blank canvas and then map data onto it. There are multiple possible mapping components. Some basics from Wickham (which goes into more depth):\n\na frame, or coordinate system\nThe variables or features that define the axes and gridlines of the canvas.\na layer\nThe geometric elements (e.g. lines, points) we add to the canvas to represent either the data points themselves or patterns among the data points. Each type of geometric element is a separate layer. These geometric elements are sometimes called “geoms” or “glyphs” (like heiroglyph!)\nscales\nThe aesthetics we might add to geometric elements (e.g. color, size, shape) to incorporate additional information about data scales or groups.\nfaceting\nThe splitting up of the data into multiple subplots, or facets, to examine different groups within the data.\na theme\nAdditional controls on the “finer points” of the plot aesthetics, (e.g. font type, background, color scheme).\nEXAMPLE\nIn the NYT graphic, the data was mapped to the plot as follows:\n\n\nframe: x-axis = date, y-axis = temp\n\nlayers: add one line per year, add dots for each month in 2023\n\nscales: color each line by decade\n\nfaceting: none\n\na theme: NYT style\n\n12.1.3 ggplot + R packages\nWe will use the powerful ggplot tools in R to build (most of) our viz. The gg here is short for the “grammar of graphics”. These tools are developed in a way that:\n\nrecognizes that code is communication (it has a grammar!)\nconnects code to the components / philosophy of data viz\n\nEXAMPLES: ggplot in the News\n\n\nMPR journalist David Montgomery: R data viz\n\nBBC R data viz\n\nTo use these tools, we must first get them into R/RStudio! Recall that R is open source. Anybody can build R tools and share them through special R packages. The tidyverse package compiles a set of individual packages, including ggplot2, that share a common grammar and structure. Though the learning curve can be steep, this grammar is intuitive and generalizable once mastered. Image source: Posit BBC on X\n\nFollow the directions below to install this package. Unless the authors of a package add updates, you only need to do this once all semester.\n\n\nIf you’re working on Mac’s RStudio servertidyverse is already installed on the server! Check this 2 ways.\n\nType library(tidyverse) in your console. If you don’t get an error, it’s installed!\nCheck that it appears in the list under the “Packages” pane.\n\n\n\nIf you’re working with a desktop version of R/RStudio\nRStudiio –&gt; “Packages” pane –&gt; click “Install” –&gt; type the name of the package (tidyverse) and make sure the “Install dependencies” box is checked –&gt; and click “Install”.",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Univariate Viz</span>"
    ]
  },
  {
    "objectID": "ica/ica-uni.html#exercises",
    "href": "ica/ica-uni.html#exercises",
    "title": "\n12  Univariate Viz\n",
    "section": "\n12.2 Exercises",
    "text": "12.2 Exercises\n\n# Import data\nhikes &lt;- read.csv(\"https://mac-stat.github.io/data/high_peaks.csv\")\n\nExercise 1: Research Questions\nLet’s dig into the hikes data, starting with the elevation and difficulty ratings of the hikes:\n\nhead(hikes)\n\n             peak elevation difficulty ascent length time    rating\n1     Mt. Marcy        5344          5   3166   14.8 10.0  moderate\n2 Algonquin Peak       5114          5   2936    9.6  9.0  moderate\n3   Mt. Haystack       4960          7   3570   17.8 12.0 difficult\n4   Mt. Skylight       4926          7   4265   17.9 15.0 difficult\n5 Whiteface Mtn.       4867          4   2535   10.4  8.5      easy\n6       Dix Mtn.       4857          5   2800   13.2 10.0  moderate\n\n\n\nWhat features would we like a visualization of the categorical difficulty rating variable to capture?\n\nThe categorical variable rating contains different kind of info whose trend would be hard to directly exhibit. We want the visualization to capture: - The number of hikes in each difficulty category - The difficult level distribution pattern in the ordinal way\n\nWhat about a visualization of the quantitative elevation variable?\n\nFor the quantitative elevation variable, we want the visualization to capture: - Show the overall distribution of elevation values - Reveal any overall trend in the data\nExercise 2: Load tidyverse\nWe’ll address the above questions using ggplot tools. Try running the following chunk and simply take note of the error message – this is one you’ll get a lot!\n\n# Use the ggplot function\nggplot(hikes, aes(x = rating))\n\nIn order to use ggplot tools, we have to first load the tidyverse package in which they live. We’ve installed the package but we need to tell R when we want to use it. Run the chunk below to load the library. You’ll need to do this within any .qmd file that uses ggplot().\n\n# Load the package\nlibrary(tidyverse)\n\nExercise 3: Bar Chart of Ratings - Part 1\nConsider some specific research questions about the difficulty rating of the hikes:\n\nHow many hikes fall into each category?\nAre the hikes evenly distributed among these categories, or are some more common than others?\n\nAll of these questions can be answered with: (1) a bar chart; of (2) the categorical data recorded in the rating column. First, set up the plotting frame:\n\nggplot(hikes, aes(x = rating))\n\nThink about:\n\nWhat did this do? What do you observe?\nWhat, in general, is the first argument of the ggplot() function?\nWhat is the purpose of writing x = rating?\nWhat do you think aes stands for?!?\nExercise 4: Bar Chart of Ratings - Part 2\nNow let’s add a geometric layer to the frame / canvas, and start customizing the plot’s theme. To this end, try each chunk below, one by one. In each chunk, make a comment about how both the code and the corresponding plot both changed.\nNOTE:\n\nPay attention to the general code properties and structure, not memorization.\nNot all of these are “good” plots. We’re just exploring ggplot.\n\n\n# add bars to the graph\nggplot(hikes, aes(x = rating)) +\n  geom_bar()\n\n\n# add labels to the graph\nggplot(hikes, aes(x = rating)) +\n  geom_bar() +\n  labs(x = \"Rating\", y = \"Number of hikes\")\n\n\n# add color to the bars\nggplot(hikes, aes(x = rating)) +\n  geom_bar(fill = \"blue\") +\n  labs(x = \"Rating\", y = \"Number of hikes\")\n\n\n# add line color\nggplot(hikes, aes(x = rating)) +\n  geom_bar(color = \"orange\", fill = \"blue\") +\n  labs(x = \"Rating\", y = \"Number of hikes\")\n\n\n# creates a white background with no grid lines or borders\nggplot(hikes, aes(x = rating)) +\n  geom_bar(color = \"orange\", fill = \"blue\")  +\n  labs(x = \"Rating\", y = \"Number of hikes\") +\n  theme_minimal()\n\nExercise 5: Bar Chart Follow-up\nPart a\nReflect on the ggplot() code.\n\nWhat’s the purpose of the +? When do we use it? Answer: The + operator is used to add a new component or layer to our visualization.\nWe added the bars using geom_bar()? Why “geom”? Answer: “geom” means “geometric object”, “geom” functions are used for visualizations.\nWhat does labs() stand for? Answer: “labs()” means “labels”.\nWhat’s the difference between color and fill? Answer: color means the borderline colors of graphic components while fillmeans the filling color of the graphic object.\nPart b\nIn general, bar charts allow us to examine the following properties of a categorical variable:\n\n\nobserved categories: What categories did we observe?\n\nvariability between categories: Are observations evenly spread out among the categories, or are some categories more common than others?\n\nWe must then translate this information into the context of our analysis, here hikes in the Adirondacks. Summarize below what you learned from the bar chart, in context.\nAnswer: The difficulty ratings for hikes in the Adirondacks fall into three observed categories: difficult, easy, and moderate. Most hikes are rated as moderate, indicating that they are of average challenge level. Very few hikes are rated as difficult, suggesting that extreme hikes are relatively rare.\nPart c\nIs there anything you don’t like about this barplot? For example: check out the x-axis again.\nAnswer: It does not contain the information of the specfic number of hikes for each level of rating.\nExercise 6: Sad Bar Chart\nLet’s now consider some research questions related to the quantitative elevation variable:\n\nAmong the hikes, what’s the range of elevation and how are the hikes distributed within this range (e.g. evenly, in clumps, “normally”)?\nWhat’s a typical elevation?\nAre there any outliers, i.e. hikes that have unusually high or low elevations?\n\nHere:\n\nConstruct a bar chart of the quantitative elevation variable.\nExplain why this might not be an effective visualization for this and other quantitative variables. (What questions does / doesn’t it help answer?)\n\nExplaination: There are too many bars scattering, and thus the overall trend is not well presented.\n\nggplot(hikes, aes(x = elevation)) +\n  geom_bar(color = \"orange\", fill = \"blue\")  +\n  theme_minimal()\n\nExercise 7: A Histogram of Elevation\nQuantitative variables require different viz than categorical variables. Especially when there are many possible outcomes of the quantitative variable. It’s typically insufficient to simply count up the number of times we’ve observed a particular outcome as the bar graph did above. It gives us a sense of ranges and typical outcomes, but not a good sense of how the observations are distributed across this range. We’ll explore two methods for graphing quantitative variables: histograms and density plots.\nHistograms are constructed by (1) dividing up the observed range of the variable into ‘bins’ of equal width; and (2) counting up the number of cases that fall into each bin. Check out the example below:\n\n\nggplot(hikes, aes(x = elevation)) +\n  geom_density(color = \"orange\")  +\n  theme_minimal()\n\nggplot(hikes, aes(x = elevation)) +\n  geom_histogram(color = \"orange\")  +\n  theme_minimal()\n\nPart a\nLet’s dig into some details.\n\nHow many hikes have an elevation between 4500 and 4700 feet? Answer: 6\nHow many total hikes have an elevation of at least 5100 feet? Answer: 2\nPart b\nNow the bigger picture. In general, histograms allow us to examine the following properties of a quantitative variable:\n\n\ntypical outcome: Where’s the center of the data points? What’s typical?\n\nvariability & range: How spread out are the outcomes? What are the max and min outcomes?\n\nshape: How are values distributed along the observed range? Is the distribution symmetric, right-skewed, left-skewed, bi-modal, or uniform (flat)?\n\noutliers: Are there any outliers, i.e. outcomes that are unusually large/small?\n\nWe must then translate this information into the context of our analysis, here hikes in the Adirondacks. Addressing each of the features in the above list, summarize below what you learned from the histogram, in context.\nAnswer: - typical outcome:The center of the data points is 4500 and a outcome between 4000-5000 is typical. - variability & range: The range of elevations vary from 3700 to 5700. - shape: The distribution is left-skewed. - outliers: There are some outliners.\nExercise 8: Building Histograms - Part 1\n2-MINUTE CHALLENGE: Thinking of the bar chart code, try to intuit what line you can tack on to the below frame of elevation to add a histogram layer. Don’t forget a +. If it doesn’t come to you within 2 minutes, no problem – all will be revealed in the next exercise.\n\nggplot(hikes, aes(x = elevation)) +\n  geom_histogram()\n\nExercise 9: Building Histograms - Part 2\nLet’s build some histograms. Try each chunk below, one by one. In each chunk, make a comment about how both the code and the corresponding plot both changed.\n\n# add labels\nggplot(hikes, aes(x = elevation)) +\n  geom_histogram() + \n  labs(\n    title = \"Distribution of Mountain Elevations\",\n    x = \"Elevation (feet)\",\n    y = \"Count\"\n  ) \n\n\n# change the borderline color to white\nggplot(hikes, aes(x = elevation)) +\n  geom_histogram(color = \"white\")  + \n  labs(\n    title = \"Distribution of Mountain Elevations\",\n    x = \"Elevation (feet)\",\n    y = \"Count\"\n  ) \n\n\n# change the filling color to blue\nggplot(hikes, aes(x = elevation)) +\n  geom_histogram(color = \"white\",fill = \"blue\")  + \n  labs(\n    title = \"Distribution of Mountain Elevations\",\n    x = \"Elevation (feet)\",\n    y = \"Count\"\n  ) \n\n\n# change the theme to classic\nggplot(hikes, aes(x = elevation)) +\n  geom_histogram(color = \"white\",fill = \"blue\")  + \n  labs(\n    title = \"Distribution of Mountain Elevations\",\n    x = \"Elevation (feet)\",\n    y = \"Count\"\n  ) +\n  theme_classic()\n\n\n# Changed binwidth to 1000 feet\nggplot(hikes, aes(x = elevation)) +\n  geom_histogram(color = \"white\", binwidth = 1000) +\n  labs(x = \"Elevation (feet)\", y = \"Number of hikes\")\n\n\n# Changed binwidth to 5 feet \nggplot(hikes, aes(x = elevation)) +\n  geom_histogram(color = \"white\", binwidth = 5) +\n  labs(x = \"Elevation (feet)\", y = \"Number of hikes\")\n\n\n# Changed binwidth to 200 feet\nggplot(hikes, aes(x = elevation)) +\n  geom_histogram(color = \"white\", binwidth = 200) +\n  labs(x = \"Elevation (feet)\", y = \"Number of hikes\")\n\nExercise 10: Histogram Follow-up\n\nWhat function added the histogram layer / geometry?Answer: The function geom_histogram() adds the histogram layer to the plot – tells ggplot to visualize the distribution of a numeric variable using bars.\nWhat’s the difference between color and fill?Answer: fill controls the filling color of the bars, while color controls the border of each bar.\nWhy does adding color = \"white\" improve the visualization?Answer: Adding color = \"white\" creates clear separation between adjacent bars and thus increase its accountability.\nWhat did binwidth do?Answer: The binwidth argument defines the width of each bar in the histogram – how the continuous variable is grouped.\nWhy does the histogram become ineffective if the binwidth is too big (e.g., 1000 feet)?Answer: If the binwidth is too large, too many values are grouped together, which limits the variation in the data and makes the distribution look overly simplified or flat.\nWhy does the histogram become ineffective if the binwidth is too small (e.g., 5 feet)?Answer: If the binwidth is too small, the plot becomes “noisy”, making it hard to see overall trends in the data.\nExercise 11: Density Plots\nDensity plots are essentially smooth versions of the histogram. Instead of sorting observations into discrete bins, the “density” of observations is calculated across the entire range of outcomes. The greater the number of observations, the greater the density! The density is then scaled so that the area under the density curve always equals 1 and the area under any fraction of the curve represents the fraction of cases that lie in that range.\nCheck out a density plot of elevation. Notice that the y-axis (density) has no contextual interpretation – it’s a relative measure. The higher the density, the more common are elevations in that range.\n\nggplot(hikes, aes(x = elevation)) +\n  geom_density()\n\nQuestions\n\n\nINTUITION CHECK: Before tweaking the code and thinking back to geom_bar() and geom_histogram(), how do you anticipate the following code will change the plot?\n\ngeom_density(color = \"blue\")\ngeom_density(fill = \"orange\")\n\n\nTRY IT! Test out those lines in the chunk below. Was your intuition correct?\n\n\n# color blue\nggplot(hikes, aes(x = elevation)) +\n  geom_density(color = \"blue\")\n\n# fill orange\nggplot(hikes, aes(x = elevation)) +\n  geom_density(fill = \"orange\")\n\n\nExamine the density plot. How does it compare to the histogram? What does it tell you about the typical elevation, variability / range in elevations, and shape of the distribution of elevations within this range?\n\nAnswer: The density plot provides a smoother and more continuous view of the distribution compared to the histogram. While the histogram groups data into discrete bins, the density plot estimates the probability density function, showing a general trend without being affected by bin width choices. The peak of the curves is around 4200 feets, which means the most typical outcome is 4200. The range of elevations spans roughly from 3600 to 5400 feet, indicating a spread of nearly 2000 feet. The distribution is right-skewed, suggesting there are more hikes with lower to mid elevations, and fewer hikes with very high elevations.\nExercise 12: Density Plots vs Histograms\nThe histogram and density plot both allow us to visualize the behavior of a quantitative variable: typical outcome, variability / range, shape, and outliers. What are the pros/cons of each? What do you like/not like about each?\nAnswer:\nHistogramPros: - Easy to interpret - Directly shows counts - Great for detecting outliers or gaps in the data if on the choice of bin width is small enough Cons: - Binned nature may obscure overall trends or give confusing results\nDensity PlotPros: - Smooth curve gives a clearer sense - No need to tune the bin width Cons: - Do not exhibit the number of data points\nExercise 13: Code = communication\nWe obviously won’t be done until we talk about communication. All code above has a similar general structure (where the details can change):\n\nggplot(___, aes(x = ___)) + \n  geom___(color = \"___\", fill = \"___\") + \n  labs(x = \"___\", y = \"___\")\n\n\nThough not necessary to the code working, it’s common, good practice to indent or tab the lines of code after the first line (counterexample below). Why?\n\nAnswer: It makes the code more visually organized and thus useful for future reference.\n\n# YUCK\nggplot(hikes, aes(x = elevation)) +\ngeom_histogram(color = \"white\", binwidth = 200) +\nlabs(x = \"Elevation (feet)\", y = \"Number of hikes\")\n\n\nThough not necessary to the code working, it’s common, good practice to put a line break after each + (counterexample below). Why?\n\nAnswer: It makes the code more visually organized and thus useful for future reference.\n\n# YUCK \nggplot(hikes, aes(x = elevation)) + geom_histogram(color = \"white\", binwidth = 200) + labs(x = \"Elevation (feet)\", y = \"Number of hikes\")\n\nExercise 14: Practice\nPart a\nPractice your viz skills to learn about some of the variables in one of the following datasets from the previous class:\n\n# Data on students in this class\nsurvey &lt;- read.csv(\"https://hash-mac.github.io/stat112site-s25/data/survey.csv\")\n\n# World Cup data\nworld_cup &lt;- read.csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-11-29/worldcups.csv\")\n\nPart b\nCheck out the RStudio Data Visualization cheat sheet to learn more features of ggplot.\n\n\n\n\n\n\nCheck → Commit → Push\n\n\n\nWhen done, don’t forgot to click Render Book and check the resulting HTML files. If happy, jump to GitHub Desktop and commit the changes with the message Finish activity 3 and push to GitHub. Wait few seconds, then visit your portfolio website and make sure the changes are there.",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Univariate Viz</span>"
    ]
  },
  {
    "objectID": "ica/ica-bi.html",
    "href": "ica/ica-bi.html",
    "title": "\n13  Bivariate Viz\n",
    "section": "",
    "text": "13.1 Review\nLet’s review some univariate concepts and code using our class survey data. If the answers aren’t at the top of your mind, don’t fret! We’ve barely started speaking this new language, and learned a ton of vocab last week, so you naturally won’t remember it all.\n# Import data\nsurvey &lt;- read.csv(\"https://hash-mac.github.io/stat112site-s25/data/survey.csv\")\n\n# How many students have now filled out the survey?\nnrow(survey)\n\n[1] 49\n\n# What type of variables do we have?\nstr(survey)\n\n'data.frame':   49 obs. of  4 variables:\n $ cafe_mac         : chr  \"mashed potatoes\" \"is tasty\" \"burger\" \"caesar salad\" ...\n $ minutes_to_campus: int  5 5 5 12 0 10 5 0 0 5 ...\n $ fav_temp_c       : num  26 28 19 18 24 -10 21 25 18 25 ...\n $ hangout          : chr  \"the mountains\" \"a city\" \"a forest\" \"a forest\" ...",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Bivariate Viz</span>"
    ]
  },
  {
    "objectID": "ica/ica-bi.html#review",
    "href": "ica/ica-bi.html#review",
    "title": "\n13  Bivariate Viz\n",
    "section": "",
    "text": "EXAMPLE 1: Hangout Preferences\nStudents were asked, in that moment, where they’d most like to spend time outside. How did they answer? Was there a lot of agreement or a lot of variability in answers? Build and interpret a plot that helps address these questions while reviewing:\n\n“code as communication”\nconnecting with the components of a plot:\n\nset up a frame\n\nadd a layer / geometric element\nchange the theme, e.g. axis labels, color, fill\n\n\n\n\n# Attach a package needed to use the ggplot function\nlibrary(tidyverse)\nlibrary(ggplot2)\n\n# Make a ggplot\nggplot(survey, aes(x = hangout)) +\n  geom_bar(fill = \"mediumseagreen\", color = \"grey\") +\n  labs(\n    title = \"Where Students Prefer to Hang Out Outdoors\",\n    x = \"Location\",\n    y = \"Number of Students\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\nEXAMPLE 2: Temperature Preferences\nStudents were asked about their ideal outdoor temperature, in degrees Celsius. How did they answer? What was the typical response? What was the range in responses? Were there any outliers? Build and interpret 2 plots that help address these questions.\n\nggplot(survey, aes(x = fav_temp_c)) +\n  geom_histogram(binwidth = 2, fill = \"coral\", color = \"white\") +\n  labs(\n    title = \"Distribution of Preferred Outdoor Temperature\",\n    x = \"Preferred Temperature (°C)\",\n    y = \"Count\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\nggplot(survey, aes(x = fav_temp_c)) +\n  geom_density(fill = \"coral\", alpha = 0.5) +\n  labs(title = \"Density of Student Temperature Preferences\",\n       x = \"Preferred Temperature (°C)\",\n       y = \"Density\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\nBar Charts vs. Histograms\n\n\n\nBar charts & histograms can appear pretty similar, but they do different things.\n\n\nBar charts count up the number of observations of each outcome of a variable. They’re good for categorical variables, or quantitative variables with only a handful of possible outcomes.\n\nHistograms count up the number of observations that fall into different numerical ranges of variable. They’re good for quantitative variables, especially those with many different observed outcomes.",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Bivariate Viz</span>"
    ]
  },
  {
    "objectID": "ica/ica-bi.html#new-stuff",
    "href": "ica/ica-bi.html#new-stuff",
    "title": "\n13  Bivariate Viz\n",
    "section": "\n13.2 New stuff",
    "text": "13.2 New stuff\nThus far, we’ve been studying one variable at a time, using univariate plots. But once we get a sense of how individual variables behave on their own, our questions often turn to relationships among variables. For example, in our hikes data:\n\nHow much time does it take to complete a hike? ——&gt; How is time related to a hike’s elevation? What about its length?\nHow does difficult rating vary from hike to hike? ——-&gt; How is difficulty rating related to a hike’s ascent?\n\n\n13.2.1 Exploring relationships\nExploring univariate patterns often sparks follow-up questions about relationships between 2+ variables. Often, but not always, variables take on specific roles:\n\n\nresponse variable: the variable whose variability we would like to explain (time to complete a hike)\n\npredictors: variables that might explain some of the variability in the response (a hike’s elevation or length)\n\nVisualizations can help explore:\n\nrelationship trends (direction and form)\nrelationship strength (degree of variability from the trend)\n\noutliers in the relationship\n\nEXAMPLE 3\nFor each pair of variables below, sketch on paper a visualization of their relationship. Focus on general viz process, don’t worry about the exact details. The data here are totally made up.\n\n3pm temperature (response) vs 9am temperature (predictor)\n\n\ndata.frame(temp_3pm = c(24, 26, 20, 15, 15, 15), temp_9am = c(14, 18, 15, 13, 11, 11))\n\n  temp_3pm temp_9am\n1       24       14\n2       26       18\n3       20       15\n4       15       13\n5       15       11\n6       15       11\n\n\n\n3pm temperature (response) vs location (predictor)\n\n\nweather &lt;- data.frame(temp_3pm = c(24, 26, 20, 15, 15, 0, 40, 60, 57, 44, 51, 75),\n                      location = rep(c(\"A\", \"B\"), each = 6))\nweather\n\n   temp_3pm location\n1        24        A\n2        26        A\n3        20        A\n4        15        A\n5        15        A\n6         0        A\n7        40        B\n8        60        B\n9        57        B\n10       44        B\n11       51        B\n12       75        B\n\n\nThink: How might we modify the below density plot of temp_3pm to distinguish between locations?\n\nggplot(weather, aes(x = temp_3pm)) +\n      geom_density()\n\n\n\n\n\n\n\n\n\nrain_today (the response) and location (the predictor)\n\n\nweather &lt;- data.frame(rain_today = c(\"no\", \"no\", \"no\", \"no\", \"yes\", \"no\", \"yes\", \"no\", \"yes\", \"yes\", \"no\", \"yes\"),\n                        location = c(rep(\"A\", 7), rep(\"B\", 5)))\n    weather\n\n   rain_today location\n1          no        A\n2          no        A\n3          no        A\n4          no        A\n5         yes        A\n6          no        A\n7         yes        A\n8          no        B\n9         yes        B\n10        yes        B\n11         no        B\n12        yes        B\n\n\nThink: How might we modify the below bar plot of location to distinguish between days on which it did or didn’t rain?\n\nggplot(weather, aes(x = location)) +\n      geom_bar()\n\n\n\n\n\n\n\n\n13.2.2 General guidance for building bivariate plots\nAs with univariate plots, an appropriate visualization for the relationship between 2 variables depends upon whether the variables are quantitative or categorical. In general:\n\nEach quantitative variable requires a new axis (or a quantitative scale if we run out of axes).\nEach categorical variable requires a new way to “group” the graphic (eg: using colors, shapes, separate facets, etc)\nFor visualizations in which overlap in glyphs or plots obscures the patterns, try faceting or transparency.",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Bivariate Viz</span>"
    ]
  },
  {
    "objectID": "ica/ica-bi.html#exercises-required",
    "href": "ica/ica-bi.html#exercises-required",
    "title": "\n13  Bivariate Viz\n",
    "section": "\n13.3 Exercises (required)",
    "text": "13.3 Exercises (required)\nGithub user Tony McGovern has compiled and made available 2020/2016/2012 presidential election results for most of 3000+ U.S. counties, except Alaska. (Image: Wikimedia Commons)\n\nA wrangled version of this data, is imported below, after being combined with:\n\n2013 county-level demographics from the df_county_demographics data set from the choroplethr R package\nhistorical voting trends in the state in which the county falls (from https://www.270towin.com/content/blue-and-red-states):\n\nred = consistently Republican\nblue = consistently Democratic\npurple = something in between\n\n\n\n\n# Load data\nelections &lt;- read.csv(\"https://mac-stat.github.io/data/election_2020_county.csv\")\n\n# Check it out\nhead(elections)\n\n  state_name state_abbr historical    county_name county_fips total_votes_20\n1    Alabama         AL        red Autauga County        1001          27770\n2    Alabama         AL        red Baldwin County        1003         109679\n3    Alabama         AL        red Barbour County        1005          10518\n4    Alabama         AL        red    Bibb County        1007           9595\n5    Alabama         AL        red  Blount County        1009          27588\n6    Alabama         AL        red Bullock County        1011           4613\n  repub_pct_20 dem_pct_20 winner_20 total_votes_16 repub_pct_16 dem_pct_16\n1        71.44      27.02     repub          24661        73.44      23.96\n2        76.17      22.41     repub          94090        77.35      19.57\n3        53.45      45.79     repub          10390        52.27      46.66\n4        78.43      20.70     repub           8748        76.97      21.42\n5        89.57       9.57     repub          25384        89.85       8.47\n6        24.84      74.70       dem           4701        24.23      75.09\n  winner_16 total_votes_12 repub_pct_12 dem_pct_12 winner_12 total_population\n1     repub          23909        72.63      26.58     repub            54907\n2     repub          84988        77.39      21.57     repub           187114\n3     repub          11459        48.34      51.25       dem            27321\n4     repub           8391        73.07      26.22     repub            22754\n5     repub          23980        86.49      12.35     repub            57623\n6       dem           5318        23.51      76.31       dem            10746\n  percent_white percent_black percent_asian percent_hispanic per_capita_income\n1            76            18             1                2             24571\n2            83             9             1                4             26766\n3            46            46             0                5             16829\n4            75            22             0                2             17427\n5            88             1             0                8             20730\n6            22            71             0                6             18628\n  median_rent median_age\n1         668       37.5\n2         693       41.5\n3         382       38.3\n4         351       39.4\n5         403       39.6\n6         276       39.6\n\n\nWe’ll use this data to explore voting outcomes within the U.S.’s 2-party system. Here’s a list of candidates by year:\n\n\nyear\nRepublican candidate\nDemocratic candidate\n\n\n\n2020\nDonald Trump\nJoe Biden\n\n\n2016\nDonald Trump\nHillary Clinton\n\n\n2012\nMitt Romney\nBarack Obama\n\n\n\nExercise 0: Review\nPart a\nHow many, or roughly what percent, of the 3000+ counties did the Republican candidate win in 2020?\n\nTake a guess.\nThen make a plot of the winner variable.\nThen discuss what follow-up questions you might have (and that our data might help us answer).\n\nFollow-up Question: How are these counties distributed? Is there a pattern?\n\nggplot(elections,aes(x=winner_12))+\n  geom_bar()\n\n\n\n\n\n\n\nPart b\nThe repub_pct_20 variable provides more detail about the Republican support in each county. Construct a plot of repub_pct_20.\nNotice that the distribution of Republican support from county to county is slightly left skewed or negatively skewed.\nWhat follow-up questions do you have? Follow-up Question: Why the distribution is left skewed?\n\nggplot(elections,aes(x=repub_pct_20))+\n  geom_density()\n\n\n\n\n\n\n\nExercise 1: Quantitative vs Quantitative Intuition Check\n\n\n\n\n\n\nBe Quick\n\n\n\nDon’t spend more than 3 minutes on this!\n\n\nBelow is a scatterplot of the Republican support in 2020 vs 2016. Notice that:\n\nboth variables are quantitative, and get their own axes\nthe response variable is on the y-axis, demonstrating how repub_pct_20 might be predicted by repub_pct_16, not vice versa\n\nTry to replicate this using ggplot(). THINK:\n\nWhat info do you need to set up the canvas?\nWhat geometric layer (geom_???) might add these dots / points for each county? We haven’t learned this yet, just take some guesses.\n\n\n\nggplot(elections,aes(x=repub_pct_16,y=repub_pct_20))+\n  geom_point()\n\n\n\n\n\n\n\nExercise 2: 2 Quantitiative Variables\nRun each chunk below to build up a a scatterplot of repub_pct_20 vs repub_pct_16 with different glyphs representing each county. Address or think about any prompts in the comments (#).\n\n# Set up the plotting frame\n# How does this differ than the frame for our histogram of repub_pct_20 alone?\nggplot(elections, aes(y = repub_pct_20, x = repub_pct_16))\n\n\n# Add a layer of points for each county\n# Take note of the geom!\nggplot(elections, aes(y = repub_pct_20, x = repub_pct_16)) +\n  geom_point()\n\n\n# Change the shape of the points\n# What happens if you change the shape to another number?\nggplot(elections, aes(y = repub_pct_20, x = repub_pct_16)) +\n  geom_point(shape = 3)\n\n\n# YOU TRY: Modify the code to make the points \"orange\"\n# NOTE: Try to anticipate if \"color\" or \"fill\" will be useful here. Then try both.\nggplot(elections, aes(y = repub_pct_20, x = repub_pct_16)) +\n  geom_point(fill=\"orange\")\n\n\n\n\n\n\nggplot(elections, aes(y = repub_pct_20, x = repub_pct_16)) +\n  geom_point(color=\"orange\")\n\n\n\n\n\n\nggplot(elections, aes(y = repub_pct_20, x = repub_pct_16)) +\n  geom_point(color=\"orange\",fill=\"orange\")\n\n\n\n\n\n\n\n\n# Add a layer that represents each county by the state it's in\n# Take note of the geom and the info it needs to run!\nggplot(elections, aes(y = repub_pct_20, x = repub_pct_16, color = state_name)) +\n  geom_text(aes(label = state_abbr))\n\nExercise 3: Reflect\nSummarize the relationship between the Republican support in 2020 and 2016. Be sure to comment on:\n\nthe strength of the relationship (weak/moderate/strong) \nthe direction of the relationship (positive/negative) \noutliers (in what state do counties deviate from the national trend? Any ideas why this might be the case?) \nExercise 4: Visualizing trend\nThe trend of the relationship between repub_pct_20 and repub_pct_16 is clearly positive and (mostly) linear. We can highlight this trend by adding a model “smooth” to the plot:\n\nggplot(elections, aes(y = repub_pct_20, x = repub_pct_16)) +\n  geom_point() +\n  geom_smooth()\n\nPart a\nConstruct a new plot that contains the model smooth but does not include the individual point glyphs.\n\nggplot(elections, aes(y = repub_pct_20, x = repub_pct_16)) +\n  geom_smooth()\n\n\n\n\n\n\n\nPart b\nBy default, geom_smooth() adds a smooth, localized model line. To examine the “best” linear model, we can specify method = \"lm\". It’s pretty similar in this example!\n\nggplot(elections, aes(y = repub_pct_20, x = repub_pct_16)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\nExercise 5: Your Turn\nTo examine how the 2020 results are related to some county demographics, construct scatterplots of repub_pct_20 vs median_rent, and repub_pct_20 vs median_age. Summarize the relationship between these two variables and comment on which is the better predictor of repub_pct_20, median_rent or median_age.\nComments: Median age has a weak, positive correlation with Republican support, while median rent has a stronger, negative correlation. In conclusion, median_rent is a better predictor of repub_pct_20.\n\n# Scatterplot of repub_pct_20 vs median_rent\nggplot(elections, aes(y = repub_pct_20, x = median_rent)) +\n  geom_point()\n\n\n\n\n\n\n# Scatterplot of repub_pct_20 vs median_age\nggplot(elections, aes(y = repub_pct_20, x = median_age)) +\n  geom_point()\n\n\n\n\n\n\n\nExercise 6: A Sad Scatterplot\nNext, let’s explore the relationship between a county’s 2020 Republican support repub_pct_20 and the historical political trends in its state. In this case repub_pct_20 is quantitative, but historical is categorical. Explain why a scatterplot might not be an effective visualization for exploring this relationship. (What questions does / doesn’t it help answer?)\nComments: The historical variable is categorical. Points overlap heavily and do not show meaningful group patterns. A boxplot or violin plot would be better.\n\nggplot(elections, aes(y = repub_pct_20, x = historical)) +\n  geom_point()\n\n\n\n\n\n\n\nExercise 7: Quantitative vs Categorical – Violins & Boxes\nThough the above scatterplot did group the counties by historical category, it’s nearly impossible to pick out meaningful patterns in 2020 Republican support in each category. Let’s try adding 2 different geom layers to the frame:\n\n# Side-by-side violin plots\nggplot(elections, aes(y = repub_pct_20, x = historical)) +\n  geom_violin()\n\n\n# Side-by-side boxplots (defined below)\nggplot(elections, aes(y = repub_pct_20, x = historical)) +\n  geom_boxplot()\n\nBox plots are constructed from five numbers - the minimum, 25th percentile, median, 75th percentile, and maximum value of a quantitative variable:\n\nREFLECT:\nSummarize what you’ve learned about the 2020 Republican county-level support within and between red/purple/blue states.\n\nRepublican support is generally highest in red states, lowest in blue.\nThere’s considerable variability within all three groups.\nExercise 8: Quantitative vs Categorical – Intuition Check\n\n\n\n\n\n\nBe Quick\n\n\n\nDon’t spend more than 3 minutes on this!\n\n\nWe can also visualize the relationship between repub_pct_20 and historical using our familiar density plots. In the plot below, notice that we simply created a separate density plot for each historical category. (The plot itself is “bad” but we’ll fix it below.) Try to adjust the code chunk below, which starts with a density plot of repub_pct_20 alone, to re-create this image.\n\n\nggplot(elections, aes(x = repub_pct_20)) +\n  geom_density()\n\n\n\n\n\n\n\nExercise 9: Quantitative vs Categorical – Density Plots\nWork through the chunks below and address the comments therein.\n\n# the curve on the front hides parts of the curves on the back\nggplot(elections, aes(x = repub_pct_20, fill = historical)) +\n  geom_density()\n\n\n# What does scale_fill_manual do? It assigns colors to each curve\nggplot(elections, aes(x = repub_pct_20, fill = historical)) +\n  geom_density() +\n  scale_fill_manual(values = c(\"blue\", \"purple\", \"red\"))\n\n\n# What does alpha = 0.5 do? It adds transparency to each curve\n# Play around with different values of alpha, between 0 and 1\nggplot(elections, aes(x = repub_pct_20, fill = historical)) +\n  geom_density(alpha = 0.5) +\n  scale_fill_manual(values = c(\"blue\", \"purple\", \"red\"))\n\n\n# What does facet_wrap do?! It divides the graphs into 3 facets\nggplot(elections, aes(x = repub_pct_20, fill = historical)) +\n  geom_density() +\n  scale_fill_manual(values = c(\"blue\", \"purple\", \"red\")) +\n  facet_wrap(~ historical)\n\n\n# Let's try a similar grouping strategy with a histogram instead of density plot.\n# Why is this terrible? Hard for the readers to interpret the trend of blue/purple states\nggplot(elections, aes(x = repub_pct_20, fill = historical)) +\n  geom_histogram(color = \"white\") +\n  scale_fill_manual(values = c(\"blue\", \"purple\", \"red\"))\n\nExercise 10\nWe’ve now learned 3 (of many) ways to visualize the relationship between a quantitative and categorical variable: side-by-side violins, boxplots, and density plots.\n\nWhich do you like best? Answer: Boxplots\nWhat is one pro of density plots relative to boxplots? Answer: Density plots show full distribution shape.\nWhat is one con of density plots relative to boxplots? Answer: Density plots are harder to interpret quickly.\nExercise 11: Categorical vs Categorical – Intuition Check\nFinally, let’s simply explore who won each county in 2020 (winner_20) and how this breaks down by historical voting trends in the state. That is, let’s explore the relationship between 2 categorical variables! Following the same themes as above, we can utilize grouping features such as fill/color or facets to distinguish between different categories of winner_20 and historical.\n\n\n\n\n\n\nBe Quick\n\n\n\nSpend at most 5 minutes on the following intuition check. Adjust the code below to recreate the following two plots.\n\n\n\n\n# Plot 1: adjust this to recreate the top plot\nggplot(elections, aes(x = historical, fill = winner_20)) +\n  geom_bar()\n\n\n\n\n\n\n\n\n# Plot 2: adjust this to recreate the bottom plot\nggplot(elections, aes(x = winner_20)) +\n  geom_bar() +\n  facet_grid(~historical)\n\n\n\n\n\n\n\nExercise 12: Categorical vs Categorical\nConstruct the following 4 bar plot visualizations.\n\n# A stacked bar plot\n# How are the \"historical\" and \"winner_20\" variables mapped to the plot, i.e. what roles do they play? one as filling and one as x-axis\nggplot(elections, aes(x = historical, fill = winner_20)) +\n  geom_bar()\n\n\n# A faceted bar plot\nggplot(elections, aes(x = winner_20)) +\n  geom_bar() +\n  facet_wrap(~ historical)\n\n\n# A side-by-side bar plot\n# Note the new argument to geom_bar\nggplot(elections, aes(x = historical, fill = winner_20)) +\n  geom_bar(position = \"dodge\")\n\n\n# A proportional bar plot\n# Note the new argument to geom_bar\nggplot(elections, aes(x = historical, fill = winner_20)) +\n  geom_bar(position = \"fill\")\n\nPart a\nName one pro and one con of using the “proportional bar plot” instead of one of the other three options.\nPro: Proportional bar plot directly shows the distribution of winner_20 in each kind of states. Con: It does not show the number of each type of states.\nPart b\nWhat’s your favorite bar plot from part and why? Answer:I prefer stacked bar plot due to its great accountability and simplicity.\nExercise 13: Practice (now or later)\n\n\n\n\n\n\nDecide\n\n\n\nDecide what’s best for you:\n\nTry this extra practice now.\nReflect on the above exercises and come back to this extra practice later (but before the next class).\n\n\n\nImport some daily weather data from a few locations in Australia:\n\nweather &lt;- read.csv(\"https://mac-stat.github.io/data/weather_3_locations.csv\")\n\nConstruct plots that address the research questions in each chunk. You might make multiple plots–there are many ways to do things!. However, don’t just throw spaghetti at the wall.\nReflect before doing anything. What types of variables are these? How might you plot just 1 of the variables, and then tweak the plot to incorporate the other?\n\n# How do 3pm temperatures (temp3pm) differ by location?\nggplot(weather, aes(x = temp3pm, fill = location)) +\n  geom_density(alpha = 0.5)\n\n\n\n\n\n\nggplot(weather, aes(y = temp3pm, x = location)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n# How might we predict the 3pm temperature (temp3pm) by the 9am temperature (temp9am)?\nggplot(weather, aes(y = temp3pm, x = temp9am)) +\n  geom_point(alpha = 0.4)\n\n\n\n\n\n\n\n\n# How do the number of rainy days (raintoday) differ by location?\nggplot(weather, aes(x = location, fill = raintoday)) +\n  geom_bar()",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Bivariate Viz</span>"
    ]
  },
  {
    "objectID": "ica/ica-multi.html",
    "href": "ica/ica-multi.html",
    "title": "\n14  Multivariate Viz\n",
    "section": "",
    "text": "14.1 Review\nLet’s review some univariate and bivariate plotting concepts using some daily weather data from Australia. This is a subset of the data from the weatherAUS data in the rattle package.\nlibrary(tidyverse)\n\n# Import data\nweather &lt;- read.csv(\"https://mac-stat.github.io/data/weather_3_locations.csv\") |&gt; \n  mutate(date = as.Date(date))  \n\n# Check out the first 6 rows\n# What are the units of observation?\nhead(weather)\n\n        date   location mintemp maxtemp rainfall evaporation sunshine\n1 2020-01-01 Wollongong    17.1    23.1        0          NA       NA\n2 2020-01-02 Wollongong    17.7    24.2        0          NA       NA\n3 2020-01-03 Wollongong    19.7    26.8        0          NA       NA\n4 2020-01-04 Wollongong    20.4    35.5        0          NA       NA\n5 2020-01-05 Wollongong    19.8    21.4        0          NA       NA\n6 2020-01-06 Wollongong    18.3    22.9        0          NA       NA\n  windgustdir windgustspeed winddir9am winddir3pm windspeed9am windspeed3pm\n1         SSW            39        SSW        SSE           20           15\n2         SSW            37          S        ENE           13           15\n3          NE            41        NNW        NNE            7           17\n4         SSW            78         NE        NNE           15           17\n5         SSW            57        SSW          S           31           35\n6          NE            35        ESE         NE           17           20\n  humidity9am humidity3pm pressure9am pressure3pm cloud9am cloud3pm temp9am\n1          69          64      1014.9      1014.0        8        1    19.1\n2          72          54      1020.1      1017.7        7        1    19.8\n3          72          71      1017.5      1013.0        6       NA    23.4\n4          77          69      1008.8      1003.9       NA       NA    24.5\n5          70          75      1018.9      1019.9       NA        7    20.7\n6          71          71      1021.2      1018.2       NA       NA    20.9\n  temp3pm raintoday risk_mm raintomorrow\n1    22.9        No     0.0           No\n2    23.6        No     0.0           No\n3    25.7        No     0.0           No\n4    26.7        No     0.0           No\n5    20.0        No     0.0           No\n6    22.6        No     0.8           No\n\n# How many data points do we have? \nnrow(weather)\n\n[1] 2367\n\n# What type of variables do we have?\nstr(weather)\n\n'data.frame':   2367 obs. of  24 variables:\n $ date         : Date, format: \"2020-01-01\" \"2020-01-02\" ...\n $ location     : chr  \"Wollongong\" \"Wollongong\" \"Wollongong\" \"Wollongong\" ...\n $ mintemp      : num  17.1 17.7 19.7 20.4 19.8 18.3 19.9 20.1 19.8 20.5 ...\n $ maxtemp      : num  23.1 24.2 26.8 35.5 21.4 22.9 25.6 23.2 23.1 25.4 ...\n $ rainfall     : num  0 0 0 0 0 0 0.8 1.6 0 0 ...\n $ evaporation  : num  NA NA NA NA NA NA NA NA NA NA ...\n $ sunshine     : num  NA NA NA NA NA NA NA NA NA NA ...\n $ windgustdir  : chr  \"SSW\" \"SSW\" \"NE\" \"SSW\" ...\n $ windgustspeed: int  39 37 41 78 57 35 44 41 39 56 ...\n $ winddir9am   : chr  \"SSW\" \"S\" \"NNW\" \"NE\" ...\n $ winddir3pm   : chr  \"SSE\" \"ENE\" \"NNE\" \"NNE\" ...\n $ windspeed9am : int  20 13 7 15 31 17 30 31 24 19 ...\n $ windspeed3pm : int  15 15 17 17 35 20 7 33 26 39 ...\n $ humidity9am  : int  69 72 72 77 70 71 76 77 76 79 ...\n $ humidity3pm  : int  64 54 71 69 75 71 72 76 79 76 ...\n $ pressure9am  : num  1015 1020 1018 1009 1019 ...\n $ pressure3pm  : num  1014 1018 1013 1004 1020 ...\n $ cloud9am     : int  8 7 6 NA NA NA NA 8 NA NA ...\n $ cloud3pm     : int  1 1 NA NA 7 NA NA NA NA NA ...\n $ temp9am      : num  19.1 19.8 23.4 24.5 20.7 20.9 22.9 21.3 21.2 23 ...\n $ temp3pm      : num  22.9 23.6 25.7 26.7 20 22.6 24.9 22.2 22.2 25.1 ...\n $ raintoday    : chr  \"No\" \"No\" \"No\" \"No\" ...\n $ risk_mm      : num  0 0 0 0 0 0.8 1.6 0 0 1 ...\n $ raintomorrow : chr  \"No\" \"No\" \"No\" \"No\" ...",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Multivariate Viz</span>"
    ]
  },
  {
    "objectID": "ica/ica-multi.html#review",
    "href": "ica/ica-multi.html#review",
    "title": "\n14  Multivariate Viz\n",
    "section": "",
    "text": "Example 1\nConstruct a plot that allows us to examine how temp3pm varies.\nExample 2\nConstruct 3 plots that address the following research question:\nHow do afternoon temperatures (temp3pm) differ by location?\n\n# Plot 1 (no facets & starting from a density plot of temp3pm)\nggplot(weather, aes(x = temp3pm)) + \n  geom_density()\n\n\n\n\n\n\n\n\n# Plot 2 (no facets or densities)\n\n\n# Plot 3 (facets)\n\nReflection\n\nTemperatures tend to be highest, and most variable, in Uluru. There, they range from ~10 to ~45 with a typical temp around ~30 degrees.\nTemperatures tend to be lowest in Hobart. There, they range from ~5 to ~45 with a typical temp around ~15 degrees.\nWollongong temps are in between and are the least variable from day to day.\n\nSUBTLETIES: Defining fill or color by a variable\nHow we define the fill or color depends upon whether we’re defining it by a named color or by some variable in our dataset. For example:\n\ngeom___(fill = \"blue\")named colors are defined outside the aesthetics and put in quotes\ngeom___(aes(fill = variable)) or ggplot(___, aes(fill = variable))\ncolors/fills defined by a variable are defined inside the aesthetics\nExample 3\nLet’s consider Wollongong alone:\n\n# Don't worry about the syntax (we'll learn it soon)\nwoll &lt;- weather |&gt;\n  filter(location == \"Wollongong\") |&gt; \n  mutate(date = as.Date(date))  \n\n\n# How often does it raintoday?\n# Fill your geometric layer with the color blue.\nggplot(woll, aes(x = raintoday))\n\n\n\n\n\n\n\n\n# If it does raintoday, what does this tell us about raintomorrow?\n# Use your intuition first\nggplot(woll, aes(x = raintoday))\n\n\n\n\n\n\n\n\n# Now compare different approaches\n\n# Default: stacked bars\nggplot(woll, aes(x = raintoday, fill = raintomorrow)) + \n  geom_bar()\n\n\n\n\n\n\n\n\n# Side-by-side bars\nggplot(woll, aes(x = raintoday, fill = raintomorrow)) + \n  geom_bar(position = \"dodge\")\n\n\n\n\n\n\n\n\n# Proportional bars\n# position = \"fill\" refers to filling the frame, nothing to do with the color-related fill\nggplot(woll, aes(x = raintoday, fill = raintomorrow)) + \n  geom_bar(position = \"fill\")\n\n\n\n\n\n\n\nReflection\nThere’s often not one “best plot”, but a combination of plots that provide a complete picture:\n\nThe stacked and side-by-side bars reflect that on most days, it does not rain.\nThe proportional / filled bars lose that information, but make it easier to compare proportions: it’s more likely to rain tomorrow if it also rains today.\nExample 4\nConstruct a plot that illustrates how 3pm temperatures (temp3pm) vary by date in Wollongong. Represent each day on the plot and use a curve/line to help highlight the trends.\n\n# THINK: What variable goes on the y-axis?\n# For the curve, try adding span = 0.5 to tweak the curvature\n\n\n# Instead of a curve that captures the general TREND,\n# draw a line that illustrates the movement of RAW temperatures from day to day\n# NOTE: We haven't learned this geom yet! Guess.\nggplot(woll, aes(y = temp3pm, x = date))\n\n\n\n\n\n\n\nNOTE: A line plot isn’t always appropriate! It can be useful in situations like this, when our data are chronological.\nReflection\nThere’s a seasonal / cyclic behavior in temperatures – they’re highest in January (around 23 degrees) and lowest in July (around 16 degrees). There are also some outliers – some abnormally hot and cold days.",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Multivariate Viz</span>"
    ]
  },
  {
    "objectID": "ica/ica-multi.html#new-stuff",
    "href": "ica/ica-multi.html#new-stuff",
    "title": "\n14  Multivariate Viz\n",
    "section": "\n14.2 New Stuff",
    "text": "14.2 New Stuff\nNext, let’s consider the entire weather data for all 3 locations. The addition of location adds a 3rd variable into our research questions:\n\nHow does the relationship between raintoday and raintomorrow vary by location?\nHow does the behavior of temp3pm over date vary by location?\nAnd so on.\n\nThus far, we’ve focused on the following components of a plot:\n\nsetting up a frame\n\nadding layers / geometric elements\nsplitting the plot into facets for different groups / categories\nchange the theme, e.g. axis labels, color, fill\n\nWe’ll have to think about all of this, along with scales. Scales change the color, fill, size, shape, or other properties according to the levels of a new variable. This is different than just assigning scale by, for example, color = \"blue\".\nWork on the examples below in your groups. Check in with your intuition! We’ll then discuss as a group as relevant.\nExample 5\n\n# Plot temp3pm vs temp9am\n# Change the code in order to indicate the location to which each data point corresponds\nggplot(weather, aes(y = temp3pm, x = temp9am)) + \n  geom_point()\n\n\n\n\n\n\n\n\n# Change the code in order to indicate the location to which each data point corresponds\n# AND identify the days on which it rained / didn't raintoday\nggplot(weather, aes(y = temp3pm, x = temp9am)) + \n  geom_point()\n\n\n\n\n\n\n\n\n# How many ways can you think to make that plot of temp3pm vs temp9am with info about location and rain?\n# Play around!\n\nExample 6\n\n# Change the code in order to construct a line plot of temp3pm vs date for each separate location (no points!)\nggplot(weather, aes(y = temp3pm, x = date)) + \n  geom_line()\n\n\n\n\n\n\n\nExample 7\n\n# Plot the relationship of raintomorrow & raintoday\n# Change the code in order to indicate this relationship by location\nggplot(weather, aes(x = raintoday, fill = raintomorrow)) + \n  geom_bar(position = \"fill\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Not Get Overwhelmed?\n\n\n\nThere’s no end to the number and type of visualizations you could make. And it’s important to not just throw spaghetti at the wall until something sticks. FlowingData shows that one dataset can be visualized many ways, and makes good recommendations for data viz workflow, which we modify and build upon here:\n\nIdentify simple research questions.\nWhat do you want to understand about the variables or the relationships among them?\n\nStart with the basics and work incrementally.\n\nIdentify what variables you want to include in your plot and what structure these have (eg: categorical, quantitative, dates)\nStart simply. Build a plot of just 1 of these variables, or the relationship between 2 of these variables.\nSet up a plotting frame and add just one geometric layer at a time.\nStart tweaking: add whatever new variables you want to examine,\n\n\n\nAsk your plot questions.\n\nWhat questions does your plot answer? What questions are left unanswered by your plot?\nWhat new questions does your plot spark / inspire?\nDo you have the viz tools to answer these questions, or might you learn more?\n\n\nFocus.\nReporting a large number of visualizations can overwhelm the audience and obscure your conclusions. Instead, pick out a focused yet comprehensive set of visualizations.",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Multivariate Viz</span>"
    ]
  },
  {
    "objectID": "ica/ica-multi.html#exercises-required",
    "href": "ica/ica-multi.html#exercises-required",
    "title": "\n14  Multivariate Viz\n",
    "section": "\n14.3 Exercises (required)",
    "text": "14.3 Exercises (required)\nThe story\nThough far from a perfect assessment of academic preparedness, SAT scores have historically been used as one measurement of a state’s education system. The education dataset contains various education variables for each state:\n\n# Import and check out data\neducation &lt;- read.csv(\"https://mac-stat.github.io/data/sat.csv\")\nhead(education)\n\n       State expend ratio salary frac verbal math  sat  fracCat\n1    Alabama  4.405  17.2 31.144    8    491  538 1029   (0,15]\n2     Alaska  8.963  17.6 47.951   47    445  489  934 (45,100]\n3    Arizona  4.778  19.3 32.175   27    448  496  944  (15,45]\n4   Arkansas  4.459  17.1 28.934    6    482  523 1005   (0,15]\n5 California  4.992  24.0 41.078   45    417  485  902  (15,45]\n6   Colorado  5.443  18.4 34.571   29    462  518  980  (15,45]\n\n\nA codebook is provided by Danny Kaplan who also made these data accessible:\n\nExercise 1: SAT scores\nPart a\nConstruct a plot of how the average sat scores vary from state to state. (Just use 1 variable – sat not State!)\n\nggplot(education, aes(x=sat))+\n  geom_histogram()\n\n\n\n\n\n\n\nPart b\nSummarize your observations from the plot. Comment on the basics: range, typical outcomes, shape. (Any theories about what might explain this non-normal shape?)\nThe Average total score on the SAT ranges from 820 to 1150. The most typical outcome is 900. The shape is bi-modal.\nExercise 2: SAT Scores vs Per Pupil Spending & SAT Scores vs Salaries\nThe first question we’d like to answer is: Can the variability in sat scores from state to state be partially explained by how much a state spends on education, specifically its per pupil spending (expend) and typical teacher salary?\nPart a\n\n# Construct a plot of sat vs expend\n# Include a \"best fit linear regression model\" (HINT: method = \"lm\")\nggplot(education,aes(x=expend,y=sat))+\n  geom_point()+\n  geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\n\n# Construct a plot of sat vs salary\n# Include a \"best fit linear regression model\" (HINT: method = \"lm\")\nggplot(education,aes(x=salary,y=sat))+\n  geom_point()+\n  geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\nPart b\nWhat are the relationship trends between SAT scores and spending? Is there anything that surprises you?\nThe more expenditure the less the SAT scores\nExercise 3: SAT Scores vs Per Pupil Spending and Teacher Salaries\nConstruct one visualization of the relationship of sat with salary and expend. HINT: Start with just 2 variables and tweak that code to add the third variable. Try out a few things!\n\nggplot(education,aes(x=salary,y=expend,color=sat))+\n  geom_point()\n\n\n\n\n\n\n\nExercise 4: Another way to Incorporate Scale\nIt can be tough to distinguish color scales and size scales for quantitative variables. Another option is to discretize a quantitative variable, or basically cut it up into categories.\nConstruct the plot below. Check out the code and think about what’s happening here. What happens if you change “2” to “3”?\n\nggplot(education, aes(y = sat, x = salary, color = cut(expend, 2))) + \n  geom_point() + \n  geom_smooth(se = FALSE, method = \"lm\")\n\nDescribe the trivariate relationship between sat, salary, and expend.\nexpend and salary are postively associated. The less the salary the more the sat score.\nExercise 5: Finally an Explanation\nIt’s strange that SAT scores seem to decrease with spending. But we’re leaving out an important variable from our analysis: the fraction of a state’s students that actually take the SAT. The fracCat variable indicates this fraction: low (under 15% take the SAT), medium (15-45% take the SAT), and high (at least 45% take the SAT).\nPart a\nBuild a univariate viz of fracCat to better understand how many states fall into each category.\n\nggplot(education, aes(x=fracCat))+\n  geom_bar()\n\n\n\n\n\n\n\nPart b\nBuild 2 bivariate visualizations that demonstrate the relationship between sat and fracCat. What story does your graphic tell and why does this make contextual sense?\n\nggplot(education, aes(x=fracCat,y=sat))+\n  geom_violin()\n\n\n\n\n\n\n\nPart c\nMake a trivariate visualization that demonstrates the relationship of sat with expend AND fracCat. Highlight the differences in fracCat groups through color AND unique trend lines. What story does your graphic tell?\nDoes it still seem that SAT scores decrease as spending increases?\n\nggplot(education, aes(x=expend,y=sat,color=fracCat))+\n  geom_point()\n\n\n\n\n\n\n\nPart d\nPutting all of this together, explain this example of Simpson’s Paradox. That is, why did it appear that SAT scores decrease as spending increases even though the opposite is true?\nThe expenditure is positively associated to the percentage of students taking sat. When there are very few students attending the test they appears to attain a better average score.",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Multivariate Viz</span>"
    ]
  },
  {
    "objectID": "ica/ica-multi.html#exercises-optional",
    "href": "ica/ica-multi.html#exercises-optional",
    "title": "\n14  Multivariate Viz\n",
    "section": "\n14.4 Exercises (optional)",
    "text": "14.4 Exercises (optional)\nExercise 6: Heat Maps\nAs usual, we’ve only just scratched the surface! There are lots of other data viz techniques for exploring multivariate relationships. Let’s start with a heat map.\nPart a\nRun the chunks below. Check out the code, but don’t worry about every little detail! NOTES:\n\nThis is not part of the ggplot() grammar, making it a bit complicated.\nIf you’re curious about what a line in the plot does, comment it out (#) and check out what happens!\nIn the plot, for each state (row), each variable (column) is scaled to indicate whether the state has a relative high value (yellow), a relatively low value (purple), or something in between (blues/greens).\nYou can also play with the color scheme. Type ?cm.colors in the console to learn about various options.\nWe’ll improve the plot later, so don’t spend too much time trying to learn something from this plot.\n\n\n# Remove the \"State\" column and use it to label the rows\n# Then scale the variables\nplot_data &lt;- education |&gt; \n  column_to_rownames(\"State\") |&gt; \n  data.matrix() |&gt; \n  scale()\n\n# Load the gplots package needed for heatmaps\nlibrary(gplots)\n\n# Construct heatmap 1\nheatmap.2(plot_data,\n  dendrogram = \"none\",\n  Rowv = NA, \n  scale = \"column\",\n  keysize = 0.7, \n  density.info = \"none\",\n  col = hcl.colors(256), \n  margins = c(10, 20),\n  colsep = c(1:7), rowsep = (1:50), sepwidth = c(0.05, 0.05),\n  sepcolor = \"white\", trace = \"none\"\n)\n\n\n# Construct heatmap 2\nheatmap.2(plot_data,\n  dendrogram = \"none\",\n  Rowv = TRUE,             ### WE CHANGED THIS FROM NA TO TRUE\n  scale = \"column\",\n  keysize = 0.7, \n  density.info = \"none\",\n  col = hcl.colors(256), \n  margins = c(10, 20),\n  colsep = c(1:7), rowsep = (1:50), sepwidth = c(0.05, 0.05),\n  sepcolor = \"white\", trace = \"none\"\n)\n\n\n# Construct heatmap 3\nheatmap.2(plot_data,\n  dendrogram = \"row\",       ### WE CHANGED THIS FROM \"none\" TO \"row\"\n  Rowv = TRUE,            \n  scale = \"column\",\n  keysize = 0.7, \n  density.info = \"none\",\n  col = hcl.colors(256), \n  margins = c(10, 20),\n  colsep = c(1:7), rowsep = (1:50), sepwidth = c(0.05, 0.05),\n  sepcolor = \"white\", trace = \"none\"\n)\n\nPart b\nIn the final two plots, the states (rows) are rearranged by similarity with respect to these education metrics. The final plot includes a dendrogram which further indicates clusters of similar states. In short, states that have a shorter path to connection are more similar than others.\nPutting this all together, what insight do you gain about the education trends across U.S. states? Which states are similar? In what ways are they similar? Are there any outliers with respect to 1 or more of the education metrics?\nExercise 7: Star plots\nLike heat maps, star plots indicate the relative scale of each variable for each state. Thus, we can use star maps to identify similar groups of states, and unusual states!\nPart a\nConstruct and check out the star plot below. Note that each state has a “pie”, with each segment corresponding to a different variable. The larger a segment, the larger that variable’s value is in that state. For example:\n\nCheck out Minnesota. How does Minnesota’s education metrics compare to those in other states? What metrics are relatively high? Relatively low?\nWhat states appear to be similar? Do these observations agree with those that you gained from the heat map?\n\n\nstars(plot_data,\n  flip.labels = FALSE,\n  key.loc = c(10, 1.5),\n  cex = 1, \n  draw.segments = TRUE\n)\n\nPart b\nFinally, let’s plot the state stars by geographic location! What new insight do you gain here?!\n\nstars(plot_data,\n  flip.labels = FALSE,\n  locations = data.matrix(as.data.frame(state.center)),  # added external data to arrange by geo location\n  key.loc = c(-110, 28),\n  cex = 1, \n  draw.segments = TRUE\n)",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Multivariate Viz</span>"
    ]
  },
  {
    "objectID": "ica/ica-multi.html#solutions",
    "href": "ica/ica-multi.html#solutions",
    "title": "\n14  Multivariate Viz\n",
    "section": "\n14.5 Solutions",
    "text": "14.5 Solutions\n\nClick for Solutions\n\nlibrary(tidyverse)\n\n# Import data\nweather &lt;- read.csv(\"https://mac-stat.github.io/data/weather_3_locations.csv\") |&gt; \n  mutate(date = as.Date(date))  \n\n# Check out the first 6 rows\n# What are the units of observation?\nhead(weather)\n\n        date   location mintemp maxtemp rainfall evaporation sunshine\n1 2020-01-01 Wollongong    17.1    23.1        0          NA       NA\n2 2020-01-02 Wollongong    17.7    24.2        0          NA       NA\n3 2020-01-03 Wollongong    19.7    26.8        0          NA       NA\n4 2020-01-04 Wollongong    20.4    35.5        0          NA       NA\n5 2020-01-05 Wollongong    19.8    21.4        0          NA       NA\n6 2020-01-06 Wollongong    18.3    22.9        0          NA       NA\n  windgustdir windgustspeed winddir9am winddir3pm windspeed9am windspeed3pm\n1         SSW            39        SSW        SSE           20           15\n2         SSW            37          S        ENE           13           15\n3          NE            41        NNW        NNE            7           17\n4         SSW            78         NE        NNE           15           17\n5         SSW            57        SSW          S           31           35\n6          NE            35        ESE         NE           17           20\n  humidity9am humidity3pm pressure9am pressure3pm cloud9am cloud3pm temp9am\n1          69          64      1014.9      1014.0        8        1    19.1\n2          72          54      1020.1      1017.7        7        1    19.8\n3          72          71      1017.5      1013.0        6       NA    23.4\n4          77          69      1008.8      1003.9       NA       NA    24.5\n5          70          75      1018.9      1019.9       NA        7    20.7\n6          71          71      1021.2      1018.2       NA       NA    20.9\n  temp3pm raintoday risk_mm raintomorrow\n1    22.9        No     0.0           No\n2    23.6        No     0.0           No\n3    25.7        No     0.0           No\n4    26.7        No     0.0           No\n5    20.0        No     0.0           No\n6    22.6        No     0.8           No\n\n# How many data points do we have? \nnrow(weather)\n\n[1] 2367\n\n# What type of variables do we have?\nstr(weather)\n\n'data.frame':   2367 obs. of  24 variables:\n $ date         : Date, format: \"2020-01-01\" \"2020-01-02\" ...\n $ location     : chr  \"Wollongong\" \"Wollongong\" \"Wollongong\" \"Wollongong\" ...\n $ mintemp      : num  17.1 17.7 19.7 20.4 19.8 18.3 19.9 20.1 19.8 20.5 ...\n $ maxtemp      : num  23.1 24.2 26.8 35.5 21.4 22.9 25.6 23.2 23.1 25.4 ...\n $ rainfall     : num  0 0 0 0 0 0 0.8 1.6 0 0 ...\n $ evaporation  : num  NA NA NA NA NA NA NA NA NA NA ...\n $ sunshine     : num  NA NA NA NA NA NA NA NA NA NA ...\n $ windgustdir  : chr  \"SSW\" \"SSW\" \"NE\" \"SSW\" ...\n $ windgustspeed: int  39 37 41 78 57 35 44 41 39 56 ...\n $ winddir9am   : chr  \"SSW\" \"S\" \"NNW\" \"NE\" ...\n $ winddir3pm   : chr  \"SSE\" \"ENE\" \"NNE\" \"NNE\" ...\n $ windspeed9am : int  20 13 7 15 31 17 30 31 24 19 ...\n $ windspeed3pm : int  15 15 17 17 35 20 7 33 26 39 ...\n $ humidity9am  : int  69 72 72 77 70 71 76 77 76 79 ...\n $ humidity3pm  : int  64 54 71 69 75 71 72 76 79 76 ...\n $ pressure9am  : num  1015 1020 1018 1009 1019 ...\n $ pressure3pm  : num  1014 1018 1013 1004 1020 ...\n $ cloud9am     : int  8 7 6 NA NA NA NA 8 NA NA ...\n $ cloud3pm     : int  1 1 NA NA 7 NA NA NA NA NA ...\n $ temp9am      : num  19.1 19.8 23.4 24.5 20.7 20.9 22.9 21.3 21.2 23 ...\n $ temp3pm      : num  22.9 23.6 25.7 26.7 20 22.6 24.9 22.2 22.2 25.1 ...\n $ raintoday    : chr  \"No\" \"No\" \"No\" \"No\" ...\n $ risk_mm      : num  0 0 0 0 0 0.8 1.6 0 0 1 ...\n $ raintomorrow : chr  \"No\" \"No\" \"No\" \"No\" ...\n\n\nExample 1\n\nggplot(weather, aes(x = temp3pm)) + \n  geom_density()\n\n\n\n\n\n\n\nExample 2\n\n# Plot 1 (no facets & starting from a density plot of temp3pm)\nggplot(weather, aes(x = temp3pm, fill = location)) + \n  geom_density(alpha = 0.5)\n\n\n\n\n\n\n\n\n# Plot 2 (no facets or densities)\nggplot(weather, aes(y = temp3pm, x = location)) + \n  geom_boxplot()\n\n\n\n\n\n\n\n\n# Plot 3 (facets)\nggplot(weather, aes(x = temp3pm, fill = location)) + \n  geom_density(alpha = 0.5) + \n  facet_wrap(~ location)\n\n\n\n\n\n\n\nExample 3\n\n# How often does it raintoday?\n# Fill your geometric layer with the color blue.\nggplot(woll, aes(x = raintoday)) + \n  geom_bar(fill = \"blue\")\n\n\n\n\n\n\n\n\n# If it does raintoday, what does this tell us about raintomorrow?\n# Use your intuition first\nggplot(woll, aes(x = raintoday)) + \n  geom_bar(aes(fill = raintomorrow))\n\n\n\n\n\n\nggplot(woll, aes(x = raintoday, fill = raintomorrow)) + \n  geom_bar()\n\n\n\n\n\n\n\n\n# Now compare different approaches\n\n# Default: stacked bars\nggplot(woll, aes(x = raintoday, fill = raintomorrow)) + \n  geom_bar()\n\n\n\n\n\n\n\n\n# Side-by-side bars\nggplot(woll, aes(x = raintoday, fill = raintomorrow)) + \n  geom_bar(position = \"dodge\")\n\n\n\n\n\n\n\n\n# Proportional bars\n# position = \"fill\" refers to filling the frame, nothing to do with the color-related fill\nggplot(woll, aes(x = raintoday, fill = raintomorrow)) + \n  geom_bar(position = \"fill\")\n\n\n\n\n\n\n\nExample 4\n\n# THINK: What variable goes on the y-axis?\n# For the curve, try adding span = 0.5 to tweak the curvature\nggplot(woll, aes(y = temp3pm, x = date)) + \n  geom_point() + \n  geom_smooth(span = 0.5)\n\n\n\n\n\n\n\n\n# Instead of a curve that captures the general TREND,\n# draw a line that illustrates the movement of RAW temperatures from day to day\n# NOTE: We haven't learned this geom yet! Guess.\nggplot(woll, aes(y = temp3pm, x = date)) + \n  geom_line()\n\n\n\n\n\n\n\nExample 5\n\n# Plot temp3pm vs temp9am\n# Change the code in order to indicate the location to which each data point corresponds\nggplot(weather, aes(y = temp3pm, x = temp9am, color = location)) + \n  geom_point()\n\n\n\n\n\n\n\n\n# Change the code in order to indicate the location to which each data point corresponds\n# AND identify the days on which it rained / didn't raintoday\nggplot(weather, aes(y = temp3pm, x = temp9am, color = location)) + \n  geom_point() +\n  facet_wrap(~ raintoday)\n\n\n\n\n\n\n\n\n# How many ways can you think to make that plot of temp3pm vs temp9am with info about location and rain?\n# Play around!\n\nggplot(weather, aes(y = temp3pm, x = temp9am, color = location, shape = raintoday)) + \n  geom_point()\n\n\n\n\n\n\n\nExample 6\n\n# Change the code in order to construct a line plot of temp3pm vs date for each separate location (no points!)\nggplot(weather, aes(y = temp3pm, x = date, color = location)) + \n  geom_line()\n\n\n\n\n\n\n\nExample 7\n\n# Plot the relationship of raintomorrow & raintoday\n# Change the code in order to indicate this relationship by location\nggplot(weather, aes(x = raintoday, fill = raintomorrow)) + \n  geom_bar(position = \"fill\") + \n  facet_wrap(~ location)\n\n\n\n\n\n\n\nExercise 1: SAT scores\nPart a\n\n# A histogram would work too!\nggplot(education, aes(x = sat)) + \n  geom_density()\n\n\n\n\n\n\n\nPart b\naverage SAT scores range from roughly 800 to 1100. They appear bi-modal.\nExercise 2: SAT Scores vs Per Pupil Spending & SAT Scores vs Salaries\nPart a\n\n# Construct a plot of sat vs expend\n# Include a \"best fit linear regression model\"\nggplot(education, aes(y = sat, x = expend)) + \n  geom_point() + \n  geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\n\n# Construct a plot of sat vs salary\n# Include a \"best fit linear regression model\"\nggplot(education, aes(y = sat, x = salary)) + \n  geom_point() + \n  geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\nPart b\nThe higher the student expenditures and teacher salaries, the worse the SAT performance.\nExercise 3: SAT Scores vs Per Pupil Spending and Teacher Salaries\n\nggplot(education, aes(y = sat, x = salary, color = expend)) + \n  geom_point() + \n  geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\nExercise 4: Another Way to Incorporate Scale\n\nggplot(education, aes(y = sat, x = salary, color = cut(expend, 2))) + \n  geom_point() + \n  geom_smooth(se = FALSE, method = \"lm\")\n\n\n\n\n\n\nggplot(education, aes(y = sat, x = salary, color = cut(expend, 3))) + \n  geom_point() + \n  geom_smooth(se = FALSE, method = \"lm\")\n\n\n\n\n\n\n\nStates with lower salaries and expenditures tend to have higher SAT scores.\nExercise 5: Finally an Explanation\nPart a\n\nggplot(education, aes(x = fracCat)) + \n  geom_bar()\n\n\n\n\n\n\n\nPart b\nThe more students in a state that take the SAT, the lower the average scores tend to be. This is probably related to self-selection.\n\nggplot(education, aes(x = sat, fill = fracCat)) + \n  geom_density(alpha = 0.5)\n\n\n\n\n\n\n\nPart c\nWhen we control for the fraction of students that take the SAT, SAT scores increase with expenditure.\n\nggplot(education, aes(y = sat, x = expend, color = fracCat)) + \n  geom_point() + \n  geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\nPart d\nStudent participation tends to be lower among states with lower expenditures (which are likely also the states with higher ed institutions that haven’t historically required the SAT). Those same states tend to have higher SAT scores because of the self-selection of who participates.\nExercise 6: Heat Maps\nPart a\n\n# Remove the \"State\" column and use it to label the rows\n# Then scale the variables\nplot_data &lt;- education |&gt; \n  column_to_rownames(\"State\") |&gt; \n  data.matrix() |&gt; \n  scale()\n\n# Load the gplots package needed for heatmaps\nlibrary(gplots)\n\n# Construct heatmap 1\nheatmap.2(plot_data,\n  dendrogram = \"none\",\n  Rowv = NA, \n  scale = \"column\",\n  keysize = 0.7, \n  density.info = \"none\",\n  col = hcl.colors(256), \n  margins = c(10, 20),\n  colsep = c(1:7), rowsep = (1:50), sepwidth = c(0.05, 0.05),\n  sepcolor = \"white\", trace = \"none\"\n)\n\n\n\n\n\n\n\n\n# Construct heatmap 2\nheatmap.2(plot_data,\n  dendrogram = \"none\",\n  Rowv = TRUE,             ### WE CHANGED THIS FROM NA TO TRUE\n  scale = \"column\",\n  keysize = 0.7, \n  density.info = \"none\",\n  col = hcl.colors(256), \n  margins = c(10, 20),\n  colsep = c(1:7), rowsep = (1:50), sepwidth = c(0.05, 0.05),\n  sepcolor = \"white\", trace = \"none\"\n)\n\n\n\n\n\n\n\n\n# Construct heatmap 3\nheatmap.2(plot_data,\n  dendrogram = \"row\",       ### WE CHANGED THIS FROM \"none\" TO \"row\"\n  Rowv = TRUE,            \n  scale = \"column\",\n  keysize = 0.7, \n  density.info = \"none\",\n  col = hcl.colors(256), \n  margins = c(10, 20),\n  colsep = c(1:7), rowsep = (1:50), sepwidth = c(0.05, 0.05),\n  sepcolor = \"white\", trace = \"none\"\n)\n\n\n\n\n\n\n\nPart b\n\nSimilar values in verbal, math, and sat.\nHigh contrast (an inverse relationship) verbal/math/sat scores and the fraction of students that take the SAT.\nOutliers of Utah and California in ratio (more students per teacher).\nWhile grouped, fraction and salary are not as similar to each other as the sat scores; it is also interesting to notice states that have high ratios have generally low expenditures per student.\nExercise 7: Star Plots\nPart a\nMN is high on the SAT performance related metrics and low on everything else. MN is similar to Iowa, Kansas, Mississippi, Missouri, the Dakotas…\n\nstars(plot_data,\n  flip.labels = FALSE,\n  key.loc = c(10, 1.5),\n  cex = 1, \n  draw.segments = TRUE\n)\n\n\n\n\n\n\n\nPart b\nWhen the states are in geographical ordering, we’d notice more easily that states in similar regions of the U.S. have similar patterns of these variables.\n\nstars(plot_data,\n  flip.labels = FALSE,\n  locations = data.matrix(as.data.frame(state.center)),  # added external data to arrange by geo location\n  key.loc = c(-110, 28),\n  cex = 1, \n  draw.segments = TRUE\n)",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Multivariate Viz</span>"
    ]
  },
  {
    "objectID": "ica/ica-spatial.html",
    "href": "ica/ica-spatial.html",
    "title": "\n15  Spatial Viz\n",
    "section": "",
    "text": "15.1 Review\nIn the previous activity, we explored a Simpson’s Paradox–it seemed that - states with higher spending… - tend to have lower average SAT scores.\nBUT this was explained by a confounding or omitted or lurking variable which is the % of students in a state that take the SAT. Hence,\nThus, when controlling for the % of students that take the SAT, more spending is correlated with higher scores.\nLet’s explore a Simpson’s paradox related to Mac!\nBack in the 2000s, Macalester invested in insulating a few campus-owned houses, with the hopes of leading to energy savings.  Former Mac Prof Danny Kaplan accessed monthly data on energy use and other info for these addresses, before and after renovations:\n# Load tidyverse package for plotting and wrangling\nlibrary(tidyverse)\n\n# Import the data and only keep 2 addresses\nenergy &lt;- read.csv(\"https://mac-stat.github.io/data/MacNaturalGas.csv\") |&gt; \n  mutate(date = as.Date(paste0(month, \"/1/\", year), \"%m/%d/%Y\")) |&gt; \n  filter(address != \"c\")\n\n# Check it out\nhead(energy)\n\n  month year  price therms hdd address renovated       date\n1     6 2005  35.21     21   0       a        no 2005-06-01\n2     7 2005  37.37     21   0       a        no 2005-07-01\n3     8 2005  36.93     21   3       a        no 2005-08-01\n4     9 2005  62.36     39  61       a        no 2005-09-01\n5    10 2005 184.15    120 416       a        no 2005-10-01\n6    11 2005 433.35    286 845       a        no 2005-11-01\nThe part of dataset codebook is below:",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Spatial Viz</span>"
    ]
  },
  {
    "objectID": "ica/ica-spatial.html#review",
    "href": "ica/ica-spatial.html#review",
    "title": "\n15  Spatial Viz\n",
    "section": "",
    "text": "States with higher spending…\ntend to have a higher % of students of students that take the SAT…\nwhich then “leads to” lower average SAT scores.\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nmeaning\n\n\n\ntherms\na measure of energy use–the more energy used, the larger the therms\n\n\naddress\na or b\n\n\nrenovated\nwhether the location had been renovated, yes or no\n\n\nmonth\nfrom 1 (January) to 12 (December)\n\n\nhdd\nmonthly heating degree days. A proxy measure of outside temperatures–the higher the hdd, the COLDER it was outside",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Spatial Viz</span>"
    ]
  },
  {
    "objectID": "ica/ica-spatial.html#examples",
    "href": "ica/ica-spatial.html#examples",
    "title": "\n15  Spatial Viz\n",
    "section": "\n15.2 Examples",
    "text": "15.2 Examples\n\n\n\n\n\n\nInstructions\n\n\n\n\nConstruct a plot that addresses each research question\nInclude a 1-sentence summary of the plot.\n\n\n\nExample 1\nWhat was the range and typical energy used each month, as measured by therms? How does this differ by address?\nExample 2\nHow did energy use (therms) change over time (date) at the two addresses?\nExample 3\nHow did the typical energy use (therms) at the two addresses change before and after they were renovated?\nExample 4\nThat seems unfortunate that energy usage went up after renovations. But also fishy.\nTake 5 minutes (in your groups) to try and explain what’s going on here. Think: What confounding or lurking or omitted variable related to energy usage are we ignoring here? Try to make some plots to prove your point.\nExample 5\nLet’s summarize the punchlines by filling in the ???. It seemed that:\n\nAfter renovation…\nenergy use increased.\n\nBUT this was explained by a confounding or omitted or lurking variable: ???\n\nAfter renovation…\n???…\nwhich then leads to higher energy use.\n\nThus, when controlling for ???, renovations led to decreased energy use.",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Spatial Viz</span>"
    ]
  },
  {
    "objectID": "ica/ica-spatial.html#new-stuff",
    "href": "ica/ica-spatial.html#new-stuff",
    "title": "\n15  Spatial Viz\n",
    "section": "\n15.3 New stuff",
    "text": "15.3 New stuff\nTypes of spatial viz:\n\nPoint Maps: plotting locations of individual observations\nexample: bigfoot sightings\nContour Maps: plotting the density or distribution of observations (not the individual observations themselves)\n\nChoropleth Maps: plotting outcomes in different regions\n\nNYT article on effects of redlining\nMinnesota Reformer article on how Mpls / St Paul voted on 2021 ballot measures related to mayoral, policing, and rent policies\n\n\n\nThese spatial maps can be static or dynamic/interactive.",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Spatial Viz</span>"
    ]
  },
  {
    "objectID": "ica/ica-spatial.html#exercises",
    "href": "ica/ica-spatial.html#exercises",
    "title": "\n15  Spatial Viz\n",
    "section": "\n15.4 Exercises",
    "text": "15.4 Exercises\n\n15.4.1 Preview\nYou’ll explore some R spatial viz tools below. In general, there are two important pieces to every map:\nPiece 1: A dataset\nThis dataset must include either:\n\nlocation coordinates for your points of interest (for point maps); or\nvariable outcomes for your regions of interest (for choropleth maps)\n\n\nPiece 2: A background map\nWe need latitude and longitude coordinates to specify the boundaries for your regions of interest (eg: countries, states). This is where it gets really sticky!\n\nCounty-level, state-level, country-level, continent-level info live in multiple places.\nWhere we grab this info can depend upon whether we want to make a point map or a choropleth map. (The background maps can be used somewhat interchangeably, but it requires extra code :/)\nWhere we grab this info can also depend upon the structure of our data and how much data wrangling / cleaning we’re up for. For choropleth maps, the labels of regions in our data must match those in the background map. For example, if our data labels states with their abbreviations (eg: MN) and the background map refers to them as full names in lower case (eg: minnesota), we have to wrangle our data so that it matches the background map.\n\nIn short, the code for spatial viz gets very specialized. The goal of these exercises is to:\n\nplay around and experience the wide variety of spatial viz tools out there\nunderstand the difference between point maps and choropleth maps\nhave fun\n\nYou can skip around as you wish and it’s totally fine if you don’t finish everything. Just come back at some point to play around.\nPart 1: Interactive points on a map with leaflet\n\nLeaflet is an open-source JavaScript library for creating maps. We can use it inside R through the leaflet package.\nThis uses a different plotting framework than ggplot2, but still has a tidyverse feel (which will become more clear as we learn other tidyverse tools!).\nThe general steps are as follows:\n\nCreate a map widget by calling leaflet() and telling it the data to use.\nAdd a base map using addTiles() (the default) or addProviderTiles().\nAdd layers to the map using layer functions (e.g. addMarkers(), addPolygons()).\nPrint the map widget to display it.\nExercise 1: A leaflet with markers / points\nEarlier this semester, I asked for the latitude and longitude of one of your favorite places. I rounded these to the nearest whole number, so that they’re near to but not exactly at those places. Let’s load the data and map it!\n\nlibrary(tidyverse)\n# #| echo: false\n# \n# fave_places &lt;- read.csv(\"../data/our_fave_places.csv\")\n# \n# # Check it out\n# head(fave_places)\n\n\nfave_places &lt;- read.csv(\"https://hash-mac.github.io/stat112site-s25/data/our_fave_places.csv\")\n\n# Check it out\nhead(fave_places)\n\n  latitude longitude\n1       59        18\n2       45       -93\n3       33      -117\n4       40       116\n5       40       106\n6       37      -122\n\n\nPart a\nYou can use a “two-finger scroll” to zoom in and out.\n\n# Load the leaflet package\nlibrary(leaflet)\n\n# Just a plotting frame\nleaflet(data = fave_places)\n\n\n\n\n\n\n# Now what do we have?\nleaflet(data = fave_places) |&gt; \n  addTiles()\n\n\n\n\n\n\n# Now what do we have?\n# longitude and latitude refer to the variables in our data\nleaflet(data = fave_places) |&gt; \n  addTiles() |&gt; \n  addMarkers(lng = ~longitude, lat = ~latitude)\n\n\n\n\n\n\n# Since we named them \"longitude\" and \"latitude\", the function\n# automatically recognizes these variables. No need to write them!\nleaflet(data = fave_places) |&gt; \n  addTiles() |&gt; \n  addMarkers()\n\n\n\n\n\nPart b\nPLAY AROUND! This map is interactive. Zoom in on one location. Keep zooming – what level of detail can you get into? How does that detail depend upon where you try to zoom in (thus what are the limitations of this tool)?\nExercise 2: Details\nWe can change all sorts of details in leaflet maps.\n\n# Load package needed to change color\nlibrary(gplots)\n\n# We can add colored circles instead of markers at each location\nleaflet(data = fave_places) |&gt; \n  addTiles() |&gt; \n  addCircles(color = col2hex(\"red\"))\n\n\n\n\n\n\n# We can change the background\n# Mark locations with yellow dots\n# And connect the dots, in their order in the dataset, with green lines\n# (These green lines don't mean anything here, but would if this were somebody's travel path!)\nleaflet(data = fave_places) |&gt;\n  addProviderTiles(\"USGS\") |&gt;\n  addCircles(weight = 10, opacity = 1, color = col2hex(\"yellow\")) |&gt;\n  addPolylines(\n    lng = ~longitude,\n    lat = ~latitude,\n    color = col2hex(\"green\")\n  )\n\n\n\n\n\nIn general:\n\naddProviderTiles() changes the base map.\nTo explore all available provider base maps, type providers in the console. (Though some don’t work :/)\n\nUse addMarkers() or addCircles() to mark locations. Type ?addControl into the console to pull up a help file which summarizes the aesthetics of these markers and how you can change them. For example:\n\n\nweight = how thick to make the lines, points, pixels\n\nopacity = transparency (like alpha in ggplot2)\ncolors need to be in “hex” form. We used the col2hex() function from the gplots library to do that\n\n\nExercise 3: Your turn\nThe starbucks data, compiled by Danny Kaplan, contains information about every Starbucks in the world at the time the data were collected, including Latitude and Longitude:\n\n# Import starbucks location data\nstarbucks &lt;- read.csv(\"https://mac-stat.github.io/data/starbucks.csv\")\n\nLet’s focus on only those in Minnesota for now:\n\n# Don't worry about the syntax\nstarbucks_mn &lt;- starbucks |&gt;   \n  filter(Country == \"US\", State.Province == \"MN\")\n\nCreate a leaflet map of the Starbucks locations in Minnesota. Keep it simple – go back to Exercise 1 for an example.\nPart 2: Static points on a map\nLeaflet is very powerful and fun. But:\n\nIt’s not great when we have lots of points to map – it takes lots of time.\nIt makes good interactive maps, but we often need a static map (eg: we can not print interactive maps!).\n\nLet’s explore how to make point maps with ggplot(), not leaflet().\nExercise 3: A simple scatterplot\nLet’s start with the ggplot() tools we already know. Construct a scatterplot of all starbucks locations, not just those in Minnesota, with:\n\nLatitude and Longitude coordinates (which goes on the y-axis?!)\nMake the points transparent (alpha = 0.2) and smaller (size = 0.2)\n\nIt’s pretty cool that the plots we already know can provide some spatial context. But what don’t you like about this plot?\n\nlibrary(ggplot2)\nggplot(starbucks,aes(x=Latitude,y=Longitude))+\n  geom_point(alpha=0.2,size=0.2)\n\n\n\n\n\n\n\nExercise 4: Adding a country-level background\nLet’s add a background map of country-level boundaries.\nPart a\nFirst, we can grab country-level boundaries from the rnaturalearth package.\n\n# Load the package\nlibrary(rnaturalearth)\n\n# Get info about country boundaries across the world\n# in a \"sf\" or simple feature format\nworld_boundaries &lt;- ne_countries(returnclass = \"sf\")\n\nIn your console, type world_boundaries to check out what’s stored there. Don’t print it our in your Rmd – printing it would be really messy there (even just the head()).\nPart b\nRun the chunks below to build up a new map.\n\n# What does this code produce?\n# What geom are we using for the point map?\nggplot(world_boundaries) + \n  geom_sf()\n\n\n\n\n\n\n\n\n# Load package needed to change map theme\nlibrary(mosaic)\n\n# Add a point for each Starbucks\n# NOTE: The Starbucks info is in our starbucks data, not world_boundaries\n# How does this change how we use geom_point?!\nggplot(world_boundaries) + \n  geom_sf() + \n  geom_point(\n    data = starbucks,\n    aes(x = Longitude, y = Latitude),\n    alpha = 0.3, size = 0.2, color = \"darkgreen\"\n  ) +\n  theme_map()\n\n\n\n\n\n\n\nPart c\nSummarize what you learned about Starbucks from this map.\nExercise 5: Zooming in on some countries\nInstead of world_boundaries &lt;- ne_countries(returnclass = 'sf') we could zoom in on…\n\nthe continent of Africa: ne_countries(continent = 'Africa', returnclass = 'sf')\n\na set of countries: ne_countries(country = c('france', 'united kingdom', 'germany'), returnclass = 'sf')\n\nboundaries within a country: ne_states(country = 'united states of america', returnclass = 'sf')\n\n\nOur goal here will be to map the Starbucks locations in Canada, Mexico, and the US.\nPart a\nTo make this map, we again need two pieces of information.\n\nData on Starbucks for only Canada, Mexico, and the US, labeled as “CA”, “MX”, “US” in the starbucks data.\n\n\n# We'll learn this syntax soon! Don't worry about it now.\nstarbucks_cma &lt;- starbucks |&gt; \n  filter(Country %in% c('CA', 'MX', 'US'))\n\n\nA background map of state- and national-level boundaries in Canada, Mexico, and the US. This requires ne_states() in the rnaturalearth package where the countries are labeled ‘canada’, ‘mexico’, ‘united states of america’.\n\n\ncma_boundaries &lt;- ne_states(\n  country = c(\"canada\", \"mexico\", \"united states of america\"),\n  returnclass = \"sf\")\n\nPart b\nMake the map!\n\n# Just the boundaries\nggplot(cma_boundaries) + \n  geom_sf()\n\n\n\n\n\n\n\n\n# Add the points\n# And zoom in\nggplot(cma_boundaries) + \n  geom_sf() + \n  geom_point(\n    data = starbucks_cma,\n    aes(x = Longitude, y = Latitude),\n    alpha = 0.3,\n    size = 0.2,\n    color = \"darkgreen\"\n  ) +\n  coord_sf(xlim = c(-179.14, -50)) +\n  theme_map()\n\n\n\n\n\n\n\nExercise 6: A state and county-level map\nLet’s get an even higher resolution map of Starbucks locations within the states of Minnesota, Wisconsin, North Dakota, and South Dakota, with a background map at the county-level.\nPart a\nTo make this map, we again need two pieces of information.\n\nData on Starbucks for only the states of interest.\n\n\nstarbucks_midwest &lt;- starbucks |&gt; \n  filter(State.Province %in% c(\"MN\", \"ND\", \"SD\", \"WI\"))\n\n\nA background map of state- and county-level boundaries in these states. This requires st_as_sf() in the sf package, and map() in the maps package, where the countries are labeled ‘minnesota’, ‘north dakota’, etc.\n\n\n# Load packages\nlibrary(sf)\nlibrary(maps)\n\n# Get the boundaries\nmidwest_boundaries &lt;- st_as_sf(\n  maps::map(\"county\",\n            region = c(\"minnesota\", \"wisconsin\", \"north dakota\", \"south dakota\"), \n            fill = TRUE, plot = FALSE))\n\n# Check it out\nhead(midwest_boundaries)\n\nSimple feature collection with 6 features and 1 field\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -96.81268 ymin: 45.05167 xmax: -93.01397 ymax: 48.53526\nGeodetic CRS:  +proj=longlat +ellps=clrk66 +no_defs +type=crs\n                                     ID                           geom\nminnesota,aitkin       minnesota,aitkin MULTIPOLYGON (((-93.03689 4...\nminnesota,anoka         minnesota,anoka MULTIPOLYGON (((-93.51817 4...\nminnesota,becker       minnesota,becker MULTIPOLYGON (((-95.14537 4...\nminnesota,beltrami   minnesota,beltrami MULTIPOLYGON (((-95.58655 4...\nminnesota,benton       minnesota,benton MULTIPOLYGON (((-93.77027 4...\nminnesota,big stone minnesota,big stone MULTIPOLYGON (((-96.10794 4...\n\n\nPart b\nAdjust the code below to make the plot! Remove the # to run it.\n\nggplot(cma_boundaries) +\n  geom_sf() +\n  geom_point(\n    data = starbucks_midwest,\n    aes(x = Longitude, y = Latitude),\n    alpha = 0.7,\n    size = 0.2,\n    color = 'darkgreen'\n  ) +\n  theme_map()\n\n\n\n\n\n\n\nExercise 7: Contour maps\nEspecially when there are lots of point locations, and those locations start overlapping on a map, it can be tough to visualize areas of higher density. Consider the Starbucks locations in Canada, Mexico, and the US that we mapped earlier:\n\n# Point map (we made this earlier)\nggplot(cma_boundaries) + \n  geom_sf() + \n  geom_point(\n    data = starbucks_cma,\n    aes(x = Longitude, y = Latitude),\n    alpha = 0.3,\n    size = 0.2,\n    color = \"darkgreen\"\n  ) +\n  coord_sf(xlim = c(-179.14, -50), ylim = c(14.54, 83.11)) +\n  theme_map()\n\n\n\n\n\n\n\nNow check out the contour map.\n\n# What changed in the plot?\n# What changed in our code?!\nggplot(cma_boundaries) + \n  geom_sf() + \n  geom_density_2d(\n    data = starbucks_cma,\n    aes(x = Longitude, y = Latitude),\n    size = 0.2,\n    color = \"darkgreen\"\n  ) +\n  coord_sf(xlim = c(-179.14, -50), ylim = c(14.54, 83.11)) +\n  theme_map()\n\n\n\n\n\n\n\nPart 3: Choropleth maps\nSpatial data isn’t always in the form of point locations! For example, recall the state and county-level data on presidential elections.\n\nelections_by_state &lt;-  read.csv(\"https://mac-stat.github.io/data/election_2020_by_state.csv\")\nelections_by_counties &lt;- read.csv(\"https://mac-stat.github.io/data/election_2020_county.csv\")\n\nIn these datasets, we’re interested in the overall election outcome by region (state or county), not the specific geographic location of some observation. Let’s wrangle our data first. We’ll focus on just a few variables of interest, and create a new variable (repub_20_categories) that discretizes the repub_pct_20 variable into increments of 5 percentage points (for states) or 10 percentage points (for counties):\n\n# Don't worry about the code!\n\nelections_by_state &lt;- elections_by_state |&gt; \n  filter(state_abbr != \"DC\") |&gt; \n  select(state_name, state_abbr, repub_pct_20) |&gt; \n  mutate(repub_20_categories = \n           cut(repub_pct_20, \n               breaks = seq(30, 70, by = 5), \n               labels = c(\"30-34\", \"35-39\", \"40-44\", \"45-49\",\n                          \"50-54\", \"55-59\", \"60-64\", \"65-70\"), \n               include.lowest = TRUE))\n\nelections_by_counties &lt;- elections_by_counties |&gt; \n  select(state_name, state_abbr, county_name, county_fips,\n          repub_pct_20, median_age, median_rent) |&gt; \n  mutate(repub_20_categories = \n           cut(repub_pct_20, \n               breaks = seq(0, 100, by = 10),\n               labels = c(\"0-9\", \"10-19\", \"20-29\", \"30-39\", \"40-49\",\n                          \"50-59\", \"60-69\", \"70-79\", \"80-89\", \"90-100\"),\n               include.lowest = TRUE))\n\nExercise 8: State-level choropleth maps\nLet’s map the 2020 Republican support in each state, repub_pct_20.\nPart a\nWe again need two pieces of information.\n\nData on elections in each state, which we already have: elections_by_state.\nA background map of state boundaries in the US. The boundaries we used for point maps don’t work here. (Optional detail: they’re sf objects and we now need a data.frame object.) Instead, we can use the map_data() function from the ggplot2 package:\n\n\n# Get the latitude and longitude coordinates of state boundaries\nstates_map &lt;- map_data(\"state\")\n\n# Check it out\nhead(states_map)\n\n       long      lat group order  region subregion\n1 -87.46201 30.38968     1     1 alabama      &lt;NA&gt;\n2 -87.48493 30.37249     1     2 alabama      &lt;NA&gt;\n3 -87.52503 30.37249     1     3 alabama      &lt;NA&gt;\n4 -87.53076 30.33239     1     4 alabama      &lt;NA&gt;\n5 -87.57087 30.32665     1     5 alabama      &lt;NA&gt;\n6 -87.58806 30.32665     1     6 alabama      &lt;NA&gt;\n\n\nPause\nImportant detail: Note that the region variable in states_map, and the state_name variable in elections_by_state both label states by the full name in lower case letters. This is critical to the background map and our data being able to communicate.\n\nhead(states_map)\n\n       long      lat group order  region subregion\n1 -87.46201 30.38968     1     1 alabama      &lt;NA&gt;\n2 -87.48493 30.37249     1     2 alabama      &lt;NA&gt;\n3 -87.52503 30.37249     1     3 alabama      &lt;NA&gt;\n4 -87.53076 30.33239     1     4 alabama      &lt;NA&gt;\n5 -87.57087 30.32665     1     5 alabama      &lt;NA&gt;\n6 -87.58806 30.32665     1     6 alabama      &lt;NA&gt;\n\nhead(elections_by_state) \n\n   state_name state_abbr repub_pct_20 repub_20_categories\n1     alabama         AL        62.03               60-64\n2    arkansas         AR        62.40               60-64\n3     arizona         AZ        49.06               45-49\n4  california         CA        34.33               30-34\n5    colorado         CO        41.90               40-44\n6 connecticut         CT        39.21               35-39\n\n\nPart b\nNow map repub_pct_20 by state.\n\n# Note where the dataset, elections_by_state, is used\n# Note where the background map, states_map, is used\nggplot(elections_by_state, aes(map_id = state_name, fill = repub_pct_20)) +\n  geom_map(map = states_map) +\n  expand_limits(x = states_map$long, y = states_map$lat) +\n  theme_map() \n\n\n\n\n\n\n\n\n# Make it nicer!\nggplot(elections_by_state, aes(map_id = state_name, fill = repub_pct_20)) +\n  geom_map(map = states_map) +\n  expand_limits(x = states_map$long, y = states_map$lat) +\n  theme_map() + \n  scale_fill_gradientn(name = \"% Republican\", colors = c(\"blue\", \"purple\", \"red\"), values = scales::rescale(seq(0, 100, by = 5)))\n\n\n\n\n\n\n\nIt’s not easy to get fine control over the color scale for the quantitative repub_pct_20 variable. Instead, let’s plot the discretized version, repub_20_categories:\n\nggplot(elections_by_state, aes(map_id = state_name, fill = repub_20_categories)) +\n  geom_map(map = states_map) +\n  expand_limits(x = states_map$long, y = states_map$lat) +\n  theme_map()\n\n\n\n\n\n\n\n\n# Load package needed for refining color palette\nlibrary(RColorBrewer)\n\n# Now fix the colors\nggplot(elections_by_state, aes(map_id = state_name, fill = repub_20_categories)) +\n  geom_map(map = states_map) +\n  expand_limits(x = states_map$long, y = states_map$lat) +\n  theme_map() + \n  scale_fill_manual(values = rev(brewer.pal(8, \"RdBu\")), name = \"% Republican\")\n\n\n\n\n\n\n\nPart c\nWe can add other layers, like points, on top of a choropleth map. Add a Starbucks layer! Do you notice any relationship between Starbucks and elections? Or are we just doing things at this point? ;)\n\n# Get only the starbucks data from the US\nstarbucks_us &lt;- starbucks |&gt; \n  filter(Country == \"US\")\n\n# Map it\nggplot(elections_by_state, aes(map_id = state_name, fill = repub_20_categories)) +\n  geom_map(map = states_map) +\n  geom_point(\n    data = starbucks_us,\n    aes(x = Longitude, y = Latitude),\n    size = 0.05,\n    alpha = 0.2,\n    inherit.aes = FALSE\n  ) +\n  expand_limits(x = states_map$long, y = states_map$lat) +\n  theme_map() + \n  scale_fill_manual(values = rev(brewer.pal(8, \"RdBu\")), name = \"% Republican\")\n\n\n\n\n\n\n\nDetails (if you’re curious)\n\n\nmap_id is a required aesthetic for geom_map().\n\nIt specifies which variable in our dataset indicates the region (here state_name).\nIt connects this variable (state_name) to the region variable in our mapping background (states_map). These variables must have the same possible outcomes in order to be matched up (alabama, alaska, arizona,…).\n\n\n\nexpand_limits() assures that the map covers the entire area it’s supposed to, by pulling longitudes and latitudes from the states_map.\nPart d\nWe used geom_sf() for point maps. What geom do we use for choropleth maps?\nExercise 9: County-level choropleth maps\nLet’s map the 2020 Republican support in each county.\nPart a\nWe again need two pieces of information.\n\nData on elections in each county, which we already have: elections_by_county.\nA background map of county boundaries in the US, stored in the county_map dataset in the socviz package:\n\n\n# Get the latitude and longitude coordinates of county boundaries\nlibrary(socviz)\ndata(county_map) \n\n# Check it out\nhead(county_map)\n\n     long      lat order  hole piece            group    id\n1 1225889 -1275020     1 FALSE     1 0500000US01001.1 01001\n2 1235324 -1274008     2 FALSE     1 0500000US01001.1 01001\n3 1244873 -1272331     3 FALSE     1 0500000US01001.1 01001\n4 1244129 -1267515     4 FALSE     1 0500000US01001.1 01001\n5 1272010 -1262889     5 FALSE     1 0500000US01001.1 01001\n6 1276797 -1295514     6 FALSE     1 0500000US01001.1 01001\n\n\nPause\nImportant detail: We officially have a headache. Our county_map refers to each county by a 5-number id. Our elections_by_counties data refers to each county by a county_fips code, which is mostly the same as id, BUT drops any 0’s at the beginning of the code.\n\nhead(county_map)\n\n     long      lat order  hole piece            group    id\n1 1225889 -1275020     1 FALSE     1 0500000US01001.1 01001\n2 1235324 -1274008     2 FALSE     1 0500000US01001.1 01001\n3 1244873 -1272331     3 FALSE     1 0500000US01001.1 01001\n4 1244129 -1267515     4 FALSE     1 0500000US01001.1 01001\n5 1272010 -1262889     5 FALSE     1 0500000US01001.1 01001\n6 1276797 -1295514     6 FALSE     1 0500000US01001.1 01001\n\nhead(elections_by_counties)\n\n  state_name state_abbr    county_name county_fips repub_pct_20 median_age\n1    Alabama         AL Autauga County        1001        71.44       37.5\n2    Alabama         AL Baldwin County        1003        76.17       41.5\n3    Alabama         AL Barbour County        1005        53.45       38.3\n4    Alabama         AL    Bibb County        1007        78.43       39.4\n5    Alabama         AL  Blount County        1009        89.57       39.6\n6    Alabama         AL Bullock County        1011        24.84       39.6\n  median_rent repub_20_categories\n1         668               70-79\n2         693               70-79\n3         382               50-59\n4         351               70-79\n5         403               80-89\n6         276               20-29\n\n\nThis just means that we have to wrangle the data so that it can communicate with the background map.\n\n# Add 0's at the beginning of any fips_code that's fewer than 5 numbers long\n# Don't worry about the syntax\nelections_by_counties &lt;- elections_by_counties |&gt; \n  mutate(county_fips = as.character(county_fips)) |&gt; \n  mutate(county_fips = \n           ifelse(nchar(county_fips) == 4, paste0(\"0\", county_fips), county_fips))\n\nPart b\nNow map Republican support by county. Let’s go straight to the discretized repub_20_categories variable, and a good color scale.\n\nggplot(elections_by_counties, aes(map_id = county_fips, fill = repub_20_categories)) +\n  geom_map(map = county_map) +\n  scale_fill_manual(values = rev(brewer.pal(10, \"RdBu\")), name = \"% Republican\") +\n  expand_limits(x = county_map$long, y = county_map$lat) +\n  theme_map() +\n  theme(legend.position = \"right\") + \n  coord_equal()\n\n\n\n\n\n\n\nExercise 10: Play around!\nConstruct county-level maps of median_rent and median_age.\nExercise 11: Choropleth maps with leaflet\nThough ggplot() is often better for this purpose, we can also make choropleth maps with leaflet(). If you’re curious, check out the leaflet documentation:\nhttps://rstudio.github.io/leaflet/choropleths.html",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Spatial Viz</span>"
    ]
  },
  {
    "objectID": "ica/ica-spatial.html#solutions",
    "href": "ica/ica-spatial.html#solutions",
    "title": "\n15  Spatial Viz\n",
    "section": "\n15.5 Solutions",
    "text": "15.5 Solutions\n\nClick for Solutions\nExample 1\nBoth addresses used between 0 and 450 therms per month. There seem to be two types of months – those with lower use around 50 therms and those with higher use around 300/400 therms.\n\nggplot(energy, aes(x = therms, fill = address)) + \n  geom_density(alpha = 0.5)\n\n\n\n\n\n\n\nExample 2\nEnergy use is seasonal, with higher usage in winter months. It seems that address a uses slightly more energy.\n\nggplot(energy, aes(y = therms, x = date, color = address)) + \n  geom_point()\n\n\n\n\n\n\nggplot(energy, aes(y = therms, x = date, color = address)) + \n  geom_line()\n\n\n\n\n\n\n\nExample 3\nAt both addresses, typical energy use increased after renovations.\n\nggplot(energy, aes(y = therms, x = renovated)) + \n  geom_boxplot() + \n  facet_wrap(~ address)\n\n\n\n\n\n\n# A density plot isn't very helpful for comparing typical therms in this example!\nggplot(energy, aes(x = therms, fill = renovated)) + \n  geom_density(alpha = 0.5) + \n  facet_wrap(~ address)\n\n\n\n\n\n\n\nExample 4\nlurking variable = outdoor temperature (as reflected by hdd)\n\n# It happened to be colder outside after renovations (higher hdd)\nggplot(energy, aes(y = hdd, x = renovated)) + \n  geom_boxplot() + \n  facet_wrap(~ address)\n\n\n\n\n\n\n# When controlling for outside temps (via hdd), energy use decreased post-renovation\nggplot(energy, aes(y = therms, x = hdd, color = renovated)) + \n  geom_point(alpha = 0.5) + \n  geom_smooth(method = \"lm\", se = FALSE) + \n  facet_wrap(~ address)\n\n\n\n\n\n\n\nExample 5\nBUT this was explained by a confounding or omitted or lurking variable: hdd (outdoor temperature)\n\nAfter renovation…\n\nit happened to be colder…\nwhich then leads to higher energy use.\n\nThus, when controlling for outdoor temps, renovations led to decreased energy use.\nExercise 3: Your turn\n\nleaflet(data = starbucks_mn) |&gt; \n  addTiles() |&gt; \n  addMarkers()\n\n\n\n\n\nExercise 3: A simple scatterplot\nIt would be nice to also have some actual reference maps of countries in the background.\n\nggplot(starbucks, aes(y = Latitude, x = Longitude)) + \n  geom_point(size = 0.5)\n\n\n\n\n\n\n\nExercise 6: A state and county-level map\nPart b\nAdjust the code below to make the plot! Remove the # to run it.\n\nggplot(midwest_boundaries) +\n  geom_sf() +\n  geom_point(\n    data = starbucks_midwest,\n    aes(x = Longitude, y = Latitude),\n    alpha = 0.7,\n    size = 0.2,\n    color = 'darkgreen'\n  ) +\n  theme_map()\n\n\n\n\n\n\n\nExercise 7: Contour maps\nEspecially when there are lots of point locations, and those locations start overlapping on a map, it can be tough to visualize areas of higher density. Consider the Starbucks locations in Canada, Mexico, and the US that we mapped earlier:\n\n# Point map (we made this earlier)\nggplot(cma_boundaries) + \n  geom_sf() + \n  geom_point(\n    data = starbucks_cma,\n    aes(x = Longitude, y = Latitude),\n    alpha = 0.3,\n    size = 0.2,\n    color = \"darkgreen\"\n  ) +\n  coord_sf(xlim = c(-179.14, -50), ylim = c(14.54, 83.11)) +\n  theme_map()\n\n\n\n\n\n\n\nNow check out the contour map.\n\n# What changed in the plot?\n# What changed in our code?!\nggplot(cma_boundaries) + \n  geom_sf() + \n  geom_density_2d(\n    data = starbucks_cma,\n    aes(x = Longitude, y = Latitude),\n    size = 0.2,\n    color = \"darkgreen\"\n  ) +\n  coord_sf(xlim = c(-179.14, -50), ylim = c(14.54, 83.11)) +\n  theme_map()\n\n\n\n\n\n\n\nExercises Part 3: Choropleth maps\nSpatial data isn’t always in the form of point locations! For example, recall the state and county-level data on presidential elections.\n\nelections_by_state &lt;-  read.csv(\"https://mac-stat.github.io/data/election_2020_by_state.csv\")\nelections_by_counties &lt;- read.csv(\"https://mac-stat.github.io/data/election_2020_county.csv\")\n\nIn these datasets, we’re interested in the overall election outcome by region (state or county), not the specific geographic location of some observation. Let’s wrangle our data first.\nWe’ll focus on just a few variables of interest, and create a new variable (repub_20_categories) that discretizes the repub_pct_20 variable into increments of 5 percentage points (for states) or 10 percentage points (for counties):\n\n# Don't worry about the code!\n\nelections_by_state &lt;- elections_by_state |&gt; \n  filter(state_abbr != \"DC\") |&gt; \n  select(state_name, state_abbr, repub_pct_20) |&gt; \n  mutate(repub_20_categories = \n           cut(repub_pct_20, \n               breaks = seq(30, 70, by = 5), \n               labels = c(\"30-34\", \"35-39\", \"40-44\", \"45-49\",\n                          \"50-54\", \"55-59\", \"60-64\", \"65-70\"), \n               include.lowest = TRUE))\n\nelections_by_counties &lt;- elections_by_counties |&gt; \n  select(state_name, state_abbr, county_name, county_fips,\n          repub_pct_20, median_age, median_rent) |&gt; \n  mutate(repub_20_categories = \n           cut(repub_pct_20, \n               breaks = seq(0, 100, by = 10),\n               labels = c(\"0-9\", \"10-19\", \"20-29\", \"30-39\", \"40-49\",\n                          \"50-59\", \"60-69\", \"70-79\", \"80-89\", \"90-100\"),\n               include.lowest = TRUE))\n\n# Add 0's at the beginning of any fips_code that's fewer than 5 numbers long\n# Don't worry about the syntax\nelections_by_counties &lt;- elections_by_counties |&gt; \n  mutate(county_fips = as.character(county_fips)) |&gt; \n  mutate(county_fips = \n           ifelse(nchar(county_fips) == 4, paste0(\"0\", county_fips), county_fips))\n\nExercise 8: State-level choropleth maps\nPart d\ngeom_map()\nExercise 10: Play around!\n\nggplot(elections_by_counties, aes(map_id = county_fips, fill = median_rent)) +\n  geom_map(map = county_map) +\n  expand_limits(x = county_map$long, y = county_map$lat) +\n  theme_map() +\n  theme(legend.position = \"right\") + \n  coord_equal() + \n  scale_fill_gradientn(name = \"median rent\", colors = c(\"white\", \"lightgreen\", \"darkgreen\"))\n\n\n\n\n\n\nggplot(elections_by_counties, aes(map_id = county_fips, fill = median_age)) +\n  geom_map(map = county_map) +\n  expand_limits(x = county_map$long, y = county_map$lat) +\n  theme_map() +\n  theme(legend.position = \"right\") + \n  coord_equal() + \n  scale_fill_gradientn(name = \"median age\", colors = terrain.colors(10))",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Spatial Viz</span>"
    ]
  },
  {
    "objectID": "ica/ica-effective.html",
    "href": "ica/ica-effective.html",
    "title": "\n16  Effective Viz\n",
    "section": "",
    "text": "16.1 Warm-up\nRecall: Benefits of Visualizations",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Effective Viz</span>"
    ]
  },
  {
    "objectID": "ica/ica-effective.html#warm-up",
    "href": "ica/ica-effective.html#warm-up",
    "title": "\n16  Effective Viz\n",
    "section": "",
    "text": "Understand what we’re working with:\n\nscales & typical outcomes\noutliers, i.e. unusual cases\npatterns & relationships\n\n\nRefine research questions & inform next steps of our analysis.\nCommunicate our findings and tell a story.\n\nNo One Right Viz\nThere is no one right way to visualize a data set, eg, check out the 100 ways used to visualize one dataset: https://100.datavizproject.com/ The visualized data was featured in this TidyTuesday!\nActivity: Plot Critique\nIn groups:\n\nScroll through the plots.\nIdentify at least 1 plot that you feel illuminates some important aspect of the data.\nIdentify at least 1 plot that does NOT illuminate the data in a good way.\nUgly, Bad, Wrong Viz\nOne way to identify effective viz is to understand what makes a viz ineffective. In the Fundamentals of Data Visualization, Wilke breaks down ineffective viz into 3 categories:\n\nWrong\nThe viz is “objectively incorrect”, as in the numbers / trends being displayed are wrong.\nBad\nThe viz is “unclear, confusing, overly complicated, or deceiving”.\nUgly\nThe viz correct and clear but The aesthetics are problematic.\n\nActivity: Critical Analysis\nLet’s try some critical analysis on specific examples. For your assigned viz, identify the following:\n\nThe story the viz is trying to communicate.\nWhether the viz is good, ugly, bad, wrong, or some combination.\nAreas for improvement.\n\n\n\n\n\nIMAGE 1. Source: N. Yau, Visualize This, 2011, p. 223-225.\n\n\n\n\n\n\n\nIMAGE 2. Source: N. Yau, Visualize This, 2011, p. 242.\n\n\n\n\n\n\n\nIMAGE 3. Climate change.\n\n\n\n\n\n\n\nIMAGE 4. Source: http://viz.wtf/\n\n\n\n\n\n\n\nIMAGE 5. Source: N. Yau, Visualize This, 2011, p. 220.\n\n\n\n\n\n\n\nIMAGE 6. Source: (https://www.reddit.com/r/dataisugly/comments/vlirox/0_1_19_20_39/)\n\n\n\nFollow-up to Climate Change Plot\n\n\n\n\nIMAGE 3. Climate change.\n\n\n\nEffective & Ineffective Viz Examples\n\nExamples of good viz:\n\nFlowingData’s “Best visualizations of…”\n\n\nExamples of bad viz:\n\nWTF Visualizations\nBad viz in the wild\n\n\nEffective Viz\nYou can take a whole course in Data Viz at Mac! The topic of effective viz is too big and nuanced to boil down into a simple list. Here are some basics:\nProfessionalism\nOnce you’re ready to “share out” your viz, it should have…\n\nmeaningful axis labels\na figure caption (depending upon where the viz will appear)\nAccessibility\nOnce you’re ready to “share out” your viz, it should…\n\nhave “alt text”, a written description of the viz that can be read out by a screen reader (video)\nuse a color palette that is distinguishable across common forms of color blindness\nDesign Details\nIn designing your viz, think about comparison. Good viz make it easy for people to perceive things that are similar and things that are different.\nEthics\nMichael Correll of Tableau Research (pdf) wrote “Data visualizations have a potentially enormous influence on how data are used to make decisions across all areas of human endeavor.” Thus ethics are critical from the data we use, to the plots we build, to the way in which we communicate this work. This is a very broad topic, and we’ll focus here on data visualization alone. Relatedly, and at a minimum:\n\nData viz should not mislead, i.e. “wrong” viz are unethical.**\nYet ethics in data viz goes much deeper. Correll describes three related principles to strive for:\n\nVisibility\nMake the invisible visible. Visualize hidden labor, hidden uncertainty, hidden impacts. Credit your sources, data and otherwise.\nPrivacy\nCollect data with empathy. Encourage small Data, anthropomorphize data, obfuscate data to protect privacy.\nPower\nChallenge structures of power. Support data due process, act as data advocates, pressure unethical analytical behavior.\n\n\nTo this list, Data Feminism authors Catherine D’Ignazio and Lauren F. Klein added:\n\nEmotion & Embodiment\nValue multiple forms of knowledge, including the knowledge that comes from people as living, feeling bodies in the world.\nPluralism\nThe most complete knowledge comes from synthesizing multiple perspectives\nContext\nData are not neutral or objective",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Effective Viz</span>"
    ]
  },
  {
    "objectID": "ica/ica-effective.html#exercises",
    "href": "ica/ica-effective.html#exercises",
    "title": "\n16  Effective Viz\n",
    "section": "\n16.2 Exercises",
    "text": "16.2 Exercises\nExercise 1: Professionalism\nLet’s examine weather in 3 Australian locations.\n\n# Load tidyverse package for plotting and wrangling\nlibrary(tidyverse)\n\n# Import the data\nweather &lt;- read.csv(\"https://mac-stat.github.io/data/weather_3_locations.csv\") |&gt; \n  mutate(date = as.Date(date))\n\nThe following plot is fine for things like homework or just playing around. But we’ll make it more “professional” looking below.\n\nggplot(weather, aes(y = temp3pm, x = temp9am, color = location)) + \n  geom_point()\n\n\n\n\n\n\n\nPart a\nReplace A, B, C, and D in the code below to:\n\nAdd a short, but descriptive title. Under 10 words.\nChange the x- and y-axis labels, currently just the names of the variables in the dataset. These should be short and include units.\nChange the legend title to “Location” (just for practice, not because it’s better than “location”).\n\n\nggplot(weather, aes(y = temp3pm, x = temp9am, color = location)) + \n  geom_point() + \n  labs(x = \"A\", y = \"B\", title = \"C\", color = \"D\")  \n\n\n\n\n\n\n\nPart b\nWhen we’re including our plot in an article, paper, book, or other similar outlet, we should (and are expected to) provide a more descriptive figure caption. Typically, this is instead of a title and is more descriptive of what exactly is being plotted.\n\nAdd a figure caption in the top of the chunk.\nInclude your x-axis, y-axis, and legend labels from Part a.\nRender your Rmd and check out how the figure caption appears.\n\n\nggplot(weather, aes(y = temp3pm, x = temp9am, color = location)) + \n  geom_point() + \n  labs(x = \"???\", y = \"???\", color = \"???\")  \n\n\n\n???\n\n\n\n\n\n\n\n???\n\n\n\n\nExercise 2: Accessibility\nLet’s now make a graphic more accessible.\n\nggplot(weather, aes(x = temp3pm, fill = location)) + \n  geom_density(alpha = 0.5) + \n  labs(x = \"3pm temperature (Celsius)\")  \n\n\n\nDensity plots of 3pm temperatures in 3 Australian locations.\n\n\n\nPart a\nLet’s add some alt text that can be picked up by screen readers. This is a great resource on writing alt text for data viz. In short, whereas figure captions are quick descriptions which assume that the viz is accessible, alt text is a longer description which assumes the viz is not accessible. Alt text should concisely articulate:\n\nWhat your visualization is (e.g. a density plot of 3pm temperatures in Hobart, Uluru, and Wollongong, Australia).\nA 1-sentence description of the most important takeaway.\nA link to your data source if it’s not already in the caption.\n\nAdd appropriate alt text at the top of the chunk, in fig-alt. Then render your qmd, and hover over the image in your rendered html file to check out the alt text.\n\nggplot(weather, aes(x = temp3pm, fill = location)) + \n  geom_density(alpha = 0.5) + \n  labs(x = \"3pm temperature (Celsius)\")  \n\n\n\nDensity plots of 3pm temperatures in 3 Australian locations.\n\n\n\n\n\n\n\nDensity plots of 3pm temperatures in 3 Australian locations.\n\n\n\n\nPart b\nColor is another important accessibility consideration. Let’s check out the color accessibility of our density plot.\n\nRun the ggplot() code from Part a in your console. The viz will pop up in the Plots tab.\nIn the Plots tab, click “Export” then “Save as image”. Save the image somewhere.\nNavigate to https://www.color-blindness.com/coblis-color-blindness-simulator/\n\nAbove the image of crayons (I think it’s crayons?), click “Choose file” and choose the plot file you just saved.\nClick the various simulator buttons (eg: Red-Weak/Protanomaly) to check out how the colors in this plot might appear to others.\nSummarize what you learn. What impact might our color choices have on one’s ability to interpret the viz?\nPart c\nWe can change our color schemes! There are many color-blind friendly palettes in R. In the future, we’ll set a default, more color-blind friendly color theme at the top of our Rmds. We can also do this individually for any plot that uses color. Run the chunks below to explore various options.\n\nggplot(weather, aes(x = temp3pm, fill = location)) + \n  geom_density(alpha = 0.5) + \n  labs(x = \"3pm temperature (Celsius)\") + \n  scale_fill_viridis_d()    \n\n\n\n\n\n\n\n\n# In the color scale line:\n# Change \"fill\" to \"color\" since we use color in the aes()\n# Change \"d\" (discrete) to \"c\" (continuous) since maxtemp is on a continuous scale\nggplot(weather, aes(y = temp3pm, x = temp9am, color = maxtemp)) + \n  geom_point(alpha = 0.5) + \n  labs(x = \"3pm temperature (Celsius)\") + \n  scale_color_viridis_c()\n\n\n\n\n\n\n\n\nExercise 3: Ethics\nLet’s scratch the surface of ethics in data viz. Central to this discussion is the consideration of impact.\nPart a\nAt a minimum, our data viz should not mislead. Reconsider the climate change example from above. Why is this plot unethical and what impact might it have on policy, public opinion, etc?\n\nPart b\nAgain, data viz ethical considerations go beyond whether or not a plot is misleading. As described in the warm-up, we need to consider: visibility, privacy, power, emotion & embodiment, pluralism, & context. Depending upon the audience and goals of a data viz, addressing these points might require more nuance. Mainly, the viz tools we’ve learned are a great base or foundation, but aren’t the only approaches to data viz. \nPick one or more of the following examples of data viz to discuss with your group. How do the approaches taken:\n\nemphasize one or more of: visibility, privacy, power, emotion, embodiment, pluralism, and/or context?\nimprove upon what we might be able to convey with a simpler bar chart, scatterplot, etc?\n\n\nExample: W.E.B. Du Bois (1868–1963)\nDu Bois (“Doo Boys”) was a “sociologist, socialist, historian, civil rights activist, Pan-Africanist, author, writer, and editor”1. He was also a pioneer in elevating emotion and embodiment in data visualization. For the Paris World Fair of 1900, Du Bois and his team of students from Atlanta University presented 60 data visualizations of the Black experience in America, less than 50 years after the abolishment of slavery. Du Bois noted: “I wanted to set down its aim and method in some outstanding way which would bring my work to notice by the thinking world.” That is, he wanted to increase the impact of his work by partnering technical visualizations with design that better connects to lived experiences. NOTE: This work uses language common to that time period and addresses the topic of slavery. Check out:\n\nA complete set of the data visualizations provided by Anthony Starks (@ajstarks).\nAn article by Allen Hillery (@AlDatavizguy).\n\n\nExample: One person’s experience with long COVID\nNYT article\n\nExample: Decolonizing data viz\nblog post\n\nExample: Visualizing climate change through art\nFutures North with Prof John Kim & Mac students (by Prof Kim, Mac research students)\n\nExample: Personal data collection\nDear Data\n\nPart c\nFor a deeper treatment of similar topics, and more examples, read Data Feminism.\n\nExercise 4: Critique\nPractice critiquing some more complicated data viz listed at Modern Data Science with R, Exercise 2.5.\nThink about the following questions:\n\nWhat story does the data graphic tell? What is the main message that you take away from it?\nCan the data graphic be described in terms of the Grammar of Graphics (frame, glyphs, aesthetics, facet, scale, guide)? If so, please describe.\nCritique and/or praise the visualization choices made by the designer. Do they work? Are they misleading? Thought-provoking? Are there things that you would have done differently?\n\n\nExercise 5: Design Details\nThis final exercise is just “food for thought”. It’s more of a discussion than an exercise, and gets into some of the finer design details and data viz theory. Go as deep or not deep as you’d like here.\nIn refining the details of our data viz, Visualize This and Storytelling with Data provide some of their guiding principles. But again, every context is different.\n\nPut yourself in a reader’s shoes. What parts of the data need explanation?\nShine a light on your data. Try to remove any “chart junk” that distracts from the data.\nVary color and style to emphasize the viz elements that are most important to the story you’re telling.\nIt is easier to judge length than it is to judge area or angles.\nBe thoughtful about how your categories are ordered for categorical data.\n\nGetting into even more of the nitty gritty, we need to be mindful of what geometric elements and aesthetics we use. The following elements/aesthetics are listed in roughly descending order of human ability to perceive and compare nearby objects:2\n\nPosition\nLength\nAngle\nDirection\nShape (but only a very few different shapes)\nArea\nVolume\nShade\nColor. (Color is the most difficult, because it is a 3-dimensional quantity.)\n\nFinally, here are some facts to keep in mind about visual perception from Now You See It.\nPart a: Selectivity\nVisual perception is selective, and our attention is often drawn to contrasts from the norm.\nImplication: We should design visualizations so that the features we want to highlight stand out in contrast from those that are not worth the audience’s attention.\nExample: What stands out in this example image? This is originally from C. Ware, Information Visualization: Perception for Design, 2004? Source: S. Few, Now You See It, 2009, p. 33.\n\nPart b: Familiarity\nOur eyes are drawn to familiar patterns. We observe what we know and expect.\nImplication: Visualizations work best when they display information as patterns that familiar and easy to spot.\nExample: Do you notice anything embedded in this rose image from coolbubble.com? Source: S. Few, Now You See It, 2009, p. 34.\n\nPart c: Revisit\nRevisit Part b. Do you notice anything in the shadows? Go to https://mac-stat.github.io/images/112/rose2.png for an image.\n\nWrapping up\nIf you finish early:\n\nWork on homework if not done already\nComplete any activities you haven’t finished yet, eg, spatial viz, the optional but fun exercises in the Multivariate viz and Bivariate viz activities.\nIf you’ve done all that, explore some datasets in TidyTuesday.",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Effective Viz</span>"
    ]
  },
  {
    "objectID": "ica/ica-effective.html#solutions",
    "href": "ica/ica-effective.html#solutions",
    "title": "\n16  Effective Viz\n",
    "section": "\n16.3 Solutions",
    "text": "16.3 Solutions\nThe exercises today are discussion based. There are no “solutions”. Happy to chat in office hours about any ideas here!",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Effective Viz</span>"
    ]
  },
  {
    "objectID": "ica/ica-effective.html#footnotes",
    "href": "ica/ica-effective.html#footnotes",
    "title": "\n16  Effective Viz\n",
    "section": "",
    "text": "https://en.wikipedia.org/wiki/W._E._B._Du_Bois↩︎\nB. S. Baumer, D. T. Kaplan, and N. J. Horton, Modern Data Science with R, 2017, p. 15.↩︎",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Effective Viz</span>"
    ]
  },
  {
    "objectID": "ica/ica-wrangling.html",
    "href": "ica/ica-wrangling.html",
    "title": "\n17  Wrangling\n",
    "section": "",
    "text": "17.1 Motivation\nRecall the elections data by U.S. county:\n# Load tidyverse & data\nlibrary(tidyverse)\nelections &lt;- read.csv(\"https://mac-stat.github.io/data/election_2020_county.csv\")\nWe’ve used data viz to explore some general patterns in the election outcomes. For example, a map!\n# Get a background map\nlibrary(socviz)\ndata(county_map)\n\n# Make a choropleth map\nlibrary(RColorBrewer)  # For the color scale\nlibrary(ggthemes) # For theme_map\nelections |&gt; \n  mutate(county_fips = as.character(county_fips)) |&gt; \n  mutate(county_fips = \n           ifelse(nchar(county_fips) == 4, paste0(\"0\", county_fips), county_fips)) |&gt; \n  ggplot(aes(map_id = county_fips, fill = cut(repub_pct_20, breaks = seq(0, 100, by = 10)))) +\n    geom_map(map = county_map) +\n    scale_fill_manual(values = rev(brewer.pal(10, \"RdBu\")), name = \"% Republican\") +\n    expand_limits(x = county_map$long, y = county_map$lat)  + \n    theme_map() +\n    theme(legend.position = \"right\") + \n    coord_equal()\nConsider some fairly basic follow-up questions, each of which we cannot answer precisely (or sometimes even at all) using our data viz tools:",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Wrangling</span>"
    ]
  },
  {
    "objectID": "ica/ica-wrangling.html#motivation",
    "href": "ica/ica-wrangling.html#motivation",
    "title": "\n17  Wrangling\n",
    "section": "",
    "text": "How many total people voted for the Democratic and Republican candidates in 2020?\nWhat about in each state?\nIn just the state of Minnesota:\n\nWhich counties had the highest and lowest Democratic vote in 2020?\nHow did the Democratic vote in each county change from 2016 to 2020?",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Wrangling</span>"
    ]
  },
  {
    "objectID": "ica/ica-wrangling.html#goals",
    "href": "ica/ica-wrangling.html#goals",
    "title": "\n17  Wrangling\n",
    "section": "\n17.2 Goals",
    "text": "17.2 Goals\nWe really cannot do anything with data (viz, modeling, etc) unless we can wrangle the data. The following is a typical quote. I agree with the 90% – data wrangling isn’t something we have to do before we can do data science, it is data science! But let’s rethink the 10% – data wrangling is a fun and empowering puzzle!\n\nThe goals of data wrangling are to explore how to:\n\nGet data into the tidy shape / format we need for analysis. For example, we might want to:\n\nkeep only certain observations\ndefine new variables\nreformat or “clean” existing variables\ncombine various datasets\nprocess “string” or text data\n\n\nNumerically (not just visually) explore and summarize various characteristics of the variables in our dataset.",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Wrangling</span>"
    ]
  },
  {
    "objectID": "ica/ica-wrangling.html#tools",
    "href": "ica/ica-wrangling.html#tools",
    "title": "\n17  Wrangling\n",
    "section": "\n17.3 Tools",
    "text": "17.3 Tools\nWe’ll continue to use packages that are part of the tidyverse which share a common general grammar and structure.",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Wrangling</span>"
    ]
  },
  {
    "objectID": "ica/ica-wrangling.html#warm-up",
    "href": "ica/ica-wrangling.html#warm-up",
    "title": "\n17  Wrangling\n",
    "section": "\n17.4 Warm-Up",
    "text": "17.4 Warm-Up\nThere are lots and lots of steps that can go into data wrangling, thus lots and lots of relevant R functions. BUT just 6 functions can get us very far. People refer to these as the 6 main wrangling verbs or functions:\n\nwhy “verbs”? in the tidyverse grammar, functions serve as action words\n\nthe 6 verbs are all stored in the dplyr package within the tidyverse\n\neach verb acts on a data frame and returns a data frame\n\n\n\nverb\naction\n\n\n\narrange\n\narrange the rows according to some column\n\n\n\nfilter\n\nfilter out or obtain a subset of the rows\n\n\n\nselect\n\nselect a subset of columns\n\n\n\nmutate\n\nmutate or create a column\n\n\n\nsummarize\ncalculate a numerical summary of a column\n\n\n\ngroup_by\n\ngroup the rows by a specified column\n\n\n\n\n\n17.4.1 Example 1\nWhich verb would help us…\n\nkeep only information about state names, county names, and the 2020 and 2016 Democratic support (not the 2012 results, demographics, etc)\nget only the data on Minnesota\ndefine a new variable which calculates the change in Democratic support from 2016 to 2020, using dem_pct_20 and dem_pct_16\nsort the counties from highest to lowest Democratic support\ndetermine the total number of votes cast across all counties\n\n17.4.2 Example 2: Select Columns\nTo get a sense for the code structure, let’s explore a couple verbs together. To start, let’s simplify our dataset to include only some variables of interest. Specifically, select() only the columns relevant to state names, county names, and the 2020 and 2016 Democratic support:\n\n# What's the first argument? The second?\nselect(elections, c(state_name, county_name, dem_pct_20, dem_pct_16))\n\nLet’s re-do this with the pipe function |&gt;:\n\nelections |&gt; \n  select(state_name, county_name, dem_pct_20, dem_pct_16)\n\n\n\n\n\n\n\nPipe Function |&gt;\n\n\n\n|&gt; “passes” objects, usually datasets, to a function:\nobject |&gt; function() is the same as function(object)\n\n\n\n17.4.3 Example 3: Filter Rows\nLet’s filter() out only the rows related to Minnesota (MN):\n\n# Without a pipe\nfilter(elections, state_name == \"Minnesota\")\n\n\n# With a pipe\nelections |&gt; \n  filter(state_name == \"Minnesota\")\n\n\n\n\n\n\n\n== vs =\n\n\n\nWe use a == b to check whether a matches b.\nWe use a = b to define that a is equal to b. We typically use = for this purpose inside a function, and &lt;- for this purpose outside a function.\n\n# Ex: \"=\" defines x\nx = 2\nx\n\n[1] 2\n\n\n\n# Ex: \"==\" checks whether x is/matches 3\nx == 3\n\n[1] FALSE\n\n\n\n\n\n17.4.4 Example 4: Filter and Select\nLet’s combine select() and filter() to create a new dataset with info about the county names, and 2020 and 2016 Democratic support among Minnesota counties.\n\n# Without pipes\nfilter(select(elections, c(state_name, county_name, dem_pct_20, dem_pct_16)), state_name == \"Minnesota\")\n\n\n# With pipes: all verbs in 1 row\nelections |&gt; select(state_name, county_name, dem_pct_20, dem_pct_16) |&gt; filter(state_name == \"Minnesota\")\n\n\n# With pipes: each verb in a new row\nelections |&gt; \n  select(state_name, county_name, dem_pct_20, dem_pct_16) |&gt; \n  filter(state_name == \"Minnesota\")\n\n\n# We can even do this with UN-tidyverse code in \"base\" R\nelections[elections$state_name == \"Minnesota\", c(1, 4, 8, 12)]\n\n\n\n\n\n\n\nReflection\n\n\n\nWhy will we typically use:\n\ntidyverse code\nthe pipe function |&gt;\n\neach verb on a new row\n\n\n\n\n17.4.5 Example 5: Order of Operations\nSometimes, the order of operations matters, eg, putting on socks then shoes produces a different result than putting on shoes then socks. However, sometimes order doesn’t matter, eg, pouring cereal into a bowl then adding milk produces the same result as pouring milk into a bow then adding cereal (though one order is obviously better than the other ;)) Above (also copied below), we selected some columns and then filtered some rows:\n\nelections |&gt; \n  select(state_name, county_name, dem_pct_20, dem_pct_16) |&gt; \n  filter(state_name == \"Minnesota\")\n\nWould we get the same result if we reversed select() and filter()? Think first, then try it.\n\n# Try it\n\n\n17.4.6 Example 6: Storing Results\nTypically:\n\nWe want to store our data wrangling results.\nIt’s good practice to do so under a new name. We want to preserve, thus don’t want to overwrite, the original data (especially if our code contains errors!!).\n\n\n# Store the results\nmn &lt;- elections |&gt; \n  select(state_name, county_name, dem_pct_20, dem_pct_16) |&gt; \n  filter(state_name == \"Minnesota\")\n\n# Always check it out to confirm it's what you want it to be!\nhead(mn)\n\n  state_name      county_name dem_pct_20 dem_pct_16\n1  Minnesota    Aitkin County      35.98      34.12\n2  Minnesota     Anoka County      47.79      41.01\n3  Minnesota    Becker County      33.96      30.47\n4  Minnesota  Beltrami County      47.24      40.76\n5  Minnesota    Benton County      32.70      28.33\n6  Minnesota Big Stone County      35.41      33.75\n\nnrow(mn)\n\n[1] 87\n\nnrow(elections)\n\n[1] 3109",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Wrangling</span>"
    ]
  },
  {
    "objectID": "ica/ica-wrangling.html#exercises",
    "href": "ica/ica-wrangling.html#exercises",
    "title": "\n17  Wrangling\n",
    "section": "\n17.5 Exercises",
    "text": "17.5 Exercises\nExercise 1: select Practice\nUse select() to create a simplified dataset that we’ll use throughout the exercises below.\n\nStore this dataset as elections_small.\nOnly keep the following variables: state_name, county_name, total_votes_20, repub_pct_20, dem_pct_20, total_votes_16, dem_pct_16\n\n\n\n# Define elections_small\nelections_small &lt;- elections |&gt;\n select(`state_name`, `county_name`, `total_votes_20`, `repub_pct_20`, `dem_pct_20`, `total_votes_16`, `dem_pct_16`)\n\n# Check out the first 6 rows to confirm your code did what you think it did!\nhead(elections)\n\n  state_name state_abbr historical    county_name county_fips total_votes_20\n1    Alabama         AL        red Autauga County        1001          27770\n2    Alabama         AL        red Baldwin County        1003         109679\n3    Alabama         AL        red Barbour County        1005          10518\n4    Alabama         AL        red    Bibb County        1007           9595\n5    Alabama         AL        red  Blount County        1009          27588\n6    Alabama         AL        red Bullock County        1011           4613\n  repub_pct_20 dem_pct_20 winner_20 total_votes_16 repub_pct_16 dem_pct_16\n1        71.44      27.02     repub          24661        73.44      23.96\n2        76.17      22.41     repub          94090        77.35      19.57\n3        53.45      45.79     repub          10390        52.27      46.66\n4        78.43      20.70     repub           8748        76.97      21.42\n5        89.57       9.57     repub          25384        89.85       8.47\n6        24.84      74.70       dem           4701        24.23      75.09\n  winner_16 total_votes_12 repub_pct_12 dem_pct_12 winner_12 total_population\n1     repub          23909        72.63      26.58     repub            54907\n2     repub          84988        77.39      21.57     repub           187114\n3     repub          11459        48.34      51.25       dem            27321\n4     repub           8391        73.07      26.22     repub            22754\n5     repub          23980        86.49      12.35     repub            57623\n6       dem           5318        23.51      76.31       dem            10746\n  percent_white percent_black percent_asian percent_hispanic per_capita_income\n1            76            18             1                2             24571\n2            83             9             1                4             26766\n3            46            46             0                5             16829\n4            75            22             0                2             17427\n5            88             1             0                8             20730\n6            22            71             0                6             18628\n  median_rent median_age\n1         668       37.5\n2         693       41.5\n3         382       38.3\n4         351       39.4\n5         403       39.6\n6         276       39.6\n\n\nExercise 2: filter Demo\nWhereas select() selects certain variables or columns, filter() keeps certain units of observation or rows relative to their outcome on certain variables. To this end, we must:\n\nIdentify the variable(s) that are relevant to the filter.\n\nUse a “logical comparison operator” to define which values of the variable to keep:\n\n\nsymbol\nmeaning\n\n\n\n==\nequal to\n\n\n!=\nnot equal to\n\n\n&gt;\ngreater than\n\n\n&gt;=\ngreater than or equal to\n\n\n&lt;\nless than\n\n\n&lt;=\nless than or equal to\n\n\n%in% c(???, ???)\na list of multiple values\n\n\n\n\nUse quotes \"\" when specifying outcomes of interest for a categorical variable.\n\n\n\n\n\n\n\nCommenting/Uncommenting Code\n\n\n\nTo comment/uncomment several lines of code at once, highlight them then click ctrl/cmd+shift+c.\n\n\n\n# Keep only data on counties in Hawaii\nelections_small |&gt;\n filter(state_name%in%c(\"Hawaii\"))\n\n  state_name     county_name total_votes_20 repub_pct_20 dem_pct_20\n1     Hawaii   Hawaii County          87814        30.63      66.88\n2     Hawaii Honolulu County         382114        35.66      62.51\n3     Hawaii    Kauai County          33497        34.58      63.36\n4     Hawaii     Maui County          71044        31.14      66.59\n  total_votes_16 dem_pct_16\n1          64865      63.61\n2         285683      61.48\n3          26335      62.49\n4          51942      64.45\n\n\n\n# Keep only data on counties in Hawaii and Delaware\nelections_small |&gt;\n  filter(state_name %in% c(\"Hawaii\", \"Delaware\"))\n\n  state_name       county_name total_votes_20 repub_pct_20 dem_pct_20\n1   Delaware       Kent County          87025        47.12      51.19\n2   Delaware New Castle County         287633        30.72      67.81\n3   Delaware     Sussex County         129352        55.07      43.82\n4     Hawaii     Hawaii County          87814        30.63      66.88\n5     Hawaii   Honolulu County         382114        35.66      62.51\n6     Hawaii      Kauai County          33497        34.58      63.36\n7     Hawaii       Maui County          71044        31.14      66.59\n  total_votes_16 dem_pct_16\n1          74253      44.91\n2         261468      62.30\n3         105814      37.17\n4          64865      63.61\n5         285683      61.48\n6          26335      62.49\n7          51942      64.45\n\n\n\n# Keep only data on counties where the Republican got MORE THAN 93.97% of the vote in 2020\n# THINK: What variable is relevant here?\nelections_small |&gt;\n filter(repub_pct_20&gt;93.97)\n\n  state_name    county_name total_votes_20 repub_pct_20 dem_pct_20\n1      Texas  Borden County            416        95.43       3.85\n2      Texas    King County            159        94.97       5.03\n3      Texas Roberts County            550        96.18       3.09\n  total_votes_16 dem_pct_16\n1            365       8.49\n2            159       3.14\n3            550       3.64\n\n\n\n# Keep only data on counties where the Republican got AT LEAST 93.97% of the vote in 2020\n# This should have 1 more row (observation) than your answer above\nelections_small |&gt;\n filter(repub_pct_20&gt;=93.97)\n\n  state_name     county_name total_votes_20 repub_pct_20 dem_pct_20\n1    Montana Garfield County            813        93.97       5.04\n2      Texas   Borden County            416        95.43       3.85\n3      Texas     King County            159        94.97       5.03\n4      Texas  Roberts County            550        96.18       3.09\n  total_votes_16 dem_pct_16\n1            715       4.76\n2            365       8.49\n3            159       3.14\n4            550       3.64\n\n\nWe can also filter with respect to 2 rules! Here, think what variables are relevant.\n\n# Keep only data on counties in Texas where the Democrat got more than 65% of the vote in 2020\n# Do this 2 ways.\n# Method 1: 2 filters with 1 condition each\nelections_small |&gt;\n filter(state_name%in%c(\"Texas\")) |&gt;\n filter(dem_pct_20&gt;65)\n\n  state_name     county_name total_votes_20 repub_pct_20 dem_pct_20\n1      Texas  El Paso County         267215        31.56      66.66\n2      Texas Presidio County           2217        32.52      65.99\n3      Texas   Travis County         610349        26.43      71.41\n4      Texas   Zavala County           4379        34.03      65.40\n  total_votes_16 dem_pct_16\n1         210458      69.14\n2           2203      66.18\n3         462511      66.26\n4           3390      77.67\n\n# Method 2: 1 filter with 2 conditions\nelections_small |&gt;\n filter(state_name%in%c(\"Texas\"), dem_pct_20&gt;65)\n\n  state_name     county_name total_votes_20 repub_pct_20 dem_pct_20\n1      Texas  El Paso County         267215        31.56      66.66\n2      Texas Presidio County           2217        32.52      65.99\n3      Texas   Travis County         610349        26.43      71.41\n4      Texas   Zavala County           4379        34.03      65.40\n  total_votes_16 dem_pct_16\n1         210458      69.14\n2           2203      66.18\n3         462511      66.26\n4           3390      77.67\n\n\nExercise 3: arrange Demo\narrange() arranges or sorts the rows in a dataset according to a given column or variable, in ascending or descending order:\narrange(variable), arrange(desc(variable))\n\n# Arrange the counties in elections_small from lowest to highest percentage of 2020 Republican support\n# Print out just the first 6 rows\nelections_small |&gt;\n  arrange(desc(repub_pct_20)) |&gt;\n  head()\n\n  state_name      county_name total_votes_20 repub_pct_20 dem_pct_20\n1      Texas   Roberts County            550        96.18       3.09\n2      Texas    Borden County            416        95.43       3.85\n3      Texas      King County            159        94.97       5.03\n4    Montana  Garfield County            813        93.97       5.04\n5      Texas Glasscock County            653        93.57       5.97\n6   Nebraska     Grant County            402        93.28       4.98\n  total_votes_16 dem_pct_16\n1            550       3.64\n2            365       8.49\n3            159       3.14\n4            715       4.76\n5            602       5.65\n6            394       5.08\n\n\n\n# Arrange the counties in elections_small from highest to lowest percentage of 2020 Republican support\n# Print out just the first 6 rows\nelections_small |&gt;\n  arrange(repub_pct_20) |&gt;\n  head()\n\n            state_name            county_name total_votes_20 repub_pct_20\n1 District of Columbia   District of Columbia         344356         5.40\n2             Maryland Prince George's County         424855         8.73\n3             Maryland         Baltimore city         237461        10.69\n4             Virginia        Petersburg city          14118        11.22\n5             New York        New York County         694904        12.26\n6           California   San Francisco County         443458        12.72\n  dem_pct_20 total_votes_16 dem_pct_16\n1      92.15         280272      92.85\n2      89.26         351091      89.33\n3      87.28         208980      85.44\n4      87.75          13717      87.52\n5      86.78         591368      87.17\n6      85.27         365295      85.53\n\n\nExercise 4: mutate Demo\nmutate() can either transform / mutate an existing variable (column), or define a new variable based on existing ones.\nPart a\n\n# creates a new varialbe that is the difference between the support rate\nelections_small |&gt;\n  mutate(diff_20 = repub_pct_20 - dem_pct_20) |&gt;\n  head()\n\n  state_name    county_name total_votes_20 repub_pct_20 dem_pct_20\n1    Alabama Autauga County          27770        71.44      27.02\n2    Alabama Baldwin County         109679        76.17      22.41\n3    Alabama Barbour County          10518        53.45      45.79\n4    Alabama    Bibb County           9595        78.43      20.70\n5    Alabama  Blount County          27588        89.57       9.57\n6    Alabama Bullock County           4613        24.84      74.70\n  total_votes_16 dem_pct_16 diff_20\n1          24661      23.96   44.42\n2          94090      19.57   53.76\n3          10390      46.66    7.66\n4           8748      21.42   57.73\n5          25384       8.47   80.00\n6           4701      75.09  -49.86\n\n\n\n# creates a new variable that is the number of votes for republicans\nelections_small |&gt;\n  mutate(repub_votes_20 = round(total_votes_20 * repub_pct_20/100)) |&gt;\n  head()\n\n  state_name    county_name total_votes_20 repub_pct_20 dem_pct_20\n1    Alabama Autauga County          27770        71.44      27.02\n2    Alabama Baldwin County         109679        76.17      22.41\n3    Alabama Barbour County          10518        53.45      45.79\n4    Alabama    Bibb County           9595        78.43      20.70\n5    Alabama  Blount County          27588        89.57       9.57\n6    Alabama Bullock County           4613        24.84      74.70\n  total_votes_16 dem_pct_16 repub_votes_20\n1          24661      23.96          19839\n2          94090      19.57          83542\n3          10390      46.66           5622\n4           8748      21.42           7525\n5          25384       8.47          24711\n6           4701      75.09           1146\n\n\n\n# creates a new variable of republicans winning\nelections_small |&gt;\n  mutate(repub_win_20 = repub_pct_20 &gt; dem_pct_20) |&gt;\n  head()\n\n  state_name    county_name total_votes_20 repub_pct_20 dem_pct_20\n1    Alabama Autauga County          27770        71.44      27.02\n2    Alabama Baldwin County         109679        76.17      22.41\n3    Alabama Barbour County          10518        53.45      45.79\n4    Alabama    Bibb County           9595        78.43      20.70\n5    Alabama  Blount County          27588        89.57       9.57\n6    Alabama Bullock County           4613        24.84      74.70\n  total_votes_16 dem_pct_16 repub_win_20\n1          24661      23.96         TRUE\n2          94090      19.57         TRUE\n3          10390      46.66         TRUE\n4           8748      21.42         TRUE\n5          25384       8.47         TRUE\n6           4701      75.09        FALSE\n\n\nPart b\n\n# You try\n# Define a variable that calculates the change in Dem support in 2020 vs 2016\nelections_small |&gt; \n  mutate(dem_change = dem_pct_20 - dem_pct_16) |&gt; \n  head()\n\n  state_name    county_name total_votes_20 repub_pct_20 dem_pct_20\n1    Alabama Autauga County          27770        71.44      27.02\n2    Alabama Baldwin County         109679        76.17      22.41\n3    Alabama Barbour County          10518        53.45      45.79\n4    Alabama    Bibb County           9595        78.43      20.70\n5    Alabama  Blount County          27588        89.57       9.57\n6    Alabama Bullock County           4613        24.84      74.70\n  total_votes_16 dem_pct_16 dem_change\n1          24661      23.96       3.06\n2          94090      19.57       2.84\n3          10390      46.66      -0.87\n4           8748      21.42      -0.72\n5          25384       8.47       1.10\n6           4701      75.09      -0.39\n\n\n\n# You try\n# Define a variable that determines whether the Dem support was higher in 2020 than in 2016 (TRUE/FALSE)\nelections_small |&gt; mutate(dem_up = dem_pct_20 &gt; dem_pct_16) |&gt; head()\n\n  state_name    county_name total_votes_20 repub_pct_20 dem_pct_20\n1    Alabama Autauga County          27770        71.44      27.02\n2    Alabama Baldwin County         109679        76.17      22.41\n3    Alabama Barbour County          10518        53.45      45.79\n4    Alabama    Bibb County           9595        78.43      20.70\n5    Alabama  Blount County          27588        89.57       9.57\n6    Alabama Bullock County           4613        24.84      74.70\n  total_votes_16 dem_pct_16 dem_up\n1          24661      23.96   TRUE\n2          94090      19.57   TRUE\n3          10390      46.66  FALSE\n4           8748      21.42  FALSE\n5          25384       8.47   TRUE\n6           4701      75.09  FALSE\n\n\nExercise 5: Pipe Series\nLet’s now combine these verbs into a pipe series!\nPart a\n\n\n\n\n\n\nThink then Run\n\n\n\nBEFORE running the below chunk, what do you think it will produce?\n\n\n\nelections_small |&gt;\n  filter(state_name == \"Wisconsin\",\n         repub_pct_20 &lt; dem_pct_20) |&gt;\n  arrange(desc(total_votes_20)) |&gt;\n  head()\n\n  state_name       county_name total_votes_20 repub_pct_20 dem_pct_20\n1  Wisconsin  Milwaukee County         458971        29.27      69.13\n2  Wisconsin       Dane County         344791        22.85      75.46\n3  Wisconsin       Rock County          85360        43.51      54.66\n4  Wisconsin  La Crosse County          67884        42.25      55.75\n5  Wisconsin Eau Claire County          58275        43.49      54.26\n6  Wisconsin    Portage County          40603        47.53      50.31\n  total_votes_16 dem_pct_16\n1         434970      66.44\n2         304729      71.38\n3          75043      52.42\n4          62785      51.61\n5          54080      50.43\n6          38123      48.59\n\n\nPart b\n\n\n\n\n\n\nThink then Run\n\n\n\nBEFORE trying, what do you think will happen if you change the order of filter and arrange:\n\nthe results will be the same\nwe’ll get an error\nwe won’t get an error, but the results will be different\n\n\n\n\n# Now try it. Change the order of filter and arrange below.\nelections_small |&gt;\n  filter(state_name == \"Wisconsin\",\n         repub_pct_20 &lt; dem_pct_20) |&gt;\n  arrange(desc(total_votes_20)) |&gt;\n  head()\n\n  state_name       county_name total_votes_20 repub_pct_20 dem_pct_20\n1  Wisconsin  Milwaukee County         458971        29.27      69.13\n2  Wisconsin       Dane County         344791        22.85      75.46\n3  Wisconsin       Rock County          85360        43.51      54.66\n4  Wisconsin  La Crosse County          67884        42.25      55.75\n5  Wisconsin Eau Claire County          58275        43.49      54.26\n6  Wisconsin    Portage County          40603        47.53      50.31\n  total_votes_16 dem_pct_16\n1         434970      66.44\n2         304729      71.38\n3          75043      52.42\n4          62785      51.61\n5          54080      50.43\n6          38123      48.59\n\n\nPart c\nSo the order of filter() and arrange() did not matter – rerranging them produces the same results. BUT what is one advantage of filtering before arranging?\nPart d\n\n\n\n\n\n\nThink then Run\n\n\n\nBEFORE running the below chunk, what do you think it will produce?\n\n\n\nelections_small |&gt;\n  filter(state_name == \"Delaware\") |&gt;\n  mutate(repub_win_20 = repub_pct_20 &gt; dem_pct_20) |&gt;\n  select(county_name, repub_pct_20, dem_pct_20, repub_win_20)\n\n        county_name repub_pct_20 dem_pct_20 repub_win_20\n1       Kent County        47.12      51.19        FALSE\n2 New Castle County        30.72      67.81        FALSE\n3     Sussex County        55.07      43.82         TRUE\n\n\nPart e\n\n\n\n\n\n\nThink then Run\n\n\n\nBEFORE trying, what do you think will happen if you change the order of mutate and select:\n\nthe results will be the same\nwe’ll get an error\nwe won’t get an error, but the results will be different\n\n\n\n\n# Now try it. Change the order of mutate and select below.\nelections_small |&gt;\n  filter(state_name == \"Delaware\") |&gt;\n  mutate(repub_win_20 = repub_pct_20 &gt; dem_pct_20) |&gt;\n  select(county_name, repub_pct_20, dem_pct_20, repub_win_20)\n\n        county_name repub_pct_20 dem_pct_20 repub_win_20\n1       Kent County        47.12      51.19        FALSE\n2 New Castle County        30.72      67.81        FALSE\n3     Sussex County        55.07      43.82         TRUE\n\n\nExercise 6: DIY Pipe Series\nWe’ve now learned 4 of the 6 wrangling verbs: select, filter, mutate, arrange. Let’s practice combining these into pipe series. Here are some hot tips:\n\nBefore writing any code, translate the prompt: how many distinct wrangling steps are needed and what verb do we need in each step?\nAdd each verb one at a time – don’t try writing a whole chunk at once.\n\nPart a\nShow just the counties in Minnesota and their Democratic 2020 vote percentage, from highest to lowest. Your answer should have just 2 columns.\n\nelections_small |&gt; filter(state_name == \"Minnesota\") |&gt; select(county_name, dem_pct_20) |&gt; arrange(desc(dem_pct_20))\n\n                county_name dem_pct_20\n1             Ramsey County      71.50\n2           Hennepin County      70.46\n3               Cook County      65.58\n4          St. Louis County      56.64\n5             Dakota County      55.73\n6            Olmsted County      54.16\n7         Washington County      53.46\n8         Blue Earth County      50.84\n9               Clay County      50.74\n10              Lake County      50.64\n11          Nicollet County      50.31\n12           Carlton County      49.58\n13            Winona County      49.07\n14              Rice County      48.76\n15          Mahnomen County      48.26\n16             Anoka County      47.79\n17          Beltrami County      47.24\n18            Carver County      46.37\n19             Mower County      46.00\n20             Scott County      45.52\n21           Houston County      42.42\n22           Goodhue County      41.23\n23          Freeborn County      40.96\n24            Norman County      40.80\n25            Itasca County      40.61\n26       Koochiching County      38.41\n27          Watonwan County      38.20\n28           Kittson County      38.12\n29           Stevens County      37.80\n30           Stearns County      37.58\n31          Fillmore County      37.48\n32            Steele County      37.47\n33         Kandiyohi County      36.12\n34            Aitkin County      35.98\n35              Lyon County      35.94\n36     Lac qui Parle County      35.79\n37           Wabasha County      35.78\n38             Grant County      35.58\n39          Traverse County      35.46\n40         Big Stone County      35.41\n41        Pennington County      35.29\n42              Pope County      35.27\n43              Polk County      34.88\n44              Cass County      34.68\n45            Wright County      34.49\n46           Hubbard County      34.42\n47             Swift County      34.35\n48         Crow Wing County      34.17\n49           Chisago County      34.15\n50            Becker County      33.96\n51              Pine County      33.87\n52          Le Sueur County      33.73\n53          Chippewa County      33.67\n54            Nobles County      33.65\n55            Waseca County      33.65\n56             Dodge County      33.47\n57        Otter Tail County      32.85\n58            Benton County      32.70\n59           Douglas County      32.56\n60             Brown County      32.48\n61         Sherburne County      32.48\n62         Faribault County      31.98\n63          Red Lake County      31.47\n64          Renville County      30.71\n65            McLeod County      30.64\n66   Yellow Medicine County      30.54\n67           Lincoln County      30.08\n68        Cottonwood County      30.03\n69           Kanabec County      30.02\n70            Martin County      30.02\n71           Jackson County      29.99\n72        Mille Lacs County      29.98\n73            Wilkin County      29.91\n74              Rock County      29.69\n75            Murray County      29.60\n76            Isanti County      29.45\n77            Sibley County      28.60\n78            Meeker County      28.58\n79           Redwood County      28.43\n80 Lake of the Woods County      27.87\n81        Clearwater County      26.76\n82         Pipestone County      26.44\n83            Wadena County      26.35\n84            Roseau County      25.98\n85          Marshall County      25.33\n86              Todd County      24.79\n87          Morrison County      22.33\n\n\nPart b\nCreate a new dataset named mn_wi that sorts the counties in Minnesota and Wisconsin from lowest to highest in terms of the change in Democratic vote percentage in 2020 vs 2016. This dataset should include the following variables (and only these variables): state_name, county_name, dem_pct_20, dem_pct_16, and a variable measuring the change in Democratic vote percentage in 2020 vs 2016.\n\n# Define the dataset\n# Only store the results once you're confident that they're correct\nmn_wi &lt;- elections_small |&gt;\n  filter(state_name %in% c(\"Minnesota\", \"Wisconsin\")) |&gt;\n  mutate(dem_change = dem_pct_20 - dem_pct_16) |&gt;\n  select(state_name, county_name, dem_pct_20, dem_pct_16, dem_change) |&gt;\n  arrange(dem_change)\n\n# Check out the first 6 rows to confirm your results\nhead(mn_wi)\n\n  state_name        county_name dem_pct_20 dem_pct_16 dem_change\n1  Minnesota     Stevens County      37.80      39.55      -1.75\n2  Wisconsin      Forest County      34.06      35.12      -1.06\n3  Wisconsin    Kewaunee County      32.87      33.73      -0.86\n4  Wisconsin       Clark County      30.37      31.19      -0.82\n5  Wisconsin       Adams County      36.63      37.40      -0.77\n6  Wisconsin Trempealeau County      40.86      41.57      -0.71\n\n\nPart c\nConstruct and discuss a plot of the county-level change in Democratic vote percent in 2020 vs 2016, and how this differs between Minnesota and Wisconsin.\n\nmn_wi |&gt; \n  ggplot(aes(x = dem_change, fill = state_name)) +\n  geom_histogram(position = \"identity\", alpha = 0.6, bins = 30) +\n  facet_wrap(~ state_name, scales = \"free_y\") +\n  theme_minimal() +\n  labs(x = \"Change in Democratic Vote % (2020 vs 2016)\", y = \"Number of Counties\")\n\n\n\n\n\n\n\nExercise 7: summarize Demo\n6 verbs: select, filter, arrange, mutate, summarize, group_by\nLet’s talk about the last 2 verbs. summarize() (or equivalently summarise()) takes an entire data frame as input and outputs a single row with one or more summary statistics. For each chunk below, indicate what the code does.\n\n# present the median repub supportive rate\nelections_small |&gt;\n  summarize(median(repub_pct_20))\n\n  median(repub_pct_20)\n1                68.29\n\n\n\n# same as above\nelections_small |&gt;\n  summarize(median_repub = median(repub_pct_20))\n\n  median_repub\n1        68.29\n\n\n\n# add a new var of total votes\nelections_small |&gt;\n  summarize(median_repub = median(repub_pct_20), total_votes = sum(total_votes_20))\n\n  median_repub total_votes\n1        68.29   157949293\n\n\nExercise 8: summarize + group_by demo\nFinally, group_by() groups the units of observation or rows of a data frame by a specified set of variables. Alone, this function doesn’t change the appearance of our dataset or seem to do anything at all:\n\nelections_small |&gt;\n  group_by(state_name)\n\n# A tibble: 3,109 × 7\n# Groups:   state_name [50]\n   state_name county_name  total_votes_20 repub_pct_20 dem_pct_20 total_votes_16\n   &lt;chr&gt;      &lt;chr&gt;                 &lt;int&gt;        &lt;dbl&gt;      &lt;dbl&gt;          &lt;int&gt;\n 1 Alabama    Autauga Cou…          27770         71.4      27.0           24661\n 2 Alabama    Baldwin Cou…         109679         76.2      22.4           94090\n 3 Alabama    Barbour Cou…          10518         53.4      45.8           10390\n 4 Alabama    Bibb County            9595         78.4      20.7            8748\n 5 Alabama    Blount Coun…          27588         89.6       9.57          25384\n 6 Alabama    Bullock Cou…           4613         24.8      74.7            4701\n 7 Alabama    Butler Coun…           9488         57.5      41.8            8685\n 8 Alabama    Calhoun Cou…          50983         68.8      29.8           47376\n 9 Alabama    Chambers Co…          15284         57.3      41.6           13778\n10 Alabama    Cherokee Co…          12301         86.0      13.2           10503\n# ℹ 3,099 more rows\n# ℹ 1 more variable: dem_pct_16 &lt;dbl&gt;\n\n\nThough it does change the underlying structure of the dataset:\n\n# Check out the structure before and after group_by\nelections_small |&gt;\n  class()\n\n[1] \"data.frame\"\n\nelections_small |&gt;\n  group_by(state_name) |&gt;\n  class()\n\n[1] \"grouped_df\" \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\nWhere it really shines is in partnership with summarize().\n\n# What does this do?\n# (What if we didn't use group_by?)\nelections_small |&gt;\n  group_by(state_name) |&gt;\n  summarize(median_repub = median(repub_pct_20), total_votes = sum(total_votes_20))\n\n# A tibble: 50 × 3\n   state_name           median_repub total_votes\n   &lt;chr&gt;                       &lt;dbl&gt;       &lt;int&gt;\n 1 Alabama                      70.6     2323304\n 2 Arizona                      57.9     3387326\n 3 Arkansas                     72.1     1219069\n 4 California                   44.8    17495906\n 5 Colorado                     56.2     3256953\n 6 Connecticut                  41.0     1824280\n 7 Delaware                     47.1      504010\n 8 District of Columbia          5.4      344356\n 9 Florida                      64.6    11067456\n10 Georgia                      68       4997716\n# ℹ 40 more rows\n\n\n\n\n\n\n\n\nReflect\n\n\n\nNotice that group_by() with summarize() produces new data frame or tibble! But the units of observation are now states instead of counties within states.\n\n\nExercise 9: DIY\nLet’s practice (some of) our 6 verbs: select, filter, arrange, mutate, summarize, group_by Remember:\n\nBefore writing any code, translate the given prompts: how many distinct wrangling steps are needed and what verb do we need in each step?\nAdd each verb one at a time.\n\nPart a\nNOTE: Part a is a challenge exercise. If you get really stuck, move on to Part b which is the same overall question, but with hints.\n\n# Sort the *states* from the most to least total votes cast in 2020\nelections_small |&gt; group_by(state_name) |&gt; summarize(total_votes_20 = sum(total_votes_20, na.rm = TRUE)) |&gt; arrange(desc(total_votes_20))\n\n# A tibble: 50 × 2\n   state_name     total_votes_20\n   &lt;chr&gt;                   &lt;int&gt;\n 1 California           17495906\n 2 Texas                11317911\n 3 Florida              11067456\n 4 New York              8616205\n 5 Pennsylvania          6925255\n 6 Illinois              6038850\n 7 Ohio                  5922202\n 8 Michigan              5539302\n 9 North Carolina        5524801\n10 Georgia               4997716\n# ℹ 40 more rows\n\n\n\n# In 2020, what were the total number of votes for the Democratic candidate and the total number of votes for the Republican candidate in each *state*?\nelections_small |&gt;\n  mutate(dem_votes_20 = round(total_votes_20 * dem_pct_20 / 100),\n         repub_votes_20 = round(total_votes_20 * repub_pct_20 / 100)) |&gt;\n  group_by(state_name) |&gt;\n  summarize(total_dem_votes = sum(dem_votes_20, na.rm = TRUE), total_repub_votes = sum(repub_votes_20, na.rm = TRUE))\n\n# A tibble: 50 × 3\n   state_name           total_dem_votes total_repub_votes\n   &lt;chr&gt;                          &lt;dbl&gt;             &lt;dbl&gt;\n 1 Alabama                       849664           1441155\n 2 Arizona                      1672127           1661671\n 3 Arkansas                      423919            760641\n 4 California                  11109642           6006031\n 5 Colorado                     1804393           1364627\n 6 Connecticut                  1080677            715315\n 7 Delaware                      296274            200601\n 8 District of Columbia          317324             18595\n 9 Florida                      5297131           5668600\n10 Georgia                      2473661           2461869\n# ℹ 40 more rows\n\n\n\n# What states did the Democratic candidate win in 2020?\nelections_small |&gt;\n  mutate(dem_votes_20 = round(total_votes_20 * dem_pct_20 / 100),\n         repub_votes_20 = round(total_votes_20 * repub_pct_20 / 100)) |&gt;\n  group_by(state_name) |&gt;\n  summarize(total_dem_votes = sum(dem_votes_20, na.rm = TRUE), total_repub_votes = sum(repub_votes_20, na.rm = TRUE)) |&gt;\n  filter(total_dem_votes &gt; total_repub_votes)\n\n# A tibble: 26 × 3\n   state_name           total_dem_votes total_repub_votes\n   &lt;chr&gt;                          &lt;dbl&gt;             &lt;dbl&gt;\n 1 Arizona                      1672127           1661671\n 2 California                  11109642           6006031\n 3 Colorado                     1804393           1364627\n 4 Connecticut                  1080677            715315\n 5 Delaware                      296274            200601\n 6 District of Columbia          317324             18595\n 7 Georgia                      2473661           2461869\n 8 Hawaii                        366121            196865\n 9 Illinois                     3471916           2446931\n10 Maine                         430466            359897\n# ℹ 16 more rows\n\n\nPart b\n\n# Sort the states from the most to least total votes cast in 2020\n# HINT: Calculate the total number of votes in each state, then sort\nelections_small |&gt; group_by(state_name) |&gt; summarize(total_votes_20 = sum(total_votes_20, na.rm = TRUE)) |&gt; arrange(desc(total_votes_20))\n\n# A tibble: 50 × 2\n   state_name     total_votes_20\n   &lt;chr&gt;                   &lt;int&gt;\n 1 California           17495906\n 2 Texas                11317911\n 3 Florida              11067456\n 4 New York              8616205\n 5 Pennsylvania          6925255\n 6 Illinois              6038850\n 7 Ohio                  5922202\n 8 Michigan              5539302\n 9 North Carolina        5524801\n10 Georgia               4997716\n# ℹ 40 more rows\n\n\n\n# In 2020, what were the total number of votes for the Democratic candidate and the total number of votes for the Republican candidate in each state?\n# HINT: First calculate the number of Dem and Repub votes in each *county*\n# Then group and summarize these by state\nelections_small |&gt;\n  mutate(dem_votes_20 = round(total_votes_20 * dem_pct_20 / 100),\n         repub_votes_20 = round(total_votes_20 * repub_pct_20 / 100)) |&gt;\n  group_by(state_name) |&gt;\n  summarize(total_dem_votes = sum(dem_votes_20, na.rm = TRUE), total_repub_votes = sum(repub_votes_20, na.rm = TRUE))\n\n# A tibble: 50 × 3\n   state_name           total_dem_votes total_repub_votes\n   &lt;chr&gt;                          &lt;dbl&gt;             &lt;dbl&gt;\n 1 Alabama                       849664           1441155\n 2 Arizona                      1672127           1661671\n 3 Arkansas                      423919            760641\n 4 California                  11109642           6006031\n 5 Colorado                     1804393           1364627\n 6 Connecticut                  1080677            715315\n 7 Delaware                      296274            200601\n 8 District of Columbia          317324             18595\n 9 Florida                      5297131           5668600\n10 Georgia                      2473661           2461869\n# ℹ 40 more rows\n\n\n\n# What states did the Democratic candidate win in 2020?\n# HINT: Start with the results from the previous chunk, and then keep only some rows\nelections_small |&gt;\n  mutate(dem_votes_20 = round(total_votes_20 * dem_pct_20 / 100),\n         repub_votes_20 = round(total_votes_20 * repub_pct_20 / 100)) |&gt;\n  group_by(state_name) |&gt;\n  summarize(total_dem_votes = sum(dem_votes_20, na.rm = TRUE), total_repub_votes = sum(repub_votes_20, na.rm = TRUE)) |&gt;\n  filter(total_dem_votes &gt; total_repub_votes)\n\n# A tibble: 26 × 3\n   state_name           total_dem_votes total_repub_votes\n   &lt;chr&gt;                          &lt;dbl&gt;             &lt;dbl&gt;\n 1 Arizona                      1672127           1661671\n 2 California                  11109642           6006031\n 3 Colorado                     1804393           1364627\n 4 Connecticut                  1080677            715315\n 5 Delaware                      296274            200601\n 6 District of Columbia          317324             18595\n 7 Georgia                      2473661           2461869\n 8 Hawaii                        366121            196865\n 9 Illinois                     3471916           2446931\n10 Maine                         430466            359897\n# ℹ 16 more rows\n\n\nExercise 10: Practice on New Data\nRecall the World Cup football/soccer data from TidyTuesday:\n\nworld_cup &lt;- read.csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-11-29/worldcups.csv\")\n\nYou can find a codebook here. Use (some of) our 6 verbs (select, filter, arrange, mutate, summarize, group_by) and data viz to address the following prompts.\n\n# In what years did Brazil win the World Cup?\nworld_cup |&gt; filter(winner == \"Brazil\") |&gt; select(year)\n\n  year\n1 1958\n2 1962\n3 1970\n4 1994\n5 2002\n\n\n\n# What were the 6 World Cups with the highest attendance?\nworld_cup |&gt; arrange(desc(attendance)) |&gt; head(6)\n\n  year               host  winner    second       third      fourth\n1 1994                USA  Brazil     Italy      Sweden    Bulgaria\n2 2014             Brazil Germany Argentina Netherlands      Brazil\n3 2006            Germany   Italy    France     Germany    Portugal\n4 2018             Russia  France   Croatia     Belgium     England\n5 1998             France  France    Brazil     Croatia Netherlands\n6 2002 Japan, South Korea  Brazil   Germany      Turkey South Korea\n  goals_scored teams games attendance\n1          141    24    52    3568567\n2          171    32    64    3441450\n3          147    32    64    3367000\n4          169    32    64    3031768\n5          171    32    64    2859234\n6          161    32    64    2724604\n\n\n\n# Construct a univariate plot of goals_scored (no wrangling necessary)\n# This provides a visual summary of how the number of goals_scored varies from World Cup to World Cup\nworld_cup |&gt; ggplot(aes(x = goals_scored)) + geom_histogram(bins = 20) + theme_minimal()\n\n\n\n\n\n\n\n\n# Let's follow up the plot with some more precise numerical summaries\n# Calculate the min, median, and max number of goals_scored across all World Cups\n# NOTE: Visually compare these numerical summaries to what you observed in the plot\nworld_cup |&gt; summarize(min_goals = min(goals_scored, na.rm = TRUE), median_goals = median(goals_scored, na.rm = TRUE), max_goals = max(goals_scored, na.rm = TRUE))\n\n  min_goals median_goals max_goals\n1        70          126       171\n\n\n\n# Construct a bivariate plot of how the number of goals_scored in the World Cup has changed over the years\n# No wrangling necessary\nworld_cup |&gt; ggplot(aes(x = year, y = goals_scored)) + geom_line() + geom_point() + theme_minimal()\n\n\n\n\n\n\n\nExercise 11: Practice on Your Data\nReturn to the TidyTuesday data you’re using in Homework 3. Use your new wrangling skills to play around. What new insights can you gain?!",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Wrangling</span>"
    ]
  },
  {
    "objectID": "ica/ica-dates.html",
    "href": "ica/ica-dates.html",
    "title": "\n18  Dates\n",
    "section": "",
    "text": "18.1 Review",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Dates</span>"
    ]
  },
  {
    "objectID": "ica/ica-dates.html#review",
    "href": "ica/ica-dates.html#review",
    "title": "\n18  Dates\n",
    "section": "",
    "text": "Data Science Process\nBelow is the visual representation of the data science process we saw earlier. Which stage are we in currently?\n\nRecall that wrangling is important. It is much of what we spend our efforts on in Data Science. There are lots of steps, hence R functions, that can go into data wrangling. But we can get far with the following 6 wrangling verbs:\n\n\nverb\naction\n\n\n\narrange\n\narrange the rows according to some column\n\n\n\nfilter\n\nfilter out or obtain a subset of the rows\n\n\n\nselect\n\nselect a subset of columns\n\n\n\nmutate\n\nmutate or create a column\n\n\n\nsummarize\ncalculate a numerical summary of a column\n\n\n\ngroup_by\n\ngroup the rows by a specified column\n\n\n\nExample 1: Single Verb\nLet’s start by working with some TidyTuesday data on penguins. This data includes information about penguins’ flippers (“arms”) and bills (“mouths” or “beaks”). Let’s import this using read_csv(), a function in the tidyverse package. For the most part, this is similar to read.csv(), though read_csv() can be more efficient at importing large datasets.\n\nlibrary(tidyverse)\npenguins &lt;- read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-07-28/penguins.csv')\n\n# Check it out\nhead(penguins)\n\n# A tibble: 6 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;       &lt;dbl&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           NA            NA                  NA          NA\n5 Adelie  Torgersen           36.7          19.3               193        3450\n6 Adelie  Torgersen           39.3          20.6               190        3650\n# ℹ 2 more variables: sex &lt;chr&gt;, year &lt;dbl&gt;\n\n\n\n\n\n\n\n\nCheck Understanding\n\n\n\nConstruct a plot that allows us to examine how the relationship between body mass and bill length varies by species and sex.\n\n\n\n\n\n\n\n\nCheck Understanding\n\n\n\nUse the 6 wrangling verbs to address each task in the code chunk below. You can tack on |&gt; head() to print out just 6 rows to keep your rendered document manageable. Most of these require just 1 verb.\n\n\n\n# Get data on only Adelie penguins that weigh more than 4700g\n\n\n# Get data on penguin body mass only\n# Show just the first 6 rows\n\n\n# Sort the penguins from smallest to largest body mass\n# Show just the first 6 rows\n\n\n\n# Calculate the average body mass across all penguins\n# Note: na.rm = TRUE removes the NAs from the calculation\n\n\n\n# Calculate the average body mass by species\n\n\n\n# Create a new column that records body mass in kilograms, not grams\n# NOTE: there are 1000 g in 1 kg\n# Show just the first 6 rows\n\n\n\n\n\n\n\nCheck Understanding\n\n\n\nHow many penguins of each species do we have? Create a viz that addresses this question.\n\n\n\nggplot(penguins, aes(x = species))\n\n\n\n\n\n\n\n\n\n\n\n\n\nCheck Understanding\n\n\n\nCan we use the 6 verbs to calculate exactly how many penguins in each species?\nHINT: n() calculates group size.\n\n\n\n\n\n\n\n\ncount verb\n\n\n\nThe count() verb provides a handy shortcut!\n\npenguins |&gt; \n  count(species)\n\n# A tibble: 3 × 2\n  species       n\n  &lt;chr&gt;     &lt;int&gt;\n1 Adelie      152\n2 Chinstrap    68\n3 Gentoo      124\n\n\n\n\nExample 2: Multiple Verbs\n\n\n\n\n\n\nCheck Understanding\n\n\n\nLet’s practice combining some verbs. For each task:\n\nTranslate the prompt into our 6 verbs. That is, think before you type.\nBuild your code line by line. It’s important to understand what’s being piped into each function!\nAsk what you can rearrange and still get the same result.\nRead your final code like a paragraph / a conversation. Would another person be able to follow your logic?\n\n\n\n\n# Sort Gentoo penguins from biggest to smallest with respect to their \n# bill length in cm (there are 10 mm in a cm)\n\n\n# Sort the species from smallest to biggest with respect to their \n# average bill length in cm\n\nExample 3: Interpret Code\nLet’s practice reading and making sense of somebody else’s code. What do you think this produces?\n\nHow many columns? Rows?\nWhat are the column names?\nWhat’s represented in each row?\n\nOnce you’ve thought about it, put the code inside a chunk and run it!\npenguins |&gt; filter(species == “Chinstrap”) |&gt; group_by(sex) |&gt; summarize(min = min(body_mass_g), max = max(body_mass_g)) |&gt; mutate(range = max - min)",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Dates</span>"
    ]
  },
  {
    "objectID": "ica/ica-dates.html#exercises-part-1-same-verbs-new-tricks",
    "href": "ica/ica-dates.html#exercises-part-1-same-verbs-new-tricks",
    "title": "\n18  Dates\n",
    "section": "\n18.2 Exercises Part 1: Same Verbs, New Tricks",
    "text": "18.2 Exercises Part 1: Same Verbs, New Tricks\nExercise 1: More Filtering\nRecall the “logical comparison operators” we can use to filter() our data:\n\n\nsymbol\nmeaning\n\n\n\n==\nequal to\n\n\n!=\nnot equal to\n\n\n&gt;\ngreater than\n\n\n&gt;=\ngreater than or equal to\n\n\n&lt;\nless than\n\n\n&lt;=\nless than or equal to\n\n\n%in% c(***, ***)\na list of multiple values\n\n\n\nPart a\n\n\n\n\n\n\nCommenting/Uncommenting Code\n\n\n\nTo comment/uncomment several lines of code at once, highlight them then click ctrl/cmd+shift+c.\n\n\n\n# Create a dataset with just Adelie and Chinstrap using %in%\n# Pipe this into `count(species)` to confirm that you only have these 2 species\npenguins |&gt;\n  filter(species %in% c(\"Adelie\", \"Chinstrap\")) |&gt;\n  count(species)\n\n# A tibble: 2 × 2\n  species       n\n  &lt;chr&gt;     &lt;int&gt;\n1 Adelie      152\n2 Chinstrap    68\n\n\n\n# Create a dataset with just Adelie and Chinstrap using !=\n# Pipe this into `count(species)` to confirm that you only have these 2 species\npenguins |&gt;\n  filter(species != \"Gentoo\") |&gt;\n  count(species)\n\n# A tibble: 2 × 2\n  species       n\n  &lt;chr&gt;     &lt;int&gt;\n1 Adelie      152\n2 Chinstrap    68\n\n\nPart b\nNotice that some of our penguins have missing (NA) data on some values:\n\nhead(penguins)\n\n# A tibble: 6 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;       &lt;dbl&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           NA            NA                  NA          NA\n5 Adelie  Torgersen           36.7          19.3               193        3450\n6 Adelie  Torgersen           39.3          20.6               190        3650\n# ℹ 2 more variables: sex &lt;chr&gt;, year &lt;dbl&gt;\n\n\n\n\n\n\n\n\nHandeling NA Values\n\n\n\nThere are many ways to handle missing data. The right approach depends upon your research goals. A general rule is: Only get rid of observations with missing data if they’re missing data on variables you need for the specific task at hand!\n\n\nExample 1\nSuppose our research focus is just on body_mass_g. Two penguins are missing this info:\n\n# NOTE the use of is.na()\npenguins |&gt; \n  summarize(sum(is.na(body_mass_g)))\n\n# A tibble: 1 × 1\n  `sum(is.na(body_mass_g))`\n                      &lt;int&gt;\n1                         2\n\n\nLet’s define a new dataset that removes these penguins:\n\n# NOTE the use of is.na()\npenguins_w_body_mass &lt;- penguins |&gt; \n  filter(!is.na(body_mass_g))\n\n# Compare the number of penguins in this vs the original data\nnrow(penguins_w_body_mass)\n\n[1] 342\n\nnrow(penguins)\n\n[1] 344\n\n\nNote that some penguins in penguins_w_body_mass are missing info on sex, but we don’t care since that’s not related to our research question:\n\npenguins_w_body_mass |&gt; \n  summarize(sum(is.na(sex)))\n\n# A tibble: 1 × 1\n  `sum(is.na(sex))`\n              &lt;int&gt;\n1                 9\n\n\nExample 2\nIn the very rare case that we need complete information on every variable for the specific task at hand, we can use na.omit() to get rid of any penguin that’s missing info on any variable:\n\npenguins_complete &lt;- penguins |&gt; \n  na.omit()\n\nHow many penguins did this eliminate?\n\nnrow(penguins_complete)\n\n[1] 333\n\nnrow(penguins)\n\n[1] 344\n\n\nPart c\nExplain why we should only use na.omit() in extreme circumstances.\n\nExercise 2: More Selecting\nBeing able to select() only certain columns can help simplify our data. This is especially important when we’re working with lots of columns (which we haven’t done yet). It can also get tedious to type out every column of interest. Here are some shortcuts:\n\n\n- removes a given variable and keeps all others (e.g. select(-island))\n\nstarts_with(\"___\"), ends_with(\"___\"), or contains(\"___\") selects only the columns that either start with, end with, or simply contain the given string of characters\n\nUse these shortcuts to create the following datasets.\n\n# First: recall the variable names\nnames(penguins)\n\n[1] \"species\"           \"island\"            \"bill_length_mm\"   \n[4] \"bill_depth_mm\"     \"flipper_length_mm\" \"body_mass_g\"      \n[7] \"sex\"               \"year\"             \n\n\n\n# Use a shortcut to keep everything but the year and island variables\npenguins |&gt; select(-year, -island)\n\n# A tibble: 344 × 6\n   species bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex   \n   &lt;chr&gt;            &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt; \n 1 Adelie            39.1          18.7               181        3750 male  \n 2 Adelie            39.5          17.4               186        3800 female\n 3 Adelie            40.3          18                 195        3250 female\n 4 Adelie            NA            NA                  NA          NA &lt;NA&gt;  \n 5 Adelie            36.7          19.3               193        3450 female\n 6 Adelie            39.3          20.6               190        3650 male  \n 7 Adelie            38.9          17.8               181        3625 female\n 8 Adelie            39.2          19.6               195        4675 male  \n 9 Adelie            34.1          18.1               193        3475 &lt;NA&gt;  \n10 Adelie            42            20.2               190        4250 &lt;NA&gt;  \n# ℹ 334 more rows\n\n\n\n# Use a shortcut to keep only species and the penguin characteristics measured in mm\npenguins |&gt; select(species, ends_with(\"_mm\"))\n\n# A tibble: 344 × 4\n   species bill_length_mm bill_depth_mm flipper_length_mm\n   &lt;chr&gt;            &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;\n 1 Adelie            39.1          18.7               181\n 2 Adelie            39.5          17.4               186\n 3 Adelie            40.3          18                 195\n 4 Adelie            NA            NA                  NA\n 5 Adelie            36.7          19.3               193\n 6 Adelie            39.3          20.6               190\n 7 Adelie            38.9          17.8               181\n 8 Adelie            39.2          19.6               195\n 9 Adelie            34.1          18.1               193\n10 Adelie            42            20.2               190\n# ℹ 334 more rows\n\n\n\n# Use a shortcut to keep only species and bill-related measurements\npenguins |&gt; select(species, contains(\"bill\"))\n\n# A tibble: 344 × 3\n   species bill_length_mm bill_depth_mm\n   &lt;chr&gt;            &lt;dbl&gt;         &lt;dbl&gt;\n 1 Adelie            39.1          18.7\n 2 Adelie            39.5          17.4\n 3 Adelie            40.3          18  \n 4 Adelie            NA            NA  \n 5 Adelie            36.7          19.3\n 6 Adelie            39.3          20.6\n 7 Adelie            38.9          17.8\n 8 Adelie            39.2          19.6\n 9 Adelie            34.1          18.1\n10 Adelie            42            20.2\n# ℹ 334 more rows\n\n\n\n# Use a shortcut to keep only species and the length-related characteristics\npenguins |&gt; select(species, contains(\"length\"))\n\n# A tibble: 344 × 3\n   species bill_length_mm flipper_length_mm\n   &lt;chr&gt;            &lt;dbl&gt;             &lt;dbl&gt;\n 1 Adelie            39.1               181\n 2 Adelie            39.5               186\n 3 Adelie            40.3               195\n 4 Adelie            NA                  NA\n 5 Adelie            36.7               193\n 6 Adelie            39.3               190\n 7 Adelie            38.9               181\n 8 Adelie            39.2               195\n 9 Adelie            34.1               193\n10 Adelie            42                 190\n# ℹ 334 more rows\n\n\n\nExercise 3: Arranging, Counting, & Grouping by Multiple Variables\nWe’ve done examples where we need to filter() by more than one variable, or select() more than one variable. Use your intuition for how we can arrange(), count(), and group_by() more than one variable.\n\n# Change this code to sort the penguins by species, and then island name\n# NOTE: The first row should be an Adelie penguin living on Biscoe island\npenguins |&gt; arrange(species, island)\n\n# A tibble: 344 × 8\n   species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;chr&gt;   &lt;chr&gt;           &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;       &lt;dbl&gt;\n 1 Adelie  Biscoe           37.8          18.3               174        3400\n 2 Adelie  Biscoe           37.7          18.7               180        3600\n 3 Adelie  Biscoe           35.9          19.2               189        3800\n 4 Adelie  Biscoe           38.2          18.1               185        3950\n 5 Adelie  Biscoe           38.8          17.2               180        3800\n 6 Adelie  Biscoe           35.3          18.9               187        3800\n 7 Adelie  Biscoe           40.6          18.6               183        3550\n 8 Adelie  Biscoe           40.5          17.9               187        3200\n 9 Adelie  Biscoe           37.9          18.6               172        3150\n10 Adelie  Biscoe           40.5          18.9               180        3950\n# ℹ 334 more rows\n# ℹ 2 more variables: sex &lt;chr&gt;, year &lt;dbl&gt;\n\n\n\n# Change this code to count the number of male/female penguins observed for each species\npenguins |&gt; count(species, sex)\n\n# A tibble: 8 × 3\n  species   sex        n\n  &lt;chr&gt;     &lt;chr&gt;  &lt;int&gt;\n1 Adelie    female    73\n2 Adelie    male      73\n3 Adelie    &lt;NA&gt;       6\n4 Chinstrap female    34\n5 Chinstrap male      34\n6 Gentoo    female    58\n7 Gentoo    male      61\n8 Gentoo    &lt;NA&gt;       5\n\n\n\n# Change this code to calculate the average body mass by species and sex\npenguins |&gt; group_by(species, sex) |&gt; summarize(mean = mean(body_mass_g, na.rm = TRUE))\n\n# A tibble: 8 × 3\n# Groups:   species [3]\n  species   sex     mean\n  &lt;chr&gt;     &lt;chr&gt;  &lt;dbl&gt;\n1 Adelie    female 3369.\n2 Adelie    male   4043.\n3 Adelie    &lt;NA&gt;   3540 \n4 Chinstrap female 3527.\n5 Chinstrap male   3939.\n6 Gentoo    female 4680.\n7 Gentoo    male   5485.\n8 Gentoo    &lt;NA&gt;   4588.\n\n\n\nExercise 4: Dates\nBefore some wrangling practice, let’s explore another important concept: working with or mutating date variables. Dates are a whole special object type or class in R that automatically respect the order of time.\n\n# Get today's date\nas.Date(today())\n\n[1] \"2025-04-23\"\n\n# Let's store this as \"today\" so we can work with it below\ntoday &lt;- as.Date(today())\n\n# Check out the class of this object\nclass(today)\n\n[1] \"Date\"\n\n\nThe lubridate package inside tidyverse contains functions that can extract various information from dates. Let’s learn about some of the most common functions by applying them to today. For each, make a comment on what the function does\n\nyear(today)\n\n[1] 2025\n\n\n\n# What do these lines produce / what's their difference?\nmonth(today)\n\n[1] 4\n\nmonth(today, label = TRUE)\n\n[1] Apr\n12 Levels: Jan &lt; Feb &lt; Mar &lt; Apr &lt; May &lt; Jun &lt; Jul &lt; Aug &lt; Sep &lt; ... &lt; Dec\n\n\n\n# What does this number mean?\nweek(today)\n\n[1] 17\n\n\n\n# What do these lines produce / what's their difference?\nmday(today)\n\n[1] 23\n\nyday(today)  # This is often called the \"Julian day\"\n\n[1] 113\n\n\n\n# What do these lines produce / what's their difference?\nwday(today)\n\n[1] 4\n\nwday(today, label = TRUE)\n\n[1] Wed\nLevels: Sun &lt; Mon &lt; Tue &lt; Wed &lt; Thu &lt; Fri &lt; Sat\n\n\n\n# What do the results of these 2 lines tell us?\ntoday &gt;= ymd(\"2024-02-14\")\n\n[1] TRUE\n\ntoday &lt; ymd(\"2024-02-14\")\n\n[1] FALSE",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Dates</span>"
    ]
  },
  {
    "objectID": "ica/ica-dates.html#exercises-part-2-application",
    "href": "ica/ica-dates.html#exercises-part-2-application",
    "title": "\n18  Dates\n",
    "section": "\n18.3 Exercises Part 2: Application",
    "text": "18.3 Exercises Part 2: Application\nThe remaining exercises are similar to some of those on the homework. Hence, the solutions are not provided. Let’s apply these ideas to the daily Birthdays dataset in the mosaic package.\n\nlibrary(mosaic)\ndata(\"Birthdays\")\nhead(Birthdays)\n\n  state year month day       date wday births\n1    AK 1969     1   1 1969-01-01  Wed     14\n2    AL 1969     1   1 1969-01-01  Wed    174\n3    AR 1969     1   1 1969-01-01  Wed     78\n4    AZ 1969     1   1 1969-01-01  Wed     84\n5    CA 1969     1   1 1969-01-01  Wed    824\n6    CO 1969     1   1 1969-01-01  Wed    100\n\n\nBirthdays gives the number of births recorded on each day of the year in each state from 1969 to 19881. We can use our wrangling skills to understand some drivers of daily births. Putting these all together can be challenging! Remember the following ways to make tasks more manageable:\n\nTranslate the prompt into our 6 verbs (and count()). That is, think before you type.\nBuild your code line by line. It’s important to understand what’s being piped into each function!\n\nExercise 5: Warming up\n\n# How many days of data do we have for each state?\nBirthdays |&gt; count(state)\n\n   state    n\n1     AK 7306\n2     AL 7312\n3     AR 7310\n4     AZ 7310\n5     CA 7325\n6     CO 7305\n7     CT 7312\n8     DC 7311\n9     DE 7307\n10    FL 7307\n11    GA 7314\n12    HI 7306\n13    IA 7306\n14    ID 7306\n15    IL 7314\n16    IN 7311\n17    KS 7311\n18    KY 7313\n19    LA 7309\n20    MA 7315\n21    MD 7311\n22    ME 7309\n23    MI 7323\n24    MN 7315\n25    MO 7309\n26    MS 7310\n27    MT 7305\n28    NC 7307\n29    ND 7305\n30    NE 7305\n31    NH 7308\n32    NJ 7321\n33    NM 7308\n34    NV 7307\n35    NY 7333\n36    OH 7319\n37    OK 7306\n38    OR 7307\n39    PA 7330\n40    RI 7305\n41    SC 7314\n42    SD 7305\n43    TN 7308\n44    TX 7330\n45    UT 7307\n46    VA 7310\n47    VT 7305\n48    WA 7306\n49    WI 7311\n50    WV 7310\n51    WY 7305\n\n# How many total births were there in this time period?\nBirthdays |&gt; summarize(total_births = sum(births))\n\n  total_births\n1     70486538\n\n# How many total births were there per state in this time period, sorted from low to high?\nBirthdays |&gt; group_by(state) |&gt; summarize(total_births = sum(births)) |&gt; arrange(total_births)\n\n# A tibble: 51 × 2\n   state total_births\n   &lt;chr&gt;        &lt;int&gt;\n 1 VT          147886\n 2 WY          154019\n 3 AK          185385\n 4 DE          188705\n 5 SD          235734\n 6 ND          238696\n 7 NV          241470\n 8 MT          253884\n 9 NH          264984\n10 RI          265038\n# ℹ 41 more rows\n\n\nExercise 6: Homework Reprise\nCreate a new dataset named daily_births that includes the total number of births per day (across all states) and the corresponding day of the week, eg, Mon. NOTE: Name the column with total births so that it’s easier to wrangle and plot.\n\ndaily_births &lt;- Birthdays |&gt;\n  group_by(date) |&gt;\n  summarize(births = sum(births), .groups = \"drop\") |&gt;\n  mutate(day_of_week = weekdays(date))\n\nUsing this data, construct a plot of births over time, indicating the day of week.\n\nggplot(daily_births, aes(x = date, y = births, color = day_of_week)) +\n  geom_line() +\n  theme_minimal()\n\n\n\n\n\n\n\nExercise 7: Wrangle & Plot\nFor each prompt below, you can decide whether you want to: (1) wrangle and store data, then plot; or (2) wrangle data and pipe directly into ggplot. For example:\n\npenguins |&gt; \n  filter(species != \"Gentoo\") |&gt; \n  ggplot(aes(y = bill_length_mm, x = bill_depth_mm, color = species)) + \n    geom_point()\n\n\n\n\n\n\n\nPart a\nCalculate the total number of births in each month and year, eg, Jan 1969, Feb 1969, …. Label month by names not numbers, eg, Jan not 1. Then, plot the births by month and comment on what you learn.\n\nBirthdays |&gt;\n  mutate(month = month(date, label = TRUE), year = year(date)) |&gt;\n  group_by(year, month) |&gt;\n  summarize(total_births = sum(births), .groups = \"drop\") |&gt;\n  ggplot(aes(x = month, y = total_births, group = year, color = as.factor(year))) +\n  geom_line() +\n  theme_minimal()\n\n\n\n\n\n\n\nPart b\nIn 1988, calculate the total number of births per week in each state. Get rid of week “53”, which isn’t a complete week! Then, make a line plot of births by week for each state and comment on what you learn. For example, do you notice any seasonal trends? Are these the same in every state? Any outliers?\n\nBirthdays |&gt;\n  filter(year == 1988, week(date) != 53) |&gt;\n  group_by(state, week = week(date)) |&gt;\n  summarize(births = sum(births), .groups = \"drop\") |&gt;\n  ggplot(aes(x = week, y = births, color = state)) +\n  geom_line() +\n  theme_minimal()\n\n\n\n\n\n\n\nPart c\nRepeat the above for just Minnesota (MN) and Louisiana (LA). MN has one of the coldest climates and LA has one of the warmest. How do their seasonal trends compare? Do you think these trends are similar in other colder and warmer states? Try it!\n\nBirthdays |&gt;\n  filter(year == 1988, week(date) != 53, state %in% c(\"MN\", \"LA\")) |&gt;\n  group_by(state, week = week(date)) |&gt;\n  summarize(births = sum(births), .groups = \"drop\") |&gt;\n  ggplot(aes(x = week, y = births, color = state)) +\n  geom_line() +\n  theme_minimal()\n\n\n\n\n\n\n\nExercise 8: More Practice\nPart a\nCreate a dataset with only births in Massachusetts (MA) in 1979 and sort the days from those with the most births to those with the fewest.\n\nBirthdays |&gt;\n  filter(state == \"MA\", year == 1979) |&gt;\n  arrange(desc(births))\n\n    state year month day       date  wday births\n1      MA 1979     9  28 1979-09-28   Fri    262\n2      MA 1979     9  11 1979-09-11  Tues    252\n3      MA 1979    12  28 1979-12-28   Fri    249\n4      MA 1979     9  26 1979-09-26   Wed    246\n5      MA 1979     7  24 1979-07-24  Tues    245\n6      MA 1979     4  27 1979-04-27   Fri    243\n7      MA 1979     8   6 1979-08-06   Mon    243\n8      MA 1979    10   2 1979-10-02  Tues    239\n9      MA 1979     6  29 1979-06-29   Fri    238\n10     MA 1979     8  23 1979-08-23 Thurs    238\n11     MA 1979     2  23 1979-02-23   Fri    237\n12     MA 1979     5   1 1979-05-01  Tues    236\n13     MA 1979     7  18 1979-07-18   Wed    236\n14     MA 1979     8   2 1979-08-02 Thurs    236\n15     MA 1979     5  24 1979-05-24 Thurs    235\n16     MA 1979     8   1 1979-08-01   Wed    235\n17     MA 1979     4   6 1979-04-06   Fri    234\n18     MA 1979     8  31 1979-08-31   Fri    234\n19     MA 1979     5  10 1979-05-10 Thurs    233\n20     MA 1979     8  15 1979-08-15   Wed    233\n21     MA 1979     3  23 1979-03-23   Fri    232\n22     MA 1979     6  12 1979-06-12  Tues    232\n23     MA 1979     7  10 1979-07-10  Tues    232\n24     MA 1979     7  25 1979-07-25   Wed    232\n25     MA 1979     6  27 1979-06-27   Wed    231\n26     MA 1979     7  27 1979-07-27   Fri    231\n27     MA 1979    10   5 1979-10-05   Fri    231\n28     MA 1979     5  23 1979-05-23   Wed    230\n29     MA 1979     9  19 1979-09-19   Wed    229\n30     MA 1979     2  16 1979-02-16   Fri    228\n31     MA 1979     8  16 1979-08-16 Thurs    228\n32     MA 1979     8  24 1979-08-24   Fri    228\n33     MA 1979     9  17 1979-09-17   Mon    228\n34     MA 1979     4  30 1979-04-30   Mon    227\n35     MA 1979     6  26 1979-06-26  Tues    227\n36     MA 1979     7  20 1979-07-20   Fri    227\n37     MA 1979     8  10 1979-08-10   Fri    227\n38     MA 1979     8  14 1979-08-14  Tues    227\n39     MA 1979     9  24 1979-09-24   Mon    227\n40     MA 1979     3   5 1979-03-05   Mon    226\n41     MA 1979     6  21 1979-06-21 Thurs    226\n42     MA 1979     2   2 1979-02-02   Fri    225\n43     MA 1979     2  27 1979-02-27  Tues    225\n44     MA 1979     4   9 1979-04-09   Mon    225\n45     MA 1979     5   8 1979-05-08  Tues    225\n46     MA 1979     7  17 1979-07-17  Tues    225\n47     MA 1979     9  14 1979-09-14   Fri    225\n48     MA 1979     9  20 1979-09-20 Thurs    225\n49     MA 1979     6  11 1979-06-11   Mon    224\n50     MA 1979    11   9 1979-11-09   Fri    224\n51     MA 1979     2  14 1979-02-14   Wed    223\n52     MA 1979     7  23 1979-07-23   Mon    223\n53     MA 1979     8   9 1979-08-09 Thurs    223\n54     MA 1979     1   3 1979-01-03   Wed    222\n55     MA 1979     1  22 1979-01-22   Mon    222\n56     MA 1979     6  19 1979-06-19  Tues    222\n57     MA 1979     9   4 1979-09-04  Tues    222\n58     MA 1979     9  21 1979-09-21   Fri    222\n59     MA 1979    10  23 1979-10-23  Tues    222\n60     MA 1979     2   6 1979-02-06  Tues    221\n61     MA 1979     3  19 1979-03-19   Mon    221\n62     MA 1979     4  25 1979-04-25   Wed    221\n63     MA 1979     8  20 1979-08-20   Mon    221\n64     MA 1979     9  18 1979-09-18  Tues    221\n65     MA 1979    11   6 1979-11-06  Tues    221\n66     MA 1979     2  20 1979-02-20  Tues    220\n67     MA 1979     4  26 1979-04-26 Thurs    220\n68     MA 1979     8  13 1979-08-13   Mon    220\n69     MA 1979     8  17 1979-08-17   Fri    220\n70     MA 1979     9  10 1979-09-10   Mon    220\n71     MA 1979    10  25 1979-10-25 Thurs    220\n72     MA 1979    10  26 1979-10-26   Fri    220\n73     MA 1979     3   6 1979-03-06  Tues    219\n74     MA 1979     6   1 1979-06-01   Fri    219\n75     MA 1979     8  29 1979-08-29   Wed    219\n76     MA 1979     9  27 1979-09-27 Thurs    219\n77     MA 1979    10   1 1979-10-01   Mon    219\n78     MA 1979    11  19 1979-11-19   Mon    219\n79     MA 1979     4  24 1979-04-24  Tues    218\n80     MA 1979     5  21 1979-05-21   Mon    218\n81     MA 1979     6  28 1979-06-28 Thurs    218\n82     MA 1979     7   9 1979-07-09   Mon    218\n83     MA 1979     8   3 1979-08-03   Fri    218\n84     MA 1979     9   6 1979-09-06 Thurs    218\n85     MA 1979    10   3 1979-10-03   Wed    218\n86     MA 1979     6   7 1979-06-07 Thurs    217\n87     MA 1979     6  14 1979-06-14 Thurs    217\n88     MA 1979     7   3 1979-07-03  Tues    217\n89     MA 1979     8   8 1979-08-08   Wed    217\n90     MA 1979    10   4 1979-10-04 Thurs    217\n91     MA 1979    10  22 1979-10-22   Mon    217\n92     MA 1979    10  29 1979-10-29   Mon    217\n93     MA 1979    10   9 1979-10-09  Tues    216\n94     MA 1979     1  16 1979-01-16  Tues    215\n95     MA 1979     3  13 1979-03-13  Tues    215\n96     MA 1979     5   4 1979-05-04   Fri    215\n97     MA 1979     5  14 1979-05-14   Mon    215\n98     MA 1979     6  22 1979-06-22   Fri    215\n99     MA 1979     3  30 1979-03-30   Fri    214\n100    MA 1979     6   5 1979-06-05  Tues    214\n101    MA 1979    12  14 1979-12-14   Fri    214\n102    MA 1979    12  27 1979-12-27 Thurs    214\n103    MA 1979     3   8 1979-03-08 Thurs    213\n104    MA 1979     4  18 1979-04-18   Wed    213\n105    MA 1979     8  28 1979-08-28  Tues    213\n106    MA 1979    10  12 1979-10-12   Fri    213\n107    MA 1979    11  14 1979-11-14   Wed    213\n108    MA 1979    12  31 1979-12-31   Mon    213\n109    MA 1979     4  23 1979-04-23   Mon    212\n110    MA 1979     5  11 1979-05-11   Fri    212\n111    MA 1979     5  18 1979-05-18   Fri    212\n112    MA 1979     5  30 1979-05-30   Wed    212\n113    MA 1979     7   6 1979-07-06   Fri    212\n114    MA 1979     7  19 1979-07-19 Thurs    212\n115    MA 1979    10  17 1979-10-17   Wed    212\n116    MA 1979    10  24 1979-10-24   Wed    212\n117    MA 1979    12  11 1979-12-11  Tues    212\n118    MA 1979     2   5 1979-02-05   Mon    211\n119    MA 1979     3   7 1979-03-07   Wed    211\n120    MA 1979     3  29 1979-03-29 Thurs    211\n121    MA 1979     9   7 1979-09-07   Fri    211\n122    MA 1979    10  30 1979-10-30  Tues    211\n123    MA 1979     7   2 1979-07-02   Mon    210\n124    MA 1979     1  15 1979-01-15   Mon    209\n125    MA 1979     3  26 1979-03-26   Mon    209\n126    MA 1979     4   4 1979-04-04   Wed    209\n127    MA 1979     7  15 1979-07-15   Sun    209\n128    MA 1979     2   8 1979-02-08 Thurs    208\n129    MA 1979     4  11 1979-04-11   Wed    208\n130    MA 1979     6   6 1979-06-06   Wed    208\n131    MA 1979     6  30 1979-06-30   Sat    208\n132    MA 1979     7  30 1979-07-30   Mon    208\n133    MA 1979     9  29 1979-09-29   Sat    208\n134    MA 1979    10  10 1979-10-10   Wed    208\n135    MA 1979     2   9 1979-02-09   Fri    207\n136    MA 1979     3   9 1979-03-09   Fri    207\n137    MA 1979     4  20 1979-04-20   Fri    207\n138    MA 1979     6  20 1979-06-20   Wed    207\n139    MA 1979     7  26 1979-07-26 Thurs    207\n140    MA 1979     8  21 1979-08-21  Tues    207\n141    MA 1979     9  25 1979-09-25  Tues    207\n142    MA 1979    11  26 1979-11-26   Mon    207\n143    MA 1979     1   2 1979-01-02  Tues    206\n144    MA 1979     2  22 1979-02-22 Thurs    206\n145    MA 1979     7  12 1979-07-12 Thurs    206\n146    MA 1979     7  13 1979-07-13   Fri    206\n147    MA 1979     9   5 1979-09-05   Wed    206\n148    MA 1979     5  25 1979-05-25   Fri    205\n149    MA 1979    11  23 1979-11-23   Fri    205\n150    MA 1979    11  28 1979-11-28   Wed    205\n151    MA 1979    12   4 1979-12-04  Tues    205\n152    MA 1979     1  26 1979-01-26   Fri    204\n153    MA 1979     3  22 1979-03-22 Thurs    204\n154    MA 1979     5  16 1979-05-16   Wed    204\n155    MA 1979    10  16 1979-10-16  Tues    204\n156    MA 1979    11  16 1979-11-16   Fri    204\n157    MA 1979    12  24 1979-12-24   Mon    204\n158    MA 1979     3   2 1979-03-02   Fri    203\n159    MA 1979     4   3 1979-04-03  Tues    203\n160    MA 1979     4  12 1979-04-12 Thurs    203\n161    MA 1979     5   3 1979-05-03 Thurs    203\n162    MA 1979    10   6 1979-10-06   Sat    203\n163    MA 1979    11   1 1979-11-01 Thurs    203\n164    MA 1979    11  30 1979-11-30   Fri    203\n165    MA 1979    12  10 1979-12-10   Mon    203\n166    MA 1979    12  17 1979-12-17   Mon    203\n167    MA 1979     6   8 1979-06-08   Fri    202\n168    MA 1979     6  15 1979-06-15   Fri    202\n169    MA 1979    11  11 1979-11-11   Sun    202\n170    MA 1979     1  29 1979-01-29   Mon    201\n171    MA 1979     3  21 1979-03-21   Wed    201\n172    MA 1979     3  28 1979-03-28   Wed    201\n173    MA 1979     4  10 1979-04-10  Tues    201\n174    MA 1979     5   2 1979-05-02   Wed    201\n175    MA 1979    10  15 1979-10-15   Mon    201\n176    MA 1979    11   7 1979-11-07   Wed    201\n177    MA 1979     1  18 1979-01-18 Thurs    200\n178    MA 1979     4  19 1979-04-19 Thurs    200\n179    MA 1979     7  11 1979-07-11   Wed    200\n180    MA 1979     7  21 1979-07-21   Sat    200\n181    MA 1979     8   5 1979-08-05   Sun    200\n182    MA 1979    10  19 1979-10-19   Fri    200\n183    MA 1979    11   2 1979-11-02   Fri    200\n184    MA 1979    11  15 1979-11-15 Thurs    200\n185    MA 1979    11  20 1979-11-20  Tues    200\n186    MA 1979     1  11 1979-01-11 Thurs    199\n187    MA 1979     2  12 1979-02-12   Mon    199\n188    MA 1979     3  14 1979-03-14   Wed    199\n189    MA 1979     5  22 1979-05-22  Tues    199\n190    MA 1979     2  26 1979-02-26   Mon    198\n191    MA 1979     3  15 1979-03-15 Thurs    198\n192    MA 1979     6  13 1979-06-13   Wed    198\n193    MA 1979     6  18 1979-06-18   Mon    198\n194    MA 1979     7  31 1979-07-31  Tues    198\n195    MA 1979     8  26 1979-08-26   Sun    198\n196    MA 1979     9  13 1979-09-13 Thurs    198\n197    MA 1979    11  13 1979-11-13  Tues    198\n198    MA 1979    12   7 1979-12-07   Fri    198\n199    MA 1979     5  31 1979-05-31 Thurs    197\n200    MA 1979    11  27 1979-11-27  Tues    197\n201    MA 1979    12   6 1979-12-06 Thurs    197\n202    MA 1979     2  21 1979-02-21   Wed    196\n203    MA 1979     4  17 1979-04-17  Tues    196\n204    MA 1979     4  21 1979-04-21   Sat    196\n205    MA 1979     6  25 1979-06-25   Mon    196\n206    MA 1979     8   4 1979-08-04   Sat    196\n207    MA 1979     8   7 1979-08-07  Tues    196\n208    MA 1979    10   8 1979-10-08   Mon    196\n209    MA 1979    10  11 1979-10-11 Thurs    196\n210    MA 1979     1  13 1979-01-13   Sat    195\n211    MA 1979     4  13 1979-04-13   Fri    195\n212    MA 1979     1  19 1979-01-19   Fri    194\n213    MA 1979     3  16 1979-03-16   Fri    194\n214    MA 1979     5   7 1979-05-07   Mon    194\n215    MA 1979     9  12 1979-09-12   Wed    194\n216    MA 1979    11   5 1979-11-05   Mon    194\n217    MA 1979     1  10 1979-01-10   Wed    193\n218    MA 1979     1  30 1979-01-30  Tues    193\n219    MA 1979     3  11 1979-03-11   Sun    193\n220    MA 1979     6  17 1979-06-17   Sun    193\n221    MA 1979     8  27 1979-08-27   Mon    193\n222    MA 1979     9   9 1979-09-09   Sun    193\n223    MA 1979    12  13 1979-12-13 Thurs    193\n224    MA 1979     1   5 1979-01-05   Fri    192\n225    MA 1979     2  19 1979-02-19   Mon    192\n226    MA 1979     3   1 1979-03-01 Thurs    192\n227    MA 1979     7  14 1979-07-14   Sat    192\n228    MA 1979    11  29 1979-11-29 Thurs    192\n229    MA 1979     1   8 1979-01-08   Mon    191\n230    MA 1979     1  12 1979-01-12   Fri    191\n231    MA 1979     2  15 1979-02-15 Thurs    191\n232    MA 1979     3  24 1979-03-24   Sat    191\n233    MA 1979    10  28 1979-10-28   Sun    191\n234    MA 1979     4   2 1979-04-02   Mon    190\n235    MA 1979     5  27 1979-05-27   Sun    190\n236    MA 1979    12   3 1979-12-03   Mon    190\n237    MA 1979    12  12 1979-12-12   Wed    190\n238    MA 1979     4   1 1979-04-01   Sun    189\n239    MA 1979     4   5 1979-04-05 Thurs    189\n240    MA 1979     8  22 1979-08-22   Wed    189\n241    MA 1979    12  26 1979-12-26   Wed    189\n242    MA 1979     1  24 1979-01-24   Wed    188\n243    MA 1979     7  16 1979-07-16   Mon    188\n244    MA 1979    11  21 1979-11-21   Wed    188\n245    MA 1979     4  16 1979-04-16   Mon    187\n246    MA 1979     6  10 1979-06-10   Sun    187\n247    MA 1979     8  11 1979-08-11   Sat    187\n248    MA 1979     2   7 1979-02-07   Wed    186\n249    MA 1979     3  17 1979-03-17   Sat    186\n250    MA 1979     3  27 1979-03-27  Tues    186\n251    MA 1979     4   7 1979-04-07   Sat    186\n252    MA 1979     5   9 1979-05-09   Wed    186\n253    MA 1979     5  15 1979-05-15  Tues    186\n254    MA 1979     5  17 1979-05-17 Thurs    186\n255    MA 1979     7   7 1979-07-07   Sat    186\n256    MA 1979     9  15 1979-09-15   Sat    186\n257    MA 1979    10   7 1979-10-07   Sun    186\n258    MA 1979     3  12 1979-03-12   Mon    185\n259    MA 1979     3  20 1979-03-20  Tues    185\n260    MA 1979    12   5 1979-12-05   Wed    185\n261    MA 1979     1   4 1979-01-04 Thurs    184\n262    MA 1979     2   1 1979-02-01 Thurs    184\n263    MA 1979     5  29 1979-05-29  Tues    184\n264    MA 1979     9   3 1979-09-03   Mon    184\n265    MA 1979    11   8 1979-11-08 Thurs    184\n266    MA 1979    12  19 1979-12-19   Wed    184\n267    MA 1979    12  21 1979-12-21   Fri    184\n268    MA 1979     1  23 1979-01-23  Tues    183\n269    MA 1979     4  28 1979-04-28   Sat    183\n270    MA 1979     6  24 1979-06-24   Sun    183\n271    MA 1979     7  28 1979-07-28   Sat    183\n272    MA 1979     7  29 1979-07-29   Sun    183\n273    MA 1979     8  19 1979-08-19   Sun    183\n274    MA 1979    10  21 1979-10-21   Sun    183\n275    MA 1979    10  18 1979-10-18 Thurs    182\n276    MA 1979     1   9 1979-01-09  Tues    180\n277    MA 1979     2  11 1979-02-11   Sun    180\n278    MA 1979     5  19 1979-05-19   Sat    180\n279    MA 1979     5  28 1979-05-28   Mon    180\n280    MA 1979     6   3 1979-06-03   Sun    180\n281    MA 1979    10  20 1979-10-20   Sat    180\n282    MA 1979    10  31 1979-10-31   Wed    180\n283    MA 1979    12  18 1979-12-18  Tues    180\n284    MA 1979     1  31 1979-01-31   Wed    179\n285    MA 1979     3  10 1979-03-10   Sat    179\n286    MA 1979     4  29 1979-04-29   Sun    179\n287    MA 1979     7   5 1979-07-05 Thurs    179\n288    MA 1979     8  30 1979-08-30 Thurs    179\n289    MA 1979     1  25 1979-01-25 Thurs    178\n290    MA 1979     5  26 1979-05-26   Sat    178\n291    MA 1979    10  14 1979-10-14   Sun    178\n292    MA 1979     2  28 1979-02-28   Wed    177\n293    MA 1979     7   8 1979-07-08   Sun    177\n294    MA 1979     2  25 1979-02-25   Sun    176\n295    MA 1979     4  15 1979-04-15   Sun    176\n296    MA 1979     9  22 1979-09-22   Sat    176\n297    MA 1979     6  23 1979-06-23   Sat    175\n298    MA 1979     7  22 1979-07-22   Sun    175\n299    MA 1979    10  27 1979-10-27   Sat    175\n300    MA 1979     2  10 1979-02-10   Sat    174\n301    MA 1979     8  25 1979-08-25   Sat    174\n302    MA 1979     9  16 1979-09-16   Sun    174\n303    MA 1979    11  12 1979-11-12   Mon    174\n304    MA 1979    11  17 1979-11-17   Sat    174\n305    MA 1979     6   2 1979-06-02   Sat    173\n306    MA 1979     7   1 1979-07-01   Sun    173\n307    MA 1979    10  13 1979-10-13   Sat    173\n308    MA 1979     3  25 1979-03-25   Sun    172\n309    MA 1979     6  16 1979-06-16   Sat    172\n310    MA 1979    11  10 1979-11-10   Sat    172\n311    MA 1979     2  13 1979-02-13  Tues    171\n312    MA 1979     3  31 1979-03-31   Sat    171\n313    MA 1979     6   9 1979-06-09   Sat    171\n314    MA 1979    11  25 1979-11-25   Sun    171\n315    MA 1979    12  22 1979-12-22   Sat    171\n316    MA 1979     5   6 1979-05-06   Sun    170\n317    MA 1979     9   2 1979-09-02   Sun    170\n318    MA 1979     1   6 1979-01-06   Sat    169\n319    MA 1979     8  12 1979-08-12   Sun    169\n320    MA 1979     9   8 1979-09-08   Sat    169\n321    MA 1979    11   3 1979-11-03   Sat    169\n322    MA 1979    12  15 1979-12-15   Sat    169\n323    MA 1979     2  18 1979-02-18   Sun    168\n324    MA 1979     9   1 1979-09-01   Sat    168\n325    MA 1979    11  18 1979-11-18   Sun    166\n326    MA 1979    12   1 1979-12-01   Sat    166\n327    MA 1979     2  24 1979-02-24   Sat    165\n328    MA 1979     4  22 1979-04-22   Sun    165\n329    MA 1979     4  14 1979-04-14   Sat    164\n330    MA 1979     7   4 1979-07-04   Wed    164\n331    MA 1979     9  23 1979-09-23   Sun    164\n332    MA 1979    12   2 1979-12-02   Sun    164\n333    MA 1979     6   4 1979-06-04   Mon    163\n334    MA 1979    12  16 1979-12-16   Sun    163\n335    MA 1979     1  21 1979-01-21   Sun    162\n336    MA 1979     1  27 1979-01-27   Sat    162\n337    MA 1979     5  12 1979-05-12   Sat    162\n338    MA 1979     9  30 1979-09-30   Sun    162\n339    MA 1979    12  29 1979-12-29   Sat    161\n340    MA 1979    12  23 1979-12-23   Sun    160\n341    MA 1979     2   3 1979-02-03   Sat    159\n342    MA 1979     8  18 1979-08-18   Sat    159\n343    MA 1979     1  20 1979-01-20   Sat    158\n344    MA 1979     2  17 1979-02-17   Sat    156\n345    MA 1979     3   3 1979-03-03   Sat    156\n346    MA 1979     3   4 1979-03-04   Sun    156\n347    MA 1979    11  24 1979-11-24   Sat    156\n348    MA 1979    12   9 1979-12-09   Sun    156\n349    MA 1979     1   7 1979-01-07   Sun    155\n350    MA 1979     1  14 1979-01-14   Sun    155\n351    MA 1979     1  17 1979-01-17   Wed    155\n352    MA 1979     4   8 1979-04-08   Sun    155\n353    MA 1979    11   4 1979-11-04   Sun    155\n354    MA 1979    12  20 1979-12-20 Thurs    154\n355    MA 1979     2   4 1979-02-04   Sun    153\n356    MA 1979     5  20 1979-05-20   Sun    153\n357    MA 1979     3  18 1979-03-18   Sun    152\n358    MA 1979    12  30 1979-12-30   Sun    152\n359    MA 1979    12   8 1979-12-08   Sat    151\n360    MA 1979    12  25 1979-12-25  Tues    150\n361    MA 1979     5   5 1979-05-05   Sat    148\n362    MA 1979    11  22 1979-11-22 Thurs    147\n363    MA 1979     1  28 1979-01-28   Sun    146\n364    MA 1979     1   1 1979-01-01   Mon    144\n365    MA 1979     5  13 1979-05-13   Sun    143\n\n\nPart b\nMake a table showing the five states with the most births between September 9, 1979 and September 12, 1979, including the 9th and 12th. Arrange the table in descending order of births.\n\nBirthdays |&gt;\n  filter(date &gt;= \"1979-09-09\", date &lt;= \"1979-09-12\") |&gt;\n  group_by(state) |&gt;\n  summarize(total_births = sum(births), .groups = \"drop\") |&gt;\n  arrange(desc(total_births)) |&gt;\n  slice_head(n = 5)\n\n# A tibble: 5 × 2\n  state total_births\n  &lt;chr&gt;        &lt;int&gt;\n1 CA            3454\n2 TX            2467\n3 NY            2036\n4 IL            1758\n5 OH            1527",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Dates</span>"
    ]
  },
  {
    "objectID": "ica/ica-dates.html#solutions",
    "href": "ica/ica-dates.html#solutions",
    "title": "\n18  Dates\n",
    "section": "\n18.4 Solutions",
    "text": "18.4 Solutions\n\nClick for Solutions\nExample 1: Single Verb\n\nggplot(penguins, aes(y = body_mass_g, x = bill_length_mm, color = species)) + \n  geom_point() + \n  facet_wrap(~ sex)\n\n\n\n\n\n\n\n\n\n# Get data on only Adelie penguins that weigh more than 4700g\npenguins |&gt; \n  filter(species == \"Adelie\", body_mass_g &gt; 4700)\n\n# A tibble: 2 × 8\n  species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;chr&gt;   &lt;chr&gt;           &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;       &lt;dbl&gt;\n1 Adelie  Biscoe           41              20               203        4725\n2 Adelie  Biscoe           43.2            19               197        4775\n# ℹ 2 more variables: sex &lt;chr&gt;, year &lt;dbl&gt;\n\n# Get data on penguin body mass only\n# Show just the first 6 rows\npenguins |&gt; \n  select(body_mass_g) |&gt; \n  head()\n\n# A tibble: 6 × 1\n  body_mass_g\n        &lt;dbl&gt;\n1        3750\n2        3800\n3        3250\n4          NA\n5        3450\n6        3650\n\n# Sort the penguins from smallest to largest body mass\n# Show just the first 6 rows\npenguins |&gt; \n  arrange(body_mass_g) |&gt; \n  head()\n\n# A tibble: 6 × 8\n  species   island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;chr&gt;     &lt;chr&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;       &lt;dbl&gt;\n1 Chinstrap Dream               46.9          16.6               192        2700\n2 Adelie    Biscoe              36.5          16.6               181        2850\n3 Adelie    Biscoe              36.4          17.1               184        2850\n4 Adelie    Biscoe              34.5          18.1               187        2900\n5 Adelie    Dream               33.1          16.1               178        2900\n6 Adelie    Torgersen           38.6          17                 188        2900\n# ℹ 2 more variables: sex &lt;chr&gt;, year &lt;dbl&gt;\n\n# Calculate the average body mass across all penguins\n# Note: na.rm = TRUE removes the NAs from the calculation\npenguins |&gt; \n  summarize(mean = mean(body_mass_g, na.rm = TRUE))\n\n# A tibble: 1 × 1\n   mean\n  &lt;dbl&gt;\n1 4202.\n\n# Calculate the average body mass by species\npenguins |&gt; \n  group_by(species) |&gt; \n  summarize(mean = mean(body_mass_g, na.rm = TRUE))\n\n# A tibble: 3 × 2\n  species    mean\n  &lt;chr&gt;     &lt;dbl&gt;\n1 Adelie    3701.\n2 Chinstrap 3733.\n3 Gentoo    5076.\n\n# Create a new column that records body mass in kilograms, not grams\n# NOTE: there are 1000 g in 1 kg\n# Show just the first 6 rows\npenguins |&gt; \n  mutate(body_mass_kg = body_mass_g/1000) |&gt; \n  head()\n\n# A tibble: 6 × 9\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;       &lt;dbl&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           NA            NA                  NA          NA\n5 Adelie  Torgersen           36.7          19.3               193        3450\n6 Adelie  Torgersen           39.3          20.6               190        3650\n# ℹ 3 more variables: sex &lt;chr&gt;, year &lt;dbl&gt;, body_mass_kg &lt;dbl&gt;\n\n\n\n\nggplot(penguins, aes(x = species)) + \n  geom_bar()\n\n\n\n\n\n\npenguins |&gt; \n  group_by(species) |&gt; \n  summarize(n())\n\n# A tibble: 3 × 2\n  species   `n()`\n  &lt;chr&gt;     &lt;int&gt;\n1 Adelie      152\n2 Chinstrap    68\n3 Gentoo      124\n\npenguins |&gt; \n  count(species)\n\n# A tibble: 3 × 2\n  species       n\n  &lt;chr&gt;     &lt;int&gt;\n1 Adelie      152\n2 Chinstrap    68\n3 Gentoo      124\n\n\nExample 2: Multiple Verbs\n\n# Sort Gentoo penguins from biggest to smallest with respect to their \n# bill length in cm (there are 10 mm in a cm)\npenguins |&gt; \n  filter(species == \"Gentoo\") |&gt; \n  mutate(bill_length_cm = bill_length_mm / 10) |&gt; \n  arrange(desc(bill_length_cm))\n\n# A tibble: 124 × 9\n   species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;chr&gt;   &lt;chr&gt;           &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;       &lt;dbl&gt;\n 1 Gentoo  Biscoe           59.6          17                 230        6050\n 2 Gentoo  Biscoe           55.9          17                 228        5600\n 3 Gentoo  Biscoe           55.1          16                 230        5850\n 4 Gentoo  Biscoe           54.3          15.7               231        5650\n 5 Gentoo  Biscoe           53.4          15.8               219        5500\n 6 Gentoo  Biscoe           52.5          15.6               221        5450\n 7 Gentoo  Biscoe           52.2          17.1               228        5400\n 8 Gentoo  Biscoe           52.1          17                 230        5550\n 9 Gentoo  Biscoe           51.5          16.3               230        5500\n10 Gentoo  Biscoe           51.3          14.2               218        5300\n# ℹ 114 more rows\n# ℹ 3 more variables: sex &lt;chr&gt;, year &lt;dbl&gt;, bill_length_cm &lt;dbl&gt;\n\n# Sort the species from smallest to biggest with respect to their \n# average bill length in cm\npenguins |&gt; \n  mutate(bill_length_cm = bill_length_mm / 10) |&gt; \n  group_by(species) |&gt; \n  summarize(mean_bill_length = mean(bill_length_cm, na.rm = TRUE)) |&gt; \n  arrange(desc(mean_bill_length))\n\n# A tibble: 3 × 2\n  species   mean_bill_length\n  &lt;chr&gt;                &lt;dbl&gt;\n1 Chinstrap             4.88\n2 Gentoo                4.75\n3 Adelie                3.88\n\n\nExample 3: Interpret Code\nExercise 1: More Filtering\nPart a\n\n# Create a dataset with just Adelie and Chinstrap using %in%\n# Pipe this into `count(species)` to confirm that you only have these 2 species\npenguins |&gt;\n  filter(species %in% c(\"Adelie\", \"Chinstrap\")) |&gt;\n  count(species)\n\n# A tibble: 2 × 2\n  species       n\n  &lt;chr&gt;     &lt;int&gt;\n1 Adelie      152\n2 Chinstrap    68\n\n\n\n# Create a dataset with just Adelie and Chinstrap using !=\n# Pipe this into `count(species)` to confirm that you only have these 2 species\npenguins |&gt;\n  filter(species != \"Gentoo\") |&gt;\n  count(species)\n\n# A tibble: 2 × 2\n  species       n\n  &lt;chr&gt;     &lt;int&gt;\n1 Adelie      152\n2 Chinstrap    68\n\n\nPart b\nPart c\nIt might get rid of data points even if they have complete information on the variables we need, just because they’re missing info on variables we don’t need.\nExercise 2: More selecting\n\n# First: recall the variable names\nnames(penguins)\n\n[1] \"species\"           \"island\"            \"bill_length_mm\"   \n[4] \"bill_depth_mm\"     \"flipper_length_mm\" \"body_mass_g\"      \n[7] \"sex\"               \"year\"             \n\n\n\n# Use a shortcut to keep everything but the year and island variables\npenguins |&gt; \n  select(-year, -island)\n\n# A tibble: 344 × 6\n   species bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex   \n   &lt;chr&gt;            &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt; \n 1 Adelie            39.1          18.7               181        3750 male  \n 2 Adelie            39.5          17.4               186        3800 female\n 3 Adelie            40.3          18                 195        3250 female\n 4 Adelie            NA            NA                  NA          NA &lt;NA&gt;  \n 5 Adelie            36.7          19.3               193        3450 female\n 6 Adelie            39.3          20.6               190        3650 male  \n 7 Adelie            38.9          17.8               181        3625 female\n 8 Adelie            39.2          19.6               195        4675 male  \n 9 Adelie            34.1          18.1               193        3475 &lt;NA&gt;  \n10 Adelie            42            20.2               190        4250 &lt;NA&gt;  \n# ℹ 334 more rows\n\n\n\n# Use a shortcut to keep only species and the penguin characteristics measured in mm\npenguins |&gt; \n  select(species, ends_with(\"mm\"))\n\n# A tibble: 344 × 4\n   species bill_length_mm bill_depth_mm flipper_length_mm\n   &lt;chr&gt;            &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;\n 1 Adelie            39.1          18.7               181\n 2 Adelie            39.5          17.4               186\n 3 Adelie            40.3          18                 195\n 4 Adelie            NA            NA                  NA\n 5 Adelie            36.7          19.3               193\n 6 Adelie            39.3          20.6               190\n 7 Adelie            38.9          17.8               181\n 8 Adelie            39.2          19.6               195\n 9 Adelie            34.1          18.1               193\n10 Adelie            42            20.2               190\n# ℹ 334 more rows\n\n\n\n# Use a shortcut to keep only species and bill-related measurements\npenguins |&gt; \n  select(species, starts_with(\"bill\"))\n\n# A tibble: 344 × 3\n   species bill_length_mm bill_depth_mm\n   &lt;chr&gt;            &lt;dbl&gt;         &lt;dbl&gt;\n 1 Adelie            39.1          18.7\n 2 Adelie            39.5          17.4\n 3 Adelie            40.3          18  \n 4 Adelie            NA            NA  \n 5 Adelie            36.7          19.3\n 6 Adelie            39.3          20.6\n 7 Adelie            38.9          17.8\n 8 Adelie            39.2          19.6\n 9 Adelie            34.1          18.1\n10 Adelie            42            20.2\n# ℹ 334 more rows\n\n\n\n# Use a shortcut to keep only species and the length-related characteristics\npenguins |&gt; \n  select(species, contains(\"length\"))\n\n# A tibble: 344 × 3\n   species bill_length_mm flipper_length_mm\n   &lt;chr&gt;            &lt;dbl&gt;             &lt;dbl&gt;\n 1 Adelie            39.1               181\n 2 Adelie            39.5               186\n 3 Adelie            40.3               195\n 4 Adelie            NA                  NA\n 5 Adelie            36.7               193\n 6 Adelie            39.3               190\n 7 Adelie            38.9               181\n 8 Adelie            39.2               195\n 9 Adelie            34.1               193\n10 Adelie            42                 190\n# ℹ 334 more rows\n\n\nExercise 3: Arranging, counting, & grouping by multiple variables\n\n# Change this code to sort the penguins by species, and then island name\n# NOTE: The first row should be an Adelie penguin living on Biscoe island\npenguins |&gt; \n  arrange(species, island) |&gt; \n  head()\n\n# A tibble: 6 × 8\n  species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;chr&gt;   &lt;chr&gt;           &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;       &lt;dbl&gt;\n1 Adelie  Biscoe           37.8          18.3               174        3400\n2 Adelie  Biscoe           37.7          18.7               180        3600\n3 Adelie  Biscoe           35.9          19.2               189        3800\n4 Adelie  Biscoe           38.2          18.1               185        3950\n5 Adelie  Biscoe           38.8          17.2               180        3800\n6 Adelie  Biscoe           35.3          18.9               187        3800\n# ℹ 2 more variables: sex &lt;chr&gt;, year &lt;dbl&gt;\n\n\n\n# Change this code to count the number of male/female penguins observed for each species\npenguins |&gt; \n  count(species, sex)\n\n# A tibble: 8 × 3\n  species   sex        n\n  &lt;chr&gt;     &lt;chr&gt;  &lt;int&gt;\n1 Adelie    female    73\n2 Adelie    male      73\n3 Adelie    &lt;NA&gt;       6\n4 Chinstrap female    34\n5 Chinstrap male      34\n6 Gentoo    female    58\n7 Gentoo    male      61\n8 Gentoo    &lt;NA&gt;       5\n\n\n\n# Change this code to calculate the average body mass by species and sex\npenguins |&gt; \n  group_by(species, sex) |&gt; \n  summarize(mean = mean(body_mass_g, na.rm = TRUE))\n\n# A tibble: 8 × 3\n# Groups:   species [3]\n  species   sex     mean\n  &lt;chr&gt;     &lt;chr&gt;  &lt;dbl&gt;\n1 Adelie    female 3369.\n2 Adelie    male   4043.\n3 Adelie    &lt;NA&gt;   3540 \n4 Chinstrap female 3527.\n5 Chinstrap male   3939.\n6 Gentoo    female 4680.\n7 Gentoo    male   5485.\n8 Gentoo    &lt;NA&gt;   4588.\n\n\nExercise 4: Dates\n\n# Get today's date\nas.Date(today())\n\n[1] \"2025-04-23\"\n\n# Let's store this as \"today\" so we can work with it below\ntoday &lt;- as.Date(today())\n\n# Check out the class of this object\nclass(today)\n\n[1] \"Date\"\n\n\n\n# Records just the 4-digit year\nyear(today)\n\n[1] 2025\n\n\n\n# Today's month, as a number or label\nmonth(today)\n\n[1] 4\n\nmonth(today, label = TRUE)\n\n[1] Apr\n12 Levels: Jan &lt; Feb &lt; Mar &lt; Apr &lt; May &lt; Jun &lt; Jul &lt; Aug &lt; Sep &lt; ... &lt; Dec\n\n\n\n# This is the week of the year (1-52)\nweek(today)\n\n[1] 17\n\n\n\n# Day of the month (1-31) and day of the year (1-366)\nmday(today)\n\n[1] 23\n\nyday(today)  # This is often called the \"Julian day\"\n\n[1] 113\n\n\n\n# Day of the week as a number or label\nwday(today)\n\n[1] 4\n\nwday(today, label = TRUE)\n\n[1] Wed\nLevels: Sun &lt; Mon &lt; Tue &lt; Wed &lt; Thu &lt; Fri &lt; Sat\n\n\n\n# today is on or after Feb 14, 2024\ntoday &gt;= ymd(\"2024-02-14\")\n\n[1] TRUE\n\n# today is not before Feb 14, 2024\ntoday &lt; ymd(\"2024-02-14\")\n\n[1] FALSE",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Dates</span>"
    ]
  },
  {
    "objectID": "ica/ica-dates.html#footnotes",
    "href": "ica/ica-dates.html#footnotes",
    "title": "\n18  Dates\n",
    "section": "",
    "text": "The fivethirtyeight package has more recent data.↩︎",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Dates</span>"
    ]
  },
  {
    "objectID": "ica/ica-joining.html",
    "href": "ica/ica-joining.html",
    "title": "\n19  Joining\n",
    "section": "",
    "text": "19.1 Review\nWhere are we? Data preparation\nThus far, we’ve learned how to:",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Joining</span>"
    ]
  },
  {
    "objectID": "ica/ica-joining.html#review",
    "href": "ica/ica-joining.html#review",
    "title": "\n19  Joining\n",
    "section": "",
    "text": "arrange() our data in a meaningful order\nsubset the data to only filter() the rows and select() the columns of interest\n\nmutate() existing variables and define new variables\n\nsummarize() various aspects of a variable, both overall and by group (group_by())\nreshape our data to fit the task at hand (pivot_longer(), pivot_wider())",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Joining</span>"
    ]
  },
  {
    "objectID": "ica/ica-joining.html#motivation",
    "href": "ica/ica-joining.html#motivation",
    "title": "\n19  Joining\n",
    "section": "\n19.2 Motivation",
    "text": "19.2 Motivation\nIn practice, we often have to collect and combine data from various sources in order to address our research questions. Example:\n\nWhat are the best predictors of album sales?\nCombine:\n\nSpotify data on individual songs (eg: popularity, genre, characteristics)\nsales data on individual songs\n\n\nWhat are the best predictors of flight delays?\nCombine:\n\ndata on individual flights including airline, starting airport, and destination airport\ndata on different airlines (eg: ticket prices, reliability, etc)\ndata on different airports (eg: location, reliability, etc)\n\n\n\nExample 1\nConsider the following (made up) data on students and course enrollments:\n\nstudents_1 &lt;- data.frame(\n  student = c(\"A\", \"B\", \"C\"),\n  class = c(\"STAT 101\", \"GEOL 101\", \"ANTH 101\")\n)\n\n# Check it out\nstudents_1\n\n  student    class\n1       A STAT 101\n2       B GEOL 101\n3       C ANTH 101\n\n\n\nenrollments_1 &lt;- data.frame(\n  class = c(\"STAT 101\", \"ART 101\", \"GEOL 101\"),\n  enrollment = c(18, 17, 24)\n)\n\n# Check it out\nenrollments_1\n\n     class enrollment\n1 STAT 101         18\n2  ART 101         17\n3 GEOL 101         24\n\n\nOur goal is to combine or join these datasets into one. For reference, here they are side by side:\n\nFirst, consider the following:\n\nWhat variable or key do these datasets have in common? Thus by what information can we match the observations in these datasets?\nRelative to this key, what info does students_1 have that enrollments_1 doesn’t?\nRelative to this key, what info does enrollments_1 have that students_1 doesn’t?",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Joining</span>"
    ]
  },
  {
    "objectID": "ica/ica-joining.html#mutating-joins-left-inner-full",
    "href": "ica/ica-joining.html#mutating-joins-left-inner-full",
    "title": "\n19  Joining\n",
    "section": "\n19.3 Mutating Joins: left, inner, full\n",
    "text": "19.3 Mutating Joins: left, inner, full\n\nExample 2\nThere are various ways to join these datasets:\n\nLet’s learn by doing. First, try the left_join() function:\n\nlibrary(tidyverse)\nstudents_1 |&gt; \n  left_join(enrollments_1)\n\n  student    class enrollment\n1       A STAT 101         18\n2       B GEOL 101         24\n3       C ANTH 101         NA\n\n\n\nWhat did this do? What are the roles of students_1 (the left table) and enrollments_1 (the right table)?\nWhat, if anything, would change if we reversed the order of the data tables? Think about it, then try.\nExample 3\nNext, explore how our datasets are joined using inner_join():\n\n\nstudents_1 |&gt; \n  inner_join(enrollments_1)\n\n  student    class enrollment\n1       A STAT 101         18\n2       B GEOL 101         24\n\n\n\nWhat did this do? What are the roles of students_1 (the left table) and enrollments_1 (the right table)?\nWhat, if anything, would change if we reversed the order of the data tables? Think about it, then try.\nExample 4\nNext, explore how our datasets are joined using full_join():\n\n\nstudents_1 |&gt; \n  full_join(enrollments_1)\n\n  student    class enrollment\n1       A STAT 101         18\n2       B GEOL 101         24\n3       C ANTH 101         NA\n4    &lt;NA&gt;  ART 101         17\n\n\n\nWhat did this do? What are the roles of students_1 (the left table) and enrollments_1 (the right table)?\nWhat, if anything, would change if we reversed the order of the data tables? Think about it, then try.\n\n19.3.1 Summary\nMutating joins add new variables (columns) to the left data table from matching observations in the right table:\nleft_data |&gt; mutating_join(right_data)\nThe most common mutating joins are:\n\nleft_join()\nKeeps all observations from the left, but discards any observations in the right that do not have a match in the left.1\ninner_join()\nKeeps only the observations from the left with a match in the right.\nfull_join()\nKeeps all observations from the left and the right. (This is less common than left_join() and inner_join()).\n\nNOTE: When an observation in the left table has multiple matches in the right table, these mutating joins produce a separate observation in the new table for each match.",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Joining</span>"
    ]
  },
  {
    "objectID": "ica/ica-joining.html#filtering-joins-semi-anti",
    "href": "ica/ica-joining.html#filtering-joins-semi-anti",
    "title": "\n19  Joining\n",
    "section": "\n19.4 Filtering Joins: semi, anti\n",
    "text": "19.4 Filtering Joins: semi, anti\n\nMutating joins combine information, thus increase the number of columns in a dataset (like mutate()). Filtering joins keep only certain observations in one dataset (like filter()), not based on rules related to any variables in the dataset, but on the observations that exist in another dataset. This is useful when we merely care about the membership or non-membership of an observation in the other dataset, not the raw data itself.\nExample 5\nIn our example data, suppose enrollments_1 only included courses being taught in the Theater building:\n\n\nstudents_1 |&gt; \n  semi_join(enrollments_1)\n\n  student    class\n1       A STAT 101\n2       B GEOL 101\n\n\n\nWhat did this do? What info would it give us?\nHow does semi_join() differ from inner_join()?\nWhat, if anything, would change if we reversed the order of the data tables? Think about it, then try.\nExample 6\nLet’s try another filtering join for our example data:\n\n\nstudents_1 |&gt; \n  anti_join(enrollments_1)\n\n  student    class\n1       C ANTH 101\n\n\n\nWhat did this do? What info would it give us?\nWhat, if anything, would change if we reversed the order of the data tables? Think about it, then try.\n\n19.4.1 Summary\nFiltering joins keep specific observations from the left table based on whether they match an observation in the right table.\n\nsemi_join()\nDiscards any observations in the left table that do not have a match in the right table. If there are multiple matches of right cases to a left case, it keeps just one copy of the left case.\nanti_join()\nDiscards any observations in the left table that do have a match in the right table.",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Joining</span>"
    ]
  },
  {
    "objectID": "ica/ica-joining.html#summary-of-all-joins",
    "href": "ica/ica-joining.html#summary-of-all-joins",
    "title": "\n19  Joining\n",
    "section": "\n19.5 Summary of All Joins",
    "text": "19.5 Summary of All Joins",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Joining</span>"
    ]
  },
  {
    "objectID": "ica/ica-joining.html#exercises",
    "href": "ica/ica-joining.html#exercises",
    "title": "\n19  Joining\n",
    "section": "\n19.6 Exercises",
    "text": "19.6 Exercises\nExercise 1: Where are my keys?\nPart a\nDefine two new datasets, with different students and courses:\n\nstudents_2 &lt;- data.frame(\n  student = c(\"D\", \"E\", \"F\"),\n  class = c(\"COMP 101\", \"BIOL 101\", \"POLI 101\")\n)\n\n# Check it out\nstudents_2\n\n  student    class\n1       D COMP 101\n2       E BIOL 101\n3       F POLI 101\n\nenrollments_2 &lt;- data.frame(\n  course = c(\"ART 101\", \"BIOL 101\", \"COMP 101\"),\n  enrollment = c(18, 20, 19)\n)\n\n# Check it out\nenrollments_2\n\n    course enrollment\n1  ART 101         18\n2 BIOL 101         20\n3 COMP 101         19\n\n\nTo connect the course enrollments to the students’ courses, try do a left_join(). You get an error! Identify the problem by reviewing the error message and the datasets we’re trying to join.\n\n# eval = FALSE: don't evaluate this chunk when knitting. it produces an error.\nstudents_2 |&gt; \n  left_join(enrollments_2)\n\nPart b\nThe problem is that course name, the key or variable that links these two datasets, is labeled differently: class in the students_2 data and course in the enrollments_2 data. Thus we have to specify these keys in our code:\n\nstudents_2 |&gt; \n  left_join(enrollments_2, join_by(class == course))\n\n  student    class enrollment\n1       D COMP 101         19\n2       E BIOL 101         20\n3       F POLI 101         NA\n\n\n\n# The order of the keys is important:\n# join_by(\"left data key\" == \"right data key\")\n# The order is mixed up here, thus we get an error:\nstudents_2 |&gt; \n  left_join(enrollments_2, join_by(course == class))\n\nPart c\nDefine another set of fake data which adds grade information:\n\n# Add student grades in each course\nstudents_3 &lt;- data.frame(\n  student = c(\"Y\", \"Y\", \"Z\", \"Z\"),\n  class = c(\"COMP 101\", \"BIOL 101\", \"POLI 101\", \"COMP 101\"),\n  grade = c(\"B\", \"S\", \"C\", \"A\")\n)\n\n# Check it out\nstudents_3\n\n  student    class grade\n1       Y COMP 101     B\n2       Y BIOL 101     S\n3       Z POLI 101     C\n4       Z COMP 101     A\n\n# Add average grades in each course\nenrollments_3 &lt;- data.frame(\n  class = c(\"ART 101\", \"BIOL 101\",\"COMP 101\"),\n  grade = c(\"B\", \"A\", \"A-\"),\n  enrollment = c(20, 18, 19)\n)\n\n# Check it out\nenrollments_3\n\n     class grade enrollment\n1  ART 101     B         20\n2 BIOL 101     A         18\n3 COMP 101    A-         19\n\n\nTry doing a left_join() to link the students’ classes to their enrollment info. Did this work? Try and figure out the culprit by examining the output.\n\nstudents_3 |&gt; \n  left_join(enrollments_3)\n\n  student    class grade enrollment\n1       Y COMP 101     B         NA\n2       Y BIOL 101     S         NA\n3       Z POLI 101     C         NA\n4       Z COMP 101     A         NA\n\n\nPart d\nThe issue here is that our datasets have 2 column names in common: class and grade. BUT grade is measuring 2 different things here: individual student grades in students_3 and average student grades in enrollments_3. Thus it doesn’t make sense to try to join the datasets with respect to this variable. We can again solve this by specifying that we want to join the datasets using the class variable as a key. What are grade.x and grade.y?\n\nstudents_3 |&gt; \n  left_join(enrollments_3, join_by(class == class))\n\n  student    class grade.x grade.y enrollment\n1       Y COMP 101       B      A-         19\n2       Y BIOL 101       S       A         18\n3       Z POLI 101       C    &lt;NA&gt;         NA\n4       Z COMP 101       A      A-         19\n\n\nExercise 2: More small practice\nBefore applying these ideas to bigger datasets, let’s practice identifying which join is appropriate in different scenarios. Define the following fake data on voters (people who have voted) and contact info for voting age adults (people who could vote):\n\n# People who have voted\nvoters &lt;- data.frame(\n  id = c(\"A\", \"D\", \"E\", \"F\", \"G\"),\n  times_voted = c(2, 4, 17, 6, 20)\n)\n\nvoters\n\n  id times_voted\n1  A           2\n2  D           4\n3  E          17\n4  F           6\n5  G          20\n\n# Contact info for voting age adults\ncontact &lt;- data.frame(\n  name = c(\"A\", \"B\", \"C\", \"D\"),\n  address = c(\"summit\", \"grand\", \"snelling\", \"fairview\"),\n  age = c(24, 89, 43, 38)\n)\n\ncontact\n\n  name  address age\n1    A   summit  24\n2    B    grand  89\n3    C snelling  43\n4    D fairview  38\n\n\nUse the appropriate join for each prompt below. In each case, think before you type:\n\nWhat dataset goes on the left?\nWhat do you want the resulting dataset to look like? How many rows and columns will it have?\n\n\n# 1. We want contact info for people who HAVEN'T voted\ncontact |&gt; anti_join(voters, by = c(\"name\" = \"id\"))\n\n  name  address age\n1    B    grand  89\n2    C snelling  43\n\n# 2. We want contact info for people who HAVE voted\ncontact |&gt; inner_join(voters, by = c(\"name\" = \"id\"))\n\n  name  address age times_voted\n1    A   summit  24           2\n2    D fairview  38           4\n\n# 3. We want any data available on each person\ncontact |&gt; full_join(voters, by = c(\"name\" = \"id\"))\n\n  name  address age times_voted\n1    A   summit  24           2\n2    B    grand  89          NA\n3    C snelling  43          NA\n4    D fairview  38           4\n5    E     &lt;NA&gt;  NA          17\n6    F     &lt;NA&gt;  NA           6\n7    G     &lt;NA&gt;  NA          20\n\n# 4. When possible, we want to add contact info to the voting roster\nvoters |&gt; left_join(contact, by = c(\"id\" = \"name\"))\n\n  id times_voted  address age\n1  A           2   summit  24\n2  D           4 fairview  38\n3  E          17     &lt;NA&gt;  NA\n4  F           6     &lt;NA&gt;  NA\n5  G          20     &lt;NA&gt;  NA\n\n\nExercise 3: Bigger datasets\nLet’s apply these ideas to some bigger datasets. In grades, each row is a student-class pair with information on:\n\n\nsid = student ID\n\ngrade = student’s grade\n\nsessionID = an identifier of the class section\n\n\n# Get rid of some duplicate rows!\ngrades &lt;- read.csv(\"https://mac-stat.github.io/data/grades.csv\") |&gt; \n  distinct(sid, sessionID, .keep_all = TRUE)\nhead(grades)\n\n     sid grade   sessionID\n1 S31185    D+ session1784\n2 S31185    B+ session1785\n3 S31185    A- session1791\n4 S31185    B+ session1792\n5 S31185    B- session1794\n6 S31185    C+ session1795\n\n\nIn courses, each row corresponds to a class section with information on:\n\n\nsessionID = an identifier of the class section\n\ndept = department\n\nlevel = course level (eg: 100)\n\nsem = semester\n\nenroll = enrollment (number of students)\n\niid = instructor ID\n\n\n\n    sessionID dept level    sem enroll     iid\n1 session1784    M   100 FA1991     22 inst265\n2 session1785    k   100 FA1991     52 inst458\n3 session1791    J   100 FA1993     22 inst223\n4 session1792    J   300 FA1993     20 inst235\n5 session1794    J   200 FA1993     22 inst234\n6 session1795    J   200 SP1994     26 inst230\n\n\nUse R code to take a quick glance at the data.\n\n# How many observations (rows) and variables (columns) are there in the grades data?\ndim(grades)\n\n[1] 5844    3\n\n# How many observations (rows) and variables (columns) are there in the courses data?\ndim(courses)\n\n[1] 1718    6\n\n\nExercise 4: Class size\nHow big are the classes?\nPart a\nBefore digging in, note that some courses are listed twice in the courses data:\n\ncourses |&gt; \n  count(sessionID) |&gt; \n  filter(n &gt; 1)\n\n     sessionID n\n1  session2047 2\n2  session2067 2\n3  session2448 2\n4  session2509 2\n5  session2541 2\n6  session2824 2\n7  session2826 2\n8  session2862 2\n9  session2897 2\n10 session3046 2\n11 session3057 2\n12 session3123 2\n13 session3243 2\n14 session3257 2\n15 session3387 2\n16 session3400 2\n17 session3414 2\n18 session3430 2\n19 session3489 2\n20 session3524 2\n21 session3629 2\n22 session3643 2\n23 session3821 2\n\n\nIf we pick out just 1 of these, we learn that some courses are cross-listed in multiple departments:\n\ncourses |&gt; \n  filter(sessionID == \"session2047\")\n\n    sessionID dept level    sem enroll     iid\n1 session2047    g   100 FA2001     12 inst436\n2 session2047    m   100 FA2001     28 inst436\n\n\nFor our class size exploration, obtain the total enrollments in each sessionID, combining any cross-listed sections. Save this as courses_combined. NOTE: There’s no joining to do here!\n\ncourses_combined &lt;- courses |&gt;\n  group_by(sessionID) |&gt;\n  summarize(enroll = sum(enroll), .groups = \"drop\")\n\n# Check that this has 1695 rows and 2 columns\ndim(courses_combined)\n\n[1] 1695    2\n\n\nPart b\nLet’s first examine the question of class size from the administration’s viewpoint. To this end, calculate the median class size across all class sections. (The median is the middle or 50th percentile. Unlike the mean, it’s not skewed by outliers.) THINK FIRST:\n\nWhich of the 2 datasets do you need to answer this question? One? Both?\nIf you need course information, use courses_combined not courses.\nDo you have to do any joining? If so, which dataset will go on the left, i.e. which dataset includes your primary observations of interest? Which join function will you need?\n\n\ncourses_combined |&gt; summarize(median_class_size = median(enroll))\n\n# A tibble: 1 × 1\n  median_class_size\n              &lt;int&gt;\n1                18\n\n\nPart c\nBut how big are classes from the student perspective? To this end, calculate the median class size for each individual student. Once you have the correct output, store it as student_class_size. THINK FIRST:\n\nWhich of the 2 datasets do you need to answer this question? One? Both?\nIf you need course information, use courses_combined not courses.\nDo you have to do any joining? If so, which dataset will go on the left, i.e. which dataset includes your primary observations of interest? Which join function will you need?\n\n\nstudent_class_size &lt;- grades |&gt;\n  left_join(courses_combined, by = \"sessionID\") |&gt;\n  group_by(sid) |&gt;\n  summarize(median_class_size = median(enroll), .groups = \"drop\")\nhead(student_class_size)\n\n# A tibble: 6 × 2\n  sid    median_class_size\n  &lt;chr&gt;              &lt;dbl&gt;\n1 S31185              23.5\n2 S31188              21  \n3 S31191              25  \n4 S31194              15  \n5 S31197              24  \n6 S31200              21  \n\n\nPart d\nThe median class size varies from student to student. To get a sense for the typical student experience and range in student experiences, construct and discuss a histogram of the median class sizes experienced by the students.\n\nggplot(student_class_size, aes(x = median_class_size)) +\n  geom_histogram(bins = 20) \n\n\n\n\n\n\n\nExercise 5: Narrowing in on classes\nPart a\nShow data on the students that enrolled in session1986. THINK FIRST: Which of the 2 datasets do you need to answer this question? One? Both?\n\ngrades |&gt;\n  filter(sessionID == \"session1986\")\n\n     sid grade   sessionID\n1 S31401    B+ session1986\n2 S32247     B session1986\n\n\nPart b\nBelow is a dataset with all courses in department E:\n\ndept_E &lt;- courses |&gt; \n  filter(dept == \"E\")\n\nWhat students enrolled in classes in department E? (We just want info on the students, not the classes.)\n\ngrades |&gt;\n  semi_join(dept_E, by = \"sessionID\") |&gt;\n  select(sid) |&gt;\n  distinct()\n\n     sid\n1 S31245\n2 S31470\n3 S31938\n4 S31968\n5 S32022\n6 S32046\n7 S32226\n8 S32415\n9 S32484\n\n\nExercise 6: All the wrangling\nUse all of your wrangling skills to answer the following prompts! THINK FIRST:\n\nThink about what tables you might need to join (if any). Identify the corresponding variables to match.\nYou’ll need an extra table to convert grades to grade point averages:\n\n\ngpa_conversion &lt;- tibble(\n  grade = c(\"A+\", \"A\", \"A-\", \"B+\", \"B\", \"B-\", \"C+\", \"C\", \"C-\", \"D+\", \"D\", \"D-\", \"NC\", \"AU\", \"S\"), \n  gp = c(4.3, 4, 3.7, 3.3, 3, 2.7, 2.3, 2, 1.7, 1.3, 1, 0.7, 0, NA, NA)\n)\n\ngpa_conversion\n\n# A tibble: 15 × 2\n   grade    gp\n   &lt;chr&gt; &lt;dbl&gt;\n 1 A+      4.3\n 2 A       4  \n 3 A-      3.7\n 4 B+      3.3\n 5 B       3  \n 6 B-      2.7\n 7 C+      2.3\n 8 C       2  \n 9 C-      1.7\n10 D+      1.3\n11 D       1  \n12 D-      0.7\n13 NC      0  \n14 AU     NA  \n15 S      NA  \n\n\nPart a\nHow many total student enrollments are there in each department? Order from high to low.\n\ngrades |&gt;\n  left_join(courses, by = \"sessionID\") |&gt;\n  count(dept, name = \"n\") |&gt;\n  arrange(desc(n))\n\n   dept   n\n1     d 483\n2     M 410\n3     m 363\n4     O 359\n5     W 336\n6     q 318\n7     F 296\n8     k 265\n9     j 249\n10    D 240\n11    C 237\n12    G 237\n13    R 195\n14    n 191\n15    i 177\n16    Q 157\n17    J 148\n18    X 145\n19    p 129\n20    e 128\n21    K 112\n22    H 110\n23    N  99\n24    S  97\n25    b  67\n26    T  62\n27    Y  57\n28    t  56\n29    L  50\n30    V  50\n31    g  34\n32    s  31\n33    o  27\n34    I  26\n35    P  26\n36    B  24\n37    U  24\n38    E  12\n39    A   2\n40    l   1\n\n\nPart b\nWhat’s the grade-point average (GPA) for each student?\n\ngrades |&gt;\n  left_join(gpa_conversion, by = \"grade\") |&gt;\n  group_by(sid) |&gt;\n  summarize(GPA = mean(gp, na.rm = TRUE), .groups = \"drop\")\n\n# A tibble: 443 × 2\n   sid      GPA\n   &lt;chr&gt;  &lt;dbl&gt;\n 1 S31185  2.41\n 2 S31188  3.02\n 3 S31191  3.21\n 4 S31194  3.36\n 5 S31197  3.35\n 6 S31200  2.2 \n 7 S31203  3.82\n 8 S31206  2.46\n 9 S31209  3.13\n10 S31212  3.67\n# ℹ 433 more rows\n\n\nPart c\nWhat’s the median GPA across all students?\n\ngrades |&gt;\n  left_join(gpa_conversion, by = \"grade\") |&gt;\n  group_by(sid) |&gt;\n  summarize(GPA = mean(gp, na.rm = TRUE), .groups = \"drop\") |&gt;\n  summarize(median_GPA = median(GPA, na.rm = TRUE))\n\n# A tibble: 1 × 1\n  median_GPA\n       &lt;dbl&gt;\n1       3.47\n\n\nPart d\nWhat fraction of grades are below B+?\n\ngrades |&gt;\n  left_join(gpa_conversion, by = \"grade\") |&gt;\n  filter(!is.na(gp)) |&gt;\n  mutate(lt_Bplus = gp &lt; 3.3) |&gt;\n  summarize(frac_below_Bplus = mean(lt_Bplus))\n\n  frac_below_Bplus\n1        0.2834776\n\n\nPart e\nWhat’s the grade-point average for each instructor? Order from low to high.\n\ngrades |&gt;\n  left_join(gpa_conversion, by = \"grade\") |&gt;\n  left_join(courses, by = \"sessionID\") |&gt;\n  group_by(iid) |&gt;\n  summarize(GPA = mean(gp, na.rm = TRUE), .groups = \"drop\") |&gt;\n  arrange(GPA)\n\n# A tibble: 364 × 2\n   iid       GPA\n   &lt;chr&gt;   &lt;dbl&gt;\n 1 inst265  1.3 \n 2 inst444  1.7 \n 3 inst513  1.85\n 4 inst200  2   \n 5 inst507  2.2 \n 6 inst445  2.3 \n 7 inst420  2.6 \n 8 inst262  2.65\n 9 inst176  2.66\n10 inst234  2.7 \n# ℹ 354 more rows\n\n\nPart f\nCHALLENGE: Estimate the grade-point average for each department, and sort from low to high. NOTE: Don’t include cross-listed courses. Students in cross-listed courses could be enrolled under either department, and we do not know which department to assign the grade to. HINT: You’ll need to do multiple joins.",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Joining</span>"
    ]
  },
  {
    "objectID": "ica/ica-joining.html#solutions",
    "href": "ica/ica-joining.html#solutions",
    "title": "\n19  Joining\n",
    "section": "\n19.7 Solutions",
    "text": "19.7 Solutions\n\nClick for Solutions\nExample 1\n\nclass\na student that took ANTH 101\ndata on ART 101\nExample 2\n\nWhat did this do? Linked course info to all students in students_1\n\nWhich observations from students_1 (the left table) were retained? All of them.\nWhich observations from enrollments_1 (the right table) were retained? Only STAT and GEOL, those that matched the students.\nWhat, if anything, would change if we reversed the order of the data tables? Think about it, then try. We retain the courses, not students.\n\n\nenrollments_1 |&gt; \n  left_join(students_1)\n\n     class enrollment student\n1 STAT 101         18       A\n2  ART 101         17    &lt;NA&gt;\n3 GEOL 101         24       B\n\n\nExample 3\n\nWhich observations from students_1 (the left table) were retained? A and B, only those with enrollment info.\nWhich observations from enrollments_1 (the right table) were retained? STAT and GEOL, only those with studen info.\nWhat, if anything, would change if we reversed the order of the data tables? Think about it, then try. Same info, different column order.\n\n\nenrollments_1 |&gt; \n    inner_join(students_1)\n\n     class enrollment student\n1 STAT 101         18       A\n2 GEOL 101         24       B\n\n\nExample 4\n\nWhich observations from students_1 (the left table) were retained? All\nWhich observations from enrollments_1 (the right table) were retained? All\nWhat, if anything, would change if we reversed the order of the data tables? Think about it, then try. Same data, different order.\n\n\nenrollments_1 |&gt; \n    full_join(students_1)\n\n     class enrollment student\n1 STAT 101         18       A\n2  ART 101         17    &lt;NA&gt;\n3 GEOL 101         24       B\n4 ANTH 101         NA       C\n\n\nExample 5\n\nWhich observations from students_1 (the left table) were retained? Only those with enrollment info.\nWhich observations from enrollments_1 (the right table) were retained? None.\nWhat, if anything, would change if we reversed the order of the data tables? Think about it, then try. Same data, different order.\n\n\nenrollments_1 |&gt; \n  semi_join(students_1)\n\n     class enrollment\n1 STAT 101         18\n2 GEOL 101         24\n\n\nExample 6\n\nWhich observations from students_1 (the left table) were retained? Only C, the one without enrollment info.\nWhich observations from enrollments_1 (the right table) were retained? None.\nWhat, if anything, would change if we reversed the order of the data tables? Think about it, then try. Retain only ART 101, the course with no student info.\n\n\nenrollments_1 |&gt; \n  anti_join(students_1)\n\n    class enrollment\n1 ART 101         17\n\n\nExercise 2: More small practice\n\n# 1. We want contact info for people who HAVEN'T voted\ncontact |&gt; \n  anti_join(voters, join_by(name == id))\n\n  name  address age\n1    B    grand  89\n2    C snelling  43\n\n# 2. We want contact info for people who HAVE voted\ncontact |&gt; \n  semi_join(voters, join_by(name == id))\n\n  name  address age\n1    A   summit  24\n2    D fairview  38\n\n# 3. We want any data available on each person\ncontact |&gt; \n  full_join(voters, join_by(name == id))\n\n  name  address age times_voted\n1    A   summit  24           2\n2    B    grand  89          NA\n3    C snelling  43          NA\n4    D fairview  38           4\n5    E     &lt;NA&gt;  NA          17\n6    F     &lt;NA&gt;  NA           6\n7    G     &lt;NA&gt;  NA          20\n\nvoters |&gt; \n  full_join(contact, join_by(id == name))\n\n  id times_voted  address age\n1  A           2   summit  24\n2  D           4 fairview  38\n3  E          17     &lt;NA&gt;  NA\n4  F           6     &lt;NA&gt;  NA\n5  G          20     &lt;NA&gt;  NA\n6  B          NA    grand  89\n7  C          NA snelling  43\n\n# 4. We want to add contact info, when possible, to the voting roster\nvoters |&gt; \n  left_join(contact, join_by(id == name))\n\n  id times_voted  address age\n1  A           2   summit  24\n2  D           4 fairview  38\n3  E          17     &lt;NA&gt;  NA\n4  F           6     &lt;NA&gt;  NA\n5  G          20     &lt;NA&gt;  NA\n\n\nExercise 3: Bigger datasets\n\n# How many observations (rows) and variables (columns) are there in the grades data?\ndim(grades)\n\n[1] 5844    3\n\n# How many observations (rows) and variables (columns) are there in the courses data?\ndim(courses)\n\n[1] 1718    6\n\n\nExercise 4: Class size\nPart a\n\ncourses_combined &lt;- courses |&gt;\n  group_by(sessionID) |&gt;\n  summarize(enroll = sum(enroll))\n\n# Check that this has 1695 rows and 2 columns\ndim(courses_combined)\n\n[1] 1695    2\n\n\nPart b\n\ncourses_combined |&gt; \n  summarize(median(enroll))\n\nPart c\n\nstudent_class_size &lt;- grades |&gt; \n  left_join(courses_combined) |&gt; \n  group_by(sid) |&gt; \n  summarize(med_class = median(enroll))\n\nhead(student_class_size)\n\nPart d\n\nggplot(student_class_size, aes(x = med_class)) +\n  geom_histogram(color = \"white\")\n\nExercise 5: Narrowing in on classes\nPart a\n\ngrades |&gt; \n  filter(sessionID == \"session1986\")\n\nPart b\n\ngrades |&gt; \n  semi_join(dept_E)\n\nExercise 6: All the wrangling\nPart a\n\ncourses |&gt; \n  group_by(dept) |&gt; \n  summarize(total = sum(enroll)) |&gt; \n  arrange(desc(total))\n\nPart b\n\ngrades |&gt; \n  left_join(gpa_conversion) |&gt; \n  group_by(sid) |&gt; \n  summarize(mean(gp, na.rm = TRUE))\n\nPart c\n\ngrades |&gt; \n  left_join(gpa_conversion) |&gt; \n  group_by(sid) |&gt; \n  summarize(gpa = mean(gp, na.rm = TRUE)) |&gt; \n  summarize(median(gpa))\n\nPart d\n\n# There are lots of approaches here!\ngrades |&gt; \n  left_join(gpa_conversion) |&gt; \n  mutate(below_b_plus = (gp &lt; 3.3)) |&gt; \n  summarize(mean(below_b_plus, na.rm = TRUE))\n\nPart e\n\ngrades |&gt; \n  left_join(gpa_conversion) |&gt; \n  left_join(courses) |&gt; \n  group_by(iid) |&gt; \n  summarize(gpa = mean(gp, na.rm = TRUE)) |&gt; \n  arrange(gpa)\n\nPart f\n\ncross_listed &lt;- courses |&gt; \n  count(sessionID) |&gt; \n  filter(n &gt; 1)\n\ngrades |&gt; \n  anti_join(cross_listed) |&gt; \n  inner_join(courses) |&gt; \n  left_join(gpa_conversion) |&gt; \n  group_by(dept) |&gt; \n  summarize(gpa = mean(gp, na.rm = TRUE)) |&gt; \n  arrange(gpa)",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Joining</span>"
    ]
  },
  {
    "objectID": "ica/ica-joining.html#footnotes",
    "href": "ica/ica-joining.html#footnotes",
    "title": "\n19  Joining\n",
    "section": "",
    "text": "There is also a right_join() that adds variables in the reverse direction from the left table to the right table, but we do not really need it as we can always switch the roles of the two tables.︎↩︎",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Joining</span>"
    ]
  },
  {
    "objectID": "ica/ica-reshaping.html",
    "href": "ica/ica-reshaping.html",
    "title": "\n20  ica-reshaping\n",
    "section": "",
    "text": "20.1 Review",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>ica-reshaping</span>"
    ]
  },
  {
    "objectID": "ica/ica-reshaping.html#review",
    "href": "ica/ica-reshaping.html#review",
    "title": "\n20  ica-reshaping\n",
    "section": "",
    "text": "Example 1: warm-up counts and proportions\nRecall the penguins we worked with last class:\n\nCodelibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nCodepenguins &lt;- read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-07-28/penguins.csv')\n\nRows: 344 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): species, island, sex\ndbl (5): bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g, year\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nTally up the number of male/female penguins by species in 2 ways:\n\nCode# Using count()\npenguins |&gt; count(species, sex)\n\n# A tibble: 8 × 3\n  species   sex        n\n  &lt;chr&gt;     &lt;chr&gt;  &lt;int&gt;\n1 Adelie    female    73\n2 Adelie    male      73\n3 Adelie    &lt;NA&gt;       6\n4 Chinstrap female    34\n5 Chinstrap male      34\n6 Gentoo    female    58\n7 Gentoo    male      61\n8 Gentoo    &lt;NA&gt;       5\n\nCode# Using group_by() and summarize()\npenguins |&gt; \n  group_by(species, sex) |&gt; \n  summarize(n = n())\n\n`summarise()` has grouped output by 'species'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 8 × 3\n# Groups:   species [3]\n  species   sex        n\n  &lt;chr&gt;     &lt;chr&gt;  &lt;int&gt;\n1 Adelie    female    73\n2 Adelie    male      73\n3 Adelie    &lt;NA&gt;       6\n4 Chinstrap female    34\n5 Chinstrap male      34\n6 Gentoo    female    58\n7 Gentoo    male      61\n8 Gentoo    &lt;NA&gt;       5\n\n\nDefine a new column that includes the proportion or relative frequencies of male/female penguins in each species.\n\nWe can’t do this by adjusting our count() code, but can adjust the group_by() and summarize() code since it’s still tracking the group categories in the background.\nDoes the order of species and sex in group_by() matter?\n\n\nCodepenguins |&gt; \n  group_by(species, sex) |&gt; \n  summarize(n = n()) |&gt; \n  mutate(prop = n / sum(n))\n\n`summarise()` has grouped output by 'species'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 8 × 4\n# Groups:   species [3]\n  species   sex        n   prop\n  &lt;chr&gt;     &lt;chr&gt;  &lt;int&gt;  &lt;dbl&gt;\n1 Adelie    female    73 0.480 \n2 Adelie    male      73 0.480 \n3 Adelie    &lt;NA&gt;       6 0.0395\n4 Chinstrap female    34 0.5   \n5 Chinstrap male      34 0.5   \n6 Gentoo    female    58 0.468 \n7 Gentoo    male      61 0.492 \n8 Gentoo    &lt;NA&gt;       5 0.0403\n\n\nExample 2: New data\nWhat will the following code do? Think about it before running.\n\nCodepenguin_avg &lt;- penguins |&gt; \n  group_by(species, sex) |&gt; \n  summarize(avg_body_mass = mean(body_mass_g, na.rm = TRUE)) |&gt; \n  na.omit()\n\n`summarise()` has grouped output by 'species'. You can override using the\n`.groups` argument.\n\n\nExample 3: units of observation\nTo get the information on average body masses, we reshaped our original data.\n\nDid the reshaping process change the units of observation?\n\n\nCode# Units of observation = ???\nhead(penguins)\n\n# A tibble: 6 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;       &lt;dbl&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           NA            NA                  NA          NA\n5 Adelie  Torgersen           36.7          19.3               193        3450\n6 Adelie  Torgersen           39.3          20.6               190        3650\n# ℹ 2 more variables: sex &lt;chr&gt;, year &lt;dbl&gt;\n\nCode# Units of observation = ???\nhead(penguin_avg)\n\n# A tibble: 6 × 3\n# Groups:   species [3]\n  species   sex    avg_body_mass\n  &lt;chr&gt;     &lt;chr&gt;          &lt;dbl&gt;\n1 Adelie    female         3369.\n2 Adelie    male           4043.\n3 Chinstrap female         3527.\n4 Chinstrap male           3939.\n5 Gentoo    female         4680.\n6 Gentoo    male           5485.\n\n\n\nDid the reshaping process result in any information loss from the original data?",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>ica-reshaping</span>"
    ]
  },
  {
    "objectID": "ica/ica-reshaping.html#reshaping-data",
    "href": "ica/ica-reshaping.html#reshaping-data",
    "title": "\n20  ica-reshaping\n",
    "section": "\n20.2 Reshaping Data",
    "text": "20.2 Reshaping Data\nThere are two general types of reshaped data:\n\naggregate data\nFor example, using group_by() with summarize() gains aggregate information about our observations but loses data on individual observations.\nraw data, reshaped\nWe often want to retain all information on individual observations, but need to reshape it in order to perform the task at hand.\n\nExample 4: reshape it with your mind\nLet’s calculate the difference in average body mass, male vs female, for each species. Since penguin_avg is small, we could do these calculations by hand. But this doesn’t scale up to bigger datasets.\n\nSketch out (on paper, in your head, anything) how this data would need to be reshaped, without losing any information, in order to calculate the differences in average body mass using our wrangling verbs. Make it as specific as possible, with column labels, entries, correct numbers, etc.\nIdentify the units of observation.\n\n\nCodepenguin_avg\n\n# A tibble: 6 × 3\n# Groups:   species [3]\n  species   sex    avg_body_mass\n  &lt;chr&gt;     &lt;chr&gt;          &lt;dbl&gt;\n1 Adelie    female         3369.\n2 Adelie    male           4043.\n3 Chinstrap female         3527.\n4 Chinstrap male           3939.\n5 Gentoo    female         4680.\n6 Gentoo    male           5485.\n\n\nWider vs Longer formats\nMaking our data longer or wider reshapes the data, changing the units of observation while retaining all raw information:\n\nMake the data longer, i.e. combine values from multiple variables into 1 variable. Example: 1999 and 2000 represent two years. We want to combine their results into 1 variable without losing any information.\n\n\n\nMake the data wider, i.e. spread out the values across new variables. Example: cases and pop represent two categories within type. To compare or combine their count outcomes side-by-side, we can separate them into their own variables.\n\n\nExample 5: pivot wider\nBecause it’s a small enough dataset to examine all at once, let’s start with our penguin_avg data:\n\nCodepenguin_avg\n\n# A tibble: 6 × 3\n# Groups:   species [3]\n  species   sex    avg_body_mass\n  &lt;chr&gt;     &lt;chr&gt;          &lt;dbl&gt;\n1 Adelie    female         3369.\n2 Adelie    male           4043.\n3 Chinstrap female         3527.\n4 Chinstrap male           3939.\n5 Gentoo    female         4680.\n6 Gentoo    male           5485.\n\n\nWith the goal of being able to calculate the difference in average body mass, male vs female, for each species, let’s make the dataset wider. That is, let’s get one row per species with separate columns for the average body mass by sex. Put this code into a chunk and run it:\n\nCodepenguin_avg |&gt; \npivot_wider(names_from = sex, values_from = avg_body_mass)\n\n# A tibble: 3 × 3\n# Groups:   species [3]\n  species   female  male\n  &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt;\n1 Adelie     3369. 4043.\n2 Chinstrap  3527. 3939.\n3 Gentoo     4680. 5485.\n\n\n\n\n\n\n\n\nPivot Wider\n\n\n\n\n\nnames_from = the variable whose values we want to separate into their own columns, i.e. where we want to get the new column names from\n\n\nvalues_from = which variable to take the new column values from\n\n\n\n\nFOLLOW-UP:\n\nWhat are the units of observation?\nDid we lose any information when we widened the data?\nUse the wide data to calculate the difference in average body mass, male vs female, for each species.\nExample 6: Pivot longer\nLet’s store our wide data:\n\nCodepenguin_avg_wide &lt;- penguin_avg |&gt; \n  pivot_wider(names_from = sex, values_from = avg_body_mass)\n\npenguin_avg_wide\n\n# A tibble: 3 × 3\n# Groups:   species [3]\n  species   female  male\n  &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt;\n1 Adelie     3369. 4043.\n2 Chinstrap  3527. 3939.\n3 Gentoo     4680. 5485.\n\n\nSuppose we wanted to change this data back to a longer format. In general, this happens when some variables (here female and male) represent two categories or values of some broader variable (here sex), and we want to combine them into that 1 variable without losing any information. Let’s pivot_longer():\n\nCode# We can either communicate which variables we WANT to collect into a single column (female, male)\npenguin_avg_wide |&gt; \n  pivot_longer(cols = c(female, male), names_to = \"sex\", values_to = \"avg_body_mass\")\n\n# A tibble: 6 × 3\n# Groups:   species [3]\n  species   sex    avg_body_mass\n  &lt;chr&gt;     &lt;chr&gt;          &lt;dbl&gt;\n1 Adelie    female         3369.\n2 Adelie    male           4043.\n3 Chinstrap female         3527.\n4 Chinstrap male           3939.\n5 Gentoo    female         4680.\n6 Gentoo    male           5485.\n\nCode# Or which variable(s) we do NOT want to collect into a single column (sex)\npenguin_avg_wide |&gt; \n  pivot_longer(cols = -species, names_to = \"sex\", values_to = \"avg_body_mass\")\n\n# A tibble: 6 × 3\n# Groups:   species [3]\n  species   sex    avg_body_mass\n  &lt;chr&gt;     &lt;chr&gt;          &lt;dbl&gt;\n1 Adelie    female         3369.\n2 Adelie    male           4043.\n3 Chinstrap female         3527.\n4 Chinstrap male           3939.\n5 Gentoo    female         4680.\n6 Gentoo    male           5485.\n\n\n\n\n\n\n\n\nPivot Longer\n\n\n\n\n\ncols = the columns (variables) to collect into a single, new variable. We can also specify what variables we don’t want to collect\n\nnames_to = the name of the new variable which will include the names or labels of the collected variables\n\nvalues_to = the name of the new variable which will include the values of the collected variables\n\n\n\nFOLLOW-UP:\n\nWhat are the units of observation?\nDid we lose any information when we lengthened the data?\nWhy did we put the variables in quotes “” here but not when we used pivot_wider()?\nExample 7: Practice\nLet’s make up some data on the orders of 2 different customers at 3 different restaurants:\n\nCodefood &lt;- data.frame(\n  customer = rep(c(\"A\", \"B\"), each = 3),\n  restaurant = rep(c(\"Shish\", \"FrenchMeadow\", \"DunnBros\"), 2),\n  order = c(\"falafel\", \"salad\", \"coffee\", \"baklava\", \"pastry\", \"tea\")\n)\nfood\n\n  customer   restaurant   order\n1        A        Shish falafel\n2        A FrenchMeadow   salad\n3        A     DunnBros  coffee\n4        B        Shish baklava\n5        B FrenchMeadow  pastry\n6        B     DunnBros     tea\n\n\nThe units of observation in food are customer / restaurant combinations. Wrangle this data so that the units of observation are customers, spreading the restaurants into separate columns.\nConsider 2 more customers:\n\nCodemore_food &lt;- data.frame(\n  customer = c(\"C\", \"D\"),\n  Shish = c(\"coffee\", \"maza\"),\n  FrenchMeadow = c(\"soup\", \"sandwich\"),\n  DunnBros = c(\"cookie\", \"coffee\")\n)\nmore_food\n\n  customer  Shish FrenchMeadow DunnBros\n1        C coffee         soup   cookie\n2        D   maza     sandwich   coffee\n\n\nWrangle this data so that the 3 restaurant columns are combined into 1, hence the units of observation are customer / restaurant combinations.",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>ica-reshaping</span>"
    ]
  },
  {
    "objectID": "ica/ica-reshaping.html#exercises",
    "href": "ica/ica-reshaping.html#exercises",
    "title": "\n20  ica-reshaping\n",
    "section": "\n20.3 Exercises",
    "text": "20.3 Exercises\nExercise 1: What’s the problem?\nConsider data on a sleep study in which subjects received only 3 hours of sleep per night. Each day, their reaction time to a stimulus (in ms) was recorded.1\n\nCodesleep_wide &lt;- read.csv(\"https://mac-stat.github.io/data/sleep_wide.csv\")\n\nhead(sleep_wide)\n\n  Subject  day_0  day_1  day_2  day_3  day_4  day_5  day_6  day_7  day_8  day_9\n1     308 249.56 258.70 250.80 321.44 356.85 414.69 382.20 290.15 430.59 466.35\n2     309 222.73 205.27 202.98 204.71 207.72 215.96 213.63 217.73 224.30 237.31\n3     310 199.05 194.33 234.32 232.84 229.31 220.46 235.42 255.75 261.01 247.52\n4     330 321.54 300.40 283.86 285.13 285.80 297.59 280.24 318.26 305.35 354.05\n5     331 287.61 285.00 301.82 320.12 316.28 293.32 290.08 334.82 293.75 371.58\n6     332 234.86 242.81 272.96 309.77 317.46 310.00 454.16 346.83 330.30 253.86\n\n\nPart a\nWhat are the units of observation in sleep_wide?\nOne subject’s sleeping record.\nPart b\nSuppose I ask you to plot each subject’s reaction time (y-axis) vs the number of days of sleep restriction (x-axis). “Sketch” out in words what the first few rows of the data need to look like in order to do this. It might help to think about what you’d need to complete the plotting frame:\nggplot(___, aes(y = ___, x = ___, color = ___))\nPart c\nHow can you obtain the dataset you sketched in part b?\n\njust using sleep_wide\n\npivot_longer()\npivot_wider()\nExercise 2: Pivot longer\nTo plot reaction time by day for each subject, we need to reshape the data into a long format where each row represents a subject/day combination. Specifically, we want a dataset with 3 columns and a first few rows that look something like this:\n\n\nSubject\nday\nreaction_time\n\n\n\n308\n0\n249.56\n\n\n308\n1\n258.70\n\n\n308\n2\n250.80\n\n\n\nPart a\nUse pivot_longer() to create the long-format dataset above. Show the first 3 lines (head(3)), which should be similar to those above. Follow-up: Thinking forward to plotting reaction time vs day for each subject, what would you like to fix / change about this dataset?\n\nCode# For cols, try 2 appproaches: using - and starts_with\nsleep_wide |&gt; \n  pivot_longer(cols = -Subject, \n               names_to = \"day\", \n               values_to = \"reaction_time\") |&gt; \n  head(3)\n\n# A tibble: 3 × 3\n  Subject day   reaction_time\n    &lt;int&gt; &lt;chr&gt;         &lt;dbl&gt;\n1     308 day_0          250.\n2     308 day_1          259.\n3     308 day_2          251.\n\n\nPart b\nRun this chunk:\n\nCodesleep_long &lt;- sleep_wide |&gt;\n  pivot_longer(cols = -Subject,\n               names_to = \"day\",\n               names_prefix = \"day_\",\n               values_to = \"reaction_time\")\n\nhead(sleep_long)\n\n# A tibble: 6 × 3\n  Subject day   reaction_time\n    &lt;int&gt; &lt;chr&gt;         &lt;dbl&gt;\n1     308 0              250.\n2     308 1              259.\n3     308 2              251.\n4     308 3              321.\n5     308 4              357.\n6     308 5              415.\n\n\nFollow-up:\n\nBesides putting each argument on a different line for readability and storing the results, what changed in the code?\nHow did this impact how the values are recorded in the day column?\nPart c\nUsing sleep_long, construct a line plot of reaction time vs day for each subject. This will look goofy no matter what you do. Why? HINT: look back at head(sleep_long). What class or type of variables are Subject and day? What do we want them to be?\n\nCodeggplot(sleep_long, aes(x = day, y = reaction_time, color = Subject)) +\n  geom_line()\n\n\n\n\n\n\n\nExercise 3: Changing variable classes & plotting\nLet’s finalize sleep_long by mutating the Subject variable to be a factor (categorical) and the day variable to be numeric (quantitative). Take note of the mutate() code! You’ll use this type of code a lot.\n\nCodesleep_long &lt;- sleep_wide |&gt;\n  pivot_longer(cols = -Subject,\n               names_to = \"day\",\n               names_prefix = \"day_\",\n               values_to = \"reaction_time\") |&gt; \n  mutate(Subject = as.factor(Subject), day = as.numeric(day))\n\n# Check it out\n# Same data, different class\nhead(sleep_long)\n\n# A tibble: 6 × 3\n  Subject   day reaction_time\n  &lt;fct&gt;   &lt;dbl&gt;         &lt;dbl&gt;\n1 308         0          250.\n2 308         1          259.\n3 308         2          251.\n4 308         3          321.\n5 308         4          357.\n6 308         5          415.\n\n\nPart a\nNow make some plots.\n\nCode# Make a line plot of reaction time by day for each subject\n# Put these all on the same frame\nggplot(sleep_long, aes(x = day, y = reaction_time, color = Subject)) +\n  geom_line()\n\n\n\n\n\n\n\n\nCode# Make a line plot of reaction time by day for each subject\n# Put these all on separate frames (one per subject)\n\nggplot(sleep_long, aes(x = day, y = reaction_time)) +\n  geom_line() +\n  facet_wrap(~Subject)\n\n\n\n\n\n\n\nPart b\nSummarize what you learned from the plots. For example:\n\nWhat’s the general relationship between reaction time and sleep?\nIs this the same for everybody? What differs?\nExercise 4: Pivot wider\nMake the data wide again, with each day becoming its own column.\nPart a\nAdjust the code below. What don’t you like about the column labels?\n\nCodesleep_long |&gt; \n  pivot_wider(names_from = day, values_from = reaction_time) |&gt; \n  head()\n\n# A tibble: 6 × 11\n  Subject   `0`   `1`   `2`   `3`   `4`   `5`   `6`   `7`   `8`   `9`\n  &lt;fct&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 308      250.  259.  251.  321.  357.  415.  382.  290.  431.  466.\n2 309      223.  205.  203.  205.  208.  216.  214.  218.  224.  237.\n3 310      199.  194.  234.  233.  229.  220.  235.  256.  261.  248.\n4 330      322.  300.  284.  285.  286.  298.  280.  318.  305.  354.\n5 331      288.  285   302.  320.  316.  293.  290.  335.  294.  372.\n6 332      235.  243.  273.  310.  317.  310   454.  347.  330.  254.\n\n\nPart b\nUsing your intuition, adjust your code from part a to name the reaction time columns “day_0”, “day_1”, etc.\n\nCodesleep_long |&gt; \n  pivot_wider(names_from = day, values_from = reaction_time, names_prefix = \"day_\") |&gt; \n  head()\n\n# A tibble: 6 × 11\n  Subject day_0 day_1 day_2 day_3 day_4 day_5 day_6 day_7 day_8 day_9\n  &lt;fct&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 308      250.  259.  251.  321.  357.  415.  382.  290.  431.  466.\n2 309      223.  205.  203.  205.  208.  216.  214.  218.  224.  237.\n3 310      199.  194.  234.  233.  229.  220.  235.  256.  261.  248.\n4 330      322.  300.  284.  285.  286.  298.  280.  318.  305.  354.\n5 331      288.  285   302.  320.  316.  293.  290.  335.  294.  372.\n6 332      235.  243.  273.  310.  317.  310   454.  347.  330.  254.\n\n\nExercise 5: Practice with Billboard charts\nLoad data on songs that hit the billboard charts around the year 2000. Included for each song is the artist name, track name, the date it hit the charts (date.enter), and wk-related variables that indicate rankings in each subsequent week on the charts:\n\nCode# Load data\nlibrary(tidyr)\ndata(\"billboard\")\n\n# Check it out\nhead(billboard)\n\n# A tibble: 6 × 79\n  artist      track date.entered   wk1   wk2   wk3   wk4   wk5   wk6   wk7   wk8\n  &lt;chr&gt;       &lt;chr&gt; &lt;date&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 2 Pac       Baby… 2000-02-26      87    82    72    77    87    94    99    NA\n2 2Ge+her     The … 2000-09-02      91    87    92    NA    NA    NA    NA    NA\n3 3 Doors Do… Kryp… 2000-04-08      81    70    68    67    66    57    54    53\n4 3 Doors Do… Loser 2000-10-21      76    76    72    69    67    65    55    59\n5 504 Boyz    Wobb… 2000-04-15      57    34    25    17    17    31    36    49\n6 98^0        Give… 2000-08-19      51    39    34    26    26    19     2     2\n# ℹ 68 more variables: wk9 &lt;dbl&gt;, wk10 &lt;dbl&gt;, wk11 &lt;dbl&gt;, wk12 &lt;dbl&gt;,\n#   wk13 &lt;dbl&gt;, wk14 &lt;dbl&gt;, wk15 &lt;dbl&gt;, wk16 &lt;dbl&gt;, wk17 &lt;dbl&gt;, wk18 &lt;dbl&gt;,\n#   wk19 &lt;dbl&gt;, wk20 &lt;dbl&gt;, wk21 &lt;dbl&gt;, wk22 &lt;dbl&gt;, wk23 &lt;dbl&gt;, wk24 &lt;dbl&gt;,\n#   wk25 &lt;dbl&gt;, wk26 &lt;dbl&gt;, wk27 &lt;dbl&gt;, wk28 &lt;dbl&gt;, wk29 &lt;dbl&gt;, wk30 &lt;dbl&gt;,\n#   wk31 &lt;dbl&gt;, wk32 &lt;dbl&gt;, wk33 &lt;dbl&gt;, wk34 &lt;dbl&gt;, wk35 &lt;dbl&gt;, wk36 &lt;dbl&gt;,\n#   wk37 &lt;dbl&gt;, wk38 &lt;dbl&gt;, wk39 &lt;dbl&gt;, wk40 &lt;dbl&gt;, wk41 &lt;dbl&gt;, wk42 &lt;dbl&gt;,\n#   wk43 &lt;dbl&gt;, wk44 &lt;dbl&gt;, wk45 &lt;dbl&gt;, wk46 &lt;dbl&gt;, wk47 &lt;dbl&gt;, wk48 &lt;dbl&gt;, …\n\n\nIn using this data, you’ll need to determine if and when the data needs to be reshaped for the task at hand.\nPart a\nConstruct and summarize a plot of how a song’s Billboard ranking its 2nd week on the chart (y-axis) is related to its ranking the 1st week on the charts (x-axis). Add a reference line geom_abline(intercept = 0, slope = 1). Songs above this line improved their rankings from the 1st to 2nd week.\n\nCodebillboard |&gt; \n  ggplot(aes(x = wk1, y = wk2)) +\n  geom_point() +\n  geom_abline(intercept = 0, slope = 1)\n\nWarning: Removed 5 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\nPart b\nUse your wrangling tools to identify which songs are those above the line in Part a, i.e. with rankgings that went up from week 1 to week 2.\n\nCodebillboard |&gt; \n  filter(wk2 &lt; wk1) |&gt; \n  select(artist, track, wk1, wk2)\n\n# A tibble: 255 × 4\n   artist              track                     wk1   wk2\n   &lt;chr&gt;               &lt;chr&gt;                   &lt;dbl&gt; &lt;dbl&gt;\n 1 2 Pac               Baby Don't Cry (Keep...    87    82\n 2 2Ge+her             The Hardest Part Of ...    91    87\n 3 3 Doors Down        Kryptonite                 81    70\n 4 504 Boyz            Wobble Wobble              57    34\n 5 98^0                Give Me Just One Nig...    51    39\n 6 Aaliyah             I Don't Wanna              84    62\n 7 Aaliyah             Try Again                  59    53\n 8 Aguilera, Christina Come On Over Baby (A...    57    47\n 9 Aguilera, Christina I Turn To You              50    39\n10 Aguilera, Christina What A Girl Wants          71    51\n# ℹ 245 more rows\n\n\nPart c\nDefine a new dataset, nov_1999, which:\n\nonly includes data on songs that entered the Billboard charts on November 6, 1999\nkeeps all variables except track and date.entered. HINT: How can you avoid writing out all the variable names you want to keep?\n\n\nCode# Define nov_1999\nnov_1999 &lt;- billboard |&gt; \n  filter(date.entered == \"1999-11-06\") |&gt; \n  select(-track, -date.entered)\n\n# Confirm that nov_1999 has 2 rows (songs) and 77 columns\ndim(nov_1999)\n\n[1]  2 77\n\n\nPart d\nCreate and discuss a visualization of the rankings (y-axis) over time (x-axis) for the 2 songs in nov_1999. There are hints below (if you scroll), but you’re encouraged to play around and use as few hints as possible.\n\nCodenov_1999 |&gt; \n  pivot_longer(cols = starts_with(\"wk\"), \n               names_to = \"week\", \n               values_to = \"rank\") |&gt; \n  mutate(week = parse_number(week)) |&gt; \n  ggplot(aes(x = week, y = rank, color = artist)) +\n  geom_line()\n\nWarning: Removed 79 rows containing missing values or values outside the scale range\n(`geom_line()`).\n\n\n\n\n\n\n\n\nHints:\n\nShould you first pivot wider or longer?\nOnce you pivot, the week number is turned into a character variable. How can you change it to a number?\nExercise 6: Practice with the Daily Show\nThe data associated with this article is available in the fivethirtyeight package, and is loaded into daily below. It includes a list of every guest to ever appear on Jon Stewart’s The Daily Show, a “late-night talk and satirical news” program (per Wikipedia). Check out the dataset and note that when multiple people appeared together, each person receives their own line:\n\nCodelibrary(fivethirtyeight)\n\nSome larger datasets need to be installed separately, like senators and\nhouse_district_forecast. To install these, we recommend you install the\nfivethirtyeightdata package by running:\ninstall.packages('fivethirtyeightdata', repos =\n'https://fivethirtyeightdata.github.io/drat/', type = 'source')\n\nCodedata(\"daily_show_guests\")\ndaily &lt;- daily_show_guests\n\n\nIn analyzing this data, you’ll need to determine if and when the data needs to be reshaped.\nPart a\nIdentify the 15 guests that appeared the most. (This isn’t a very diverse guest list!)\n\nCodedaily |&gt; \n  count(raw_guest_list, sort = TRUE) |&gt; \n  head(15)\n\n# A tibble: 15 × 2\n   raw_guest_list        n\n   &lt;chr&gt;             &lt;int&gt;\n 1 Fareed Zakaria       19\n 2 Denis Leary          17\n 3 Brian Williams       16\n 4 Paul Rudd            13\n 5 Ricky Gervais        13\n 6 Tom Brokaw           12\n 7 Bill O'Reilly        10\n 8 Reza Aslan           10\n 9 Richard Lewis        10\n10 Will Ferrell         10\n11 Sarah Vowell          9\n12 Adam Sandler          8\n13 Ben Affleck           8\n14 Louis C.K.            8\n15 Maggie Gyllenhaal     8\n\n\nPart b\nCHALLENGE: Create the following data set containing 19 columns:\n\nThe first column should have the 15 guests with the highest number of total appearances on the show, listed in descending order of number of appearances.\n17 columns should show the number of appearances of the corresponding guest in each year from 1999 to 2015 (one per column).\nAnother column should show the total number of appearances for the corresponding guest over the entire duration of the show.\n\nThere are hints below (if you scroll), but you’re encouraged to play around and use as few hints as possible.\n\nCodedaily |&gt; \n  count(raw_guest_list, year) |&gt; \n  pivot_wider(names_from = year, values_from = n, values_fill = 0) |&gt; \n  mutate(total = rowSums(across(where(is.numeric)))) |&gt; \n  arrange(desc(total)) |&gt; \n  head(15)\n\n# A tibble: 15 × 19\n   raw_guest_list `2004` `2008` `2001` `2011` `2013` `2015` `2009` `2012` `1999`\n   &lt;chr&gt;           &lt;int&gt;  &lt;int&gt;  &lt;int&gt;  &lt;int&gt;  &lt;int&gt;  &lt;int&gt;  &lt;int&gt;  &lt;int&gt;  &lt;int&gt;\n 1 Fareed Zakaria      2      2      1      1      1      1      2      1      0\n 2 Denis Leary         0      2      1      2      0      1      1      2      1\n 3 Brian Williams      1      3      0      1      1      0      2      1      0\n 4 Paul Rudd           1      1      1      0      1      1      1      1      1\n 5 Ricky Gervais       0      1      0      1      1      0      2      2      0\n 6 Tom Brokaw          2      2      0      1      2      1      0      1      0\n 7 Bill O'Reilly       1      1      1      1      1      0      0      1      0\n 8 Reza Aslan          0      0      0      0      2      1      2      0      0\n 9 Richard Lewis       1      1      2      1      0      1      0      0      1\n10 Will Ferrell        1      0      1      1      0      1      1      1      0\n11 Sarah Vowell        1      1      0      1      1      1      1      0      0\n12 Adam Sandler        0      1      0      1      0      0      0      0      1\n13 Ben Affleck         0      0      0      0      0      0      1      1      0\n14 Louis C.K.          0      0      0      1      1      1      0      1      0\n15 Maggie Gyllen…      0      1      0      0      1      0      0      1      0\n# ℹ 9 more variables: `2003` &lt;int&gt;, `2014` &lt;int&gt;, `2000` &lt;int&gt;, `2002` &lt;int&gt;,\n#   `2006` &lt;int&gt;, `2007` &lt;int&gt;, `2010` &lt;int&gt;, `2005` &lt;int&gt;, total &lt;dbl&gt;\n\n\nHINTS: There are lots of ways to do this. You don’t necessarily need all of these hints.\n\nFirst obtain the number of times a guest appears each year.\nAdd a new column which includes the total number of times a guest appears across all years.\nPivot (longer or wider?). When you do, use values_fill = 0 to replace NA values with 0.\nArrange, then and keep the top 15.\nPart c\nLet’s recreate the first figure from the article. This groups all guests into 3 broader occupational categories. However, our current data has 18 categories:\n\nCodedaily |&gt; \n  count(group)\n\n# A tibble: 18 × 2\n   group              n\n   &lt;chr&gt;          &lt;int&gt;\n 1 Academic         103\n 2 Acting           930\n 3 Advocacy          24\n 4 Athletics         52\n 5 Business          25\n 6 Clergy             8\n 7 Comedy           150\n 8 Consultant        18\n 9 Government        40\n10 Media            751\n11 Military          16\n12 Misc              45\n13 Musician         123\n14 Political Aide    36\n15 Politician       308\n16 Science           28\n17 media              5\n18 &lt;NA&gt;              31\n\n\nLet’s define a new dataset that includes a new variable, broad_group, that buckets these 18 categories into the 3 bigger ones used in the article. And get rid of any rows missing information on broad_group. You’ll learn the code soon! For now, just run this chunk:\n\nCodeplot_data &lt;- daily |&gt; \n  mutate(broad_group = case_when(\n    group %in% c(\"Acting\", \"Athletics\", \"Comedy\", \"Musician\") ~ \"Acting, Comedy & Music\",\n    group %in% c(\"Media\", \"media\", \"Science\", \"Academic\", \"Consultant\", \"Clergy\") ~ \"Media\",\n    group %in% c(\"Politician\", \"Political Aide\", \"Government\", \"Military\", \"Business\", \"Advocacy\") ~ \"Government and Politics\",\n    .default = NA\n  )) |&gt; \n  filter(!is.na(broad_group))\n\n\nNow, using the broad_group variable in plot_data, recreate the graphic from the article, with three different lines showing the fraction of guests in each group over time. Note: You’ll have to wrangle the data first.\n\nCodeplot_data |&gt; \n  count(year, broad_group) |&gt; \n  group_by(year) |&gt; \n  mutate(prop = n / sum(n)) |&gt; \n  ggplot(aes(x = year, y = prop, color = broad_group)) +\n  geom_line()",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>ica-reshaping</span>"
    ]
  },
  {
    "objectID": "ica/ica-reshaping.html#solutions",
    "href": "ica/ica-reshaping.html#solutions",
    "title": "\n20  ica-reshaping\n",
    "section": "\n20.4 Solutions",
    "text": "20.4 Solutions\n\nClick for Solutions\nExample 1: warm-up counts and proportions\n\nCode# Using count()\npenguins |&gt; \n  count(species, sex)\n\n# A tibble: 8 × 3\n  species   sex        n\n  &lt;chr&gt;     &lt;chr&gt;  &lt;int&gt;\n1 Adelie    female    73\n2 Adelie    male      73\n3 Adelie    &lt;NA&gt;       6\n4 Chinstrap female    34\n5 Chinstrap male      34\n6 Gentoo    female    58\n7 Gentoo    male      61\n8 Gentoo    &lt;NA&gt;       5\n\nCode# Using group_by() and summarize()\npenguins |&gt; \n  group_by(species, sex) |&gt; \n  summarize(n())\n\n`summarise()` has grouped output by 'species'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 8 × 3\n# Groups:   species [3]\n  species   sex    `n()`\n  &lt;chr&gt;     &lt;chr&gt;  &lt;int&gt;\n1 Adelie    female    73\n2 Adelie    male      73\n3 Adelie    &lt;NA&gt;       6\n4 Chinstrap female    34\n5 Chinstrap male      34\n6 Gentoo    female    58\n7 Gentoo    male      61\n8 Gentoo    &lt;NA&gt;       5\n\nCode# Relative frequencies\npenguins |&gt; \n  group_by(species, sex) |&gt; \n  summarize(n = n()) |&gt; \n  mutate(proportion = n / sum(n))\n\n`summarise()` has grouped output by 'species'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 8 × 4\n# Groups:   species [3]\n  species   sex        n proportion\n  &lt;chr&gt;     &lt;chr&gt;  &lt;int&gt;      &lt;dbl&gt;\n1 Adelie    female    73     0.480 \n2 Adelie    male      73     0.480 \n3 Adelie    &lt;NA&gt;       6     0.0395\n4 Chinstrap female    34     0.5   \n5 Chinstrap male      34     0.5   \n6 Gentoo    female    58     0.468 \n7 Gentoo    male      61     0.492 \n8 Gentoo    &lt;NA&gt;       5     0.0403\n\nCode# Changing the order calculates the proportion of species within each sex\npenguins |&gt; \n  group_by(sex, species) |&gt; \n  summarize(n = n()) |&gt; \n  mutate(proportion = n / sum(n))\n\n`summarise()` has grouped output by 'sex'. You can override using the `.groups`\nargument.\n\n\n# A tibble: 8 × 4\n# Groups:   sex [3]\n  sex    species       n proportion\n  &lt;chr&gt;  &lt;chr&gt;     &lt;int&gt;      &lt;dbl&gt;\n1 female Adelie       73      0.442\n2 female Chinstrap    34      0.206\n3 female Gentoo       58      0.352\n4 male   Adelie       73      0.435\n5 male   Chinstrap    34      0.202\n6 male   Gentoo       61      0.363\n7 &lt;NA&gt;   Adelie        6      0.545\n8 &lt;NA&gt;   Gentoo        5      0.455\n\n\nExample 3: units of observation\n\nCode# Units of observation = penguins\nhead(penguins)\n\n# A tibble: 6 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;       &lt;dbl&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           NA            NA                  NA          NA\n5 Adelie  Torgersen           36.7          19.3               193        3450\n6 Adelie  Torgersen           39.3          20.6               190        3650\n# ℹ 2 more variables: sex &lt;chr&gt;, year &lt;dbl&gt;\n\nCode# Units of observation = species/sex combos\nhead(penguin_avg)\n\n# A tibble: 6 × 3\n# Groups:   species [3]\n  species   sex    avg_body_mass\n  &lt;chr&gt;     &lt;chr&gt;          &lt;dbl&gt;\n1 Adelie    female         3369.\n2 Adelie    male           4043.\n3 Chinstrap female         3527.\n4 Chinstrap male           3939.\n5 Gentoo    female         4680.\n6 Gentoo    male           5485.\n\n\nExample 5: pivot wider\n\nCodepenguin_avg |&gt; \n  pivot_wider(names_from = sex, values_from = avg_body_mass)\n\n# A tibble: 3 × 3\n# Groups:   species [3]\n  species   female  male\n  &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt;\n1 Adelie     3369. 4043.\n2 Chinstrap  3527. 3939.\n3 Gentoo     4680. 5485.\n\n\nFOLLOW-UP:\n\nWhat are the units of observation? species\nDid we lose any information when we widened the data? no\nUse the wide data to calculate the difference in average body mass, male vs female, for each species.\n\n\nCodepenguin_avg |&gt; \n  pivot_wider(names_from = sex, values_from = avg_body_mass) |&gt; \n  mutate(diff = male - female)\n\n# A tibble: 3 × 4\n# Groups:   species [3]\n  species   female  male  diff\n  &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Adelie     3369. 4043.  675.\n2 Chinstrap  3527. 3939.  412.\n3 Gentoo     4680. 5485.  805.\n\n\nExample 6: Pivot longer\n\nCode# We can either communicate which variables we WANT to collect into a single column (female, male)\npenguin_avg_wide |&gt; \n  pivot_longer(cols = c(female, male), names_to = \"sex\", values_to = \"avg_body_mass\")\n\n# A tibble: 6 × 3\n# Groups:   species [3]\n  species   sex    avg_body_mass\n  &lt;chr&gt;     &lt;chr&gt;          &lt;dbl&gt;\n1 Adelie    female         3369.\n2 Adelie    male           4043.\n3 Chinstrap female         3527.\n4 Chinstrap male           3939.\n5 Gentoo    female         4680.\n6 Gentoo    male           5485.\n\nCode# Or which variable(s) we do NOT want to collect into a single column (sex)\npenguin_avg_wide |&gt; \n  pivot_longer(cols = -species, names_to = \"sex\", values_to = \"avg_body_mass\")\n\n# A tibble: 6 × 3\n# Groups:   species [3]\n  species   sex    avg_body_mass\n  &lt;chr&gt;     &lt;chr&gt;          &lt;dbl&gt;\n1 Adelie    female         3369.\n2 Adelie    male           4043.\n3 Chinstrap female         3527.\n4 Chinstrap male           3939.\n5 Gentoo    female         4680.\n6 Gentoo    male           5485.\n\n\nFOLLOW-UP:\n\nWhat are the units of observation? species/sex combos\nDid we lose any information when we lengthened the data? no\n\n20.4.1 Example 7: Practice [-]\n\nCodefood &lt;- data.frame(\n  customer = rep(c(\"A\", \"B\"), each = 3),\n  restaurant = rep(c(\"Shish\", \"FrenchMeadow\", \"DunnBros\"), 2),\n  order = c(\"falafel\", \"salad\", \"coffee\", \"baklava\", \"pastry\", \"tea\")\n)\n\nfood\n\n  customer   restaurant   order\n1        A        Shish falafel\n2        A FrenchMeadow   salad\n3        A     DunnBros  coffee\n4        B        Shish baklava\n5        B FrenchMeadow  pastry\n6        B     DunnBros     tea\n\nCodefood |&gt; \n  pivot_wider(names_from = restaurant, values_from = order)\n\n# A tibble: 2 × 4\n  customer Shish   FrenchMeadow DunnBros\n  &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt;        &lt;chr&gt;   \n1 A        falafel salad        coffee  \n2 B        baklava pastry       tea     \n\n\n\nCodemore_food &lt;- data.frame(\n  customer = c(\"C\", \"D\"),\n  Shish = c(\"coffee\", \"maza\"),\n  FrenchMeadow = c(\"soup\", \"sandwich\"),\n  DunnBros = c(\"cookie\", \"coffee\")\n)\n\nmore_food\n\n  customer  Shish FrenchMeadow DunnBros\n1        C coffee         soup   cookie\n2        D   maza     sandwich   coffee\n\nCodemore_food |&gt; \n  pivot_longer(cols = -customer, names_to = \"restaurant\", values_to = \"order\")\n\n# A tibble: 6 × 3\n  customer restaurant   order   \n  &lt;chr&gt;    &lt;chr&gt;        &lt;chr&gt;   \n1 C        Shish        coffee  \n2 C        FrenchMeadow soup    \n3 C        DunnBros     cookie  \n4 D        Shish        maza    \n5 D        FrenchMeadow sandwich\n6 D        DunnBros     coffee  \n\n\nExercise 1: What’s the problem?\nPart a\nsubjects/people\nPart c\npivot_longer()\nExercise 2: Pivot longer\nPart a\n\nCode# For cols, try 2 appproaches: using - and starts_with\nsleep_wide |&gt;\n  pivot_longer(cols = -Subject, names_to = \"day\", values_to = \"reaction_time\")\n\n# A tibble: 180 × 3\n   Subject day   reaction_time\n     &lt;int&gt; &lt;chr&gt;         &lt;dbl&gt;\n 1     308 day_0          250.\n 2     308 day_1          259.\n 3     308 day_2          251.\n 4     308 day_3          321.\n 5     308 day_4          357.\n 6     308 day_5          415.\n 7     308 day_6          382.\n 8     308 day_7          290.\n 9     308 day_8          431.\n10     308 day_9          466.\n# ℹ 170 more rows\n\nCodesleep_wide |&gt;\n  pivot_longer(cols = starts_with(\"day\"), names_to = \"day\", values_to = \"reaction_time\")\n\n# A tibble: 180 × 3\n   Subject day   reaction_time\n     &lt;int&gt; &lt;chr&gt;         &lt;dbl&gt;\n 1     308 day_0          250.\n 2     308 day_1          259.\n 3     308 day_2          251.\n 4     308 day_3          321.\n 5     308 day_4          357.\n 6     308 day_5          415.\n 7     308 day_6          382.\n 8     308 day_7          290.\n 9     308 day_8          431.\n10     308 day_9          466.\n# ℹ 170 more rows\n\n\nPart b\nAdding names_prefix = \"day_\" removed “day_” from the start of the day entries. did this impact how the values are recorded in the day column?\n\nCodesleep_long &lt;- sleep_wide |&gt;\n  pivot_longer(cols = -Subject,\n               names_to = \"day\",\n               names_prefix = \"day_\",\n               values_to = \"reaction_time\") \n\n\nPart c\nSubject is an integer and day is a character. We want them to be categorical (factor) and numeric, respectively.\n\nCodeggplot(sleep_long, aes(y = reaction_time, x = day, color = Subject)) + \n  geom_line()\n\n\n\n\n\n\n\nExercise 3: Changing variable classes & plotting\n\nCodesleep_long &lt;- sleep_wide |&gt;\n  pivot_longer(cols = -Subject,\n               names_to = \"day\",\n               names_prefix = \"day_\",\n               values_to = \"reaction_time\") |&gt; \n  mutate(Subject = as.factor(Subject), day = as.numeric(day))\n\n\nPart a\nNow make some plots.\n\nCode# Make a line plot of reaction time by day for each subject\n# Put these all on the same frame\nggplot(sleep_long, aes(y = reaction_time, x = day, color = Subject)) + \n  geom_line()\n\n\n\n\n\n\n\n\nCode# Make a line plot of reaction time by day for each subject\n# Put these all on separate frames (one per subject)\nggplot(sleep_long, aes(y = reaction_time, x = day, color = Subject)) + \n  geom_line() + \n  facet_wrap(~ Subject)\n\n\n\n\n\n\n\nPart b\nReaction time increases (worsens) with a lack of sleep. Some subjects seem to be more impacted than others by lack of sleep, and some tend to have faster/slower reaction times in general.\nExercise 4: Pivot wider\nPart a\n\nCodesleep_long |&gt;\n  pivot_wider(names_from = day, values_from = reaction_time) |&gt;\n  head()\n\n# A tibble: 6 × 11\n  Subject   `0`   `1`   `2`   `3`   `4`   `5`   `6`   `7`   `8`   `9`\n  &lt;fct&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 308      250.  259.  251.  321.  357.  415.  382.  290.  431.  466.\n2 309      223.  205.  203.  205.  208.  216.  214.  218.  224.  237.\n3 310      199.  194.  234.  233.  229.  220.  235.  256.  261.  248.\n4 330      322.  300.  284.  285.  286.  298.  280.  318.  305.  354.\n5 331      288.  285   302.  320.  316.  293.  290.  335.  294.  372.\n6 332      235.  243.  273.  310.  317.  310   454.  347.  330.  254.\n\n\nPart b\n\nCodesleep_long |&gt;\n  pivot_wider(names_from = day, values_from = reaction_time, names_prefix = \"day_\") |&gt;\n  head()\n\n# A tibble: 6 × 11\n  Subject day_0 day_1 day_2 day_3 day_4 day_5 day_6 day_7 day_8 day_9\n  &lt;fct&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 308      250.  259.  251.  321.  357.  415.  382.  290.  431.  466.\n2 309      223.  205.  203.  205.  208.  216.  214.  218.  224.  237.\n3 310      199.  194.  234.  233.  229.  220.  235.  256.  261.  248.\n4 330      322.  300.  284.  285.  286.  298.  280.  318.  305.  354.\n5 331      288.  285   302.  320.  316.  293.  290.  335.  294.  372.\n6 332      235.  243.  273.  310.  317.  310   454.  347.  330.  254.\n\n\nExercise 5: Practice with Billboard charts\nPart a\nThe higher a song’s week 1 rating, the higher its week 2 rating tends to be. But almost all song’s rankings drop from week 1 to week 2.\n\nCodeggplot(billboard, aes(y = wk2, x = wk1)) + \n  geom_point() +\n  geom_abline(intercept = 0, slope = 1)\n\nWarning: Removed 5 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\nPart b\n\nCodebillboard |&gt; \n  filter(wk2 &gt; wk1)\n\n# A tibble: 7 × 79\n  artist      track date.entered   wk1   wk2   wk3   wk4   wk5   wk6   wk7   wk8\n  &lt;chr&gt;       &lt;chr&gt; &lt;date&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Carey, Mar… Cryb… 2000-06-24      28    34    48    62    77    90    95    NA\n2 Clark, Ter… A Li… 2000-12-16      75    82    88    96    99    99    NA    NA\n3 Diffie, Joe The … 2000-01-01      98   100   100    90    93    94    NA    NA\n4 Hart, Beth  L.A.… 1999-11-27      99   100    98    99    99    99    98    90\n5 Jay-Z       Hey … 2000-08-12      98   100    98    94    83    83    80    78\n6 Lil' Zane   Call… 2000-07-29      83    89    57    40    34    21    33    46\n7 Pearl Jam   Noth… 2000-05-13      49    70    84    89    93    91    NA    NA\n# ℹ 68 more variables: wk9 &lt;dbl&gt;, wk10 &lt;dbl&gt;, wk11 &lt;dbl&gt;, wk12 &lt;dbl&gt;,\n#   wk13 &lt;dbl&gt;, wk14 &lt;dbl&gt;, wk15 &lt;dbl&gt;, wk16 &lt;dbl&gt;, wk17 &lt;dbl&gt;, wk18 &lt;dbl&gt;,\n#   wk19 &lt;dbl&gt;, wk20 &lt;dbl&gt;, wk21 &lt;dbl&gt;, wk22 &lt;dbl&gt;, wk23 &lt;dbl&gt;, wk24 &lt;dbl&gt;,\n#   wk25 &lt;dbl&gt;, wk26 &lt;dbl&gt;, wk27 &lt;dbl&gt;, wk28 &lt;dbl&gt;, wk29 &lt;dbl&gt;, wk30 &lt;dbl&gt;,\n#   wk31 &lt;dbl&gt;, wk32 &lt;dbl&gt;, wk33 &lt;dbl&gt;, wk34 &lt;dbl&gt;, wk35 &lt;dbl&gt;, wk36 &lt;dbl&gt;,\n#   wk37 &lt;dbl&gt;, wk38 &lt;dbl&gt;, wk39 &lt;dbl&gt;, wk40 &lt;dbl&gt;, wk41 &lt;dbl&gt;, wk42 &lt;dbl&gt;,\n#   wk43 &lt;dbl&gt;, wk44 &lt;dbl&gt;, wk45 &lt;dbl&gt;, wk46 &lt;dbl&gt;, wk47 &lt;dbl&gt;, wk48 &lt;dbl&gt;, …\n\n\nPart c\n\nCode# Define nov_1999\nnov_1999 &lt;- billboard |&gt; \n  filter(date.entered == \"1999-11-06\") |&gt; \n  select(-track, -date.entered)\n\n# Or\nnov_1999 &lt;- billboard |&gt; \n  filter(date.entered == \"1999-11-06\") |&gt; \n  select(artist, starts_with(\"wk\"))\n\n\n# Confirm that nov_1999 has 2 rows (songs) and 77 columns\ndim(nov_1999)\n\n[1]  2 77\n\n\nPart c\n\nCodenov_1999 |&gt; \n  pivot_longer(cols = -artist, names_to = \"week\", names_prefix = \"wk\", values_to = \"ranking\") |&gt; \n  mutate(week = as.numeric(week)) |&gt; \n  ggplot(aes(y = ranking, x = week, color = artist)) + \n    geom_line()\n\nWarning: Removed 79 rows containing missing values or values outside the scale range\n(`geom_line()`).\n\n\n\n\n\n\n\n\nExercise 6: Practice with the Daily Show\nPart a\n\nCodedaily |&gt; \n  count(raw_guest_list) |&gt; \n  arrange(desc(n)) |&gt; \n  head(15)\n\n# A tibble: 15 × 2\n   raw_guest_list        n\n   &lt;chr&gt;             &lt;int&gt;\n 1 Fareed Zakaria       19\n 2 Denis Leary          17\n 3 Brian Williams       16\n 4 Paul Rudd            13\n 5 Ricky Gervais        13\n 6 Tom Brokaw           12\n 7 Bill O'Reilly        10\n 8 Reza Aslan           10\n 9 Richard Lewis        10\n10 Will Ferrell         10\n11 Sarah Vowell          9\n12 Adam Sandler          8\n13 Ben Affleck           8\n14 Louis C.K.            8\n15 Maggie Gyllenhaal     8\n\n\nPart b\n\nCodedaily |&gt; \n  count(year, raw_guest_list) |&gt; \n  group_by(raw_guest_list) |&gt; \n  mutate(total = sum(n)) |&gt;\n  pivot_wider(names_from = year, \n              values_from = n,\n              values_fill = 0) |&gt; \n  arrange(desc(total)) |&gt; \n  head(15)\n\n# A tibble: 15 × 19\n# Groups:   raw_guest_list [15]\n   raw_guest_list  total `1999` `2000` `2001` `2002` `2003` `2004` `2005` `2006`\n   &lt;chr&gt;           &lt;int&gt;  &lt;int&gt;  &lt;int&gt;  &lt;int&gt;  &lt;int&gt;  &lt;int&gt;  &lt;int&gt;  &lt;int&gt;  &lt;int&gt;\n 1 Fareed Zakaria     19      0      0      1      0      1      2      2      2\n 2 Denis Leary        17      1      0      1      2      1      0      0      1\n 3 Brian Williams     16      0      0      0      0      1      1      2      1\n 4 Paul Rudd          13      1      0      1      1      1      1      1      0\n 5 Ricky Gervais      13      0      0      0      0      0      0      1      2\n 6 Tom Brokaw         12      0      0      0      1      0      2      1      0\n 7 Richard Lewis      10      1      0      2      2      1      1      0      0\n 8 Will Ferrell       10      0      1      1      0      1      1      1      1\n 9 Bill O'Reilly      10      0      0      1      1      0      1      1      0\n10 Reza Aslan         10      0      0      0      0      0      0      1      2\n11 Sarah Vowell        9      0      0      0      1      0      1      1      1\n12 Adam Sandler        8      1      2      0      1      0      0      0      1\n13 Ben Affleck         8      0      0      0      0      2      0      0      1\n14 Maggie Gyllenh…     8      0      0      0      0      1      0      1      1\n15 Louis C.K.          8      0      0      0      0      0      0      0      1\n# ℹ 9 more variables: `2007` &lt;int&gt;, `2008` &lt;int&gt;, `2009` &lt;int&gt;, `2010` &lt;int&gt;,\n#   `2011` &lt;int&gt;, `2012` &lt;int&gt;, `2013` &lt;int&gt;, `2014` &lt;int&gt;, `2015` &lt;int&gt;\n\n\nPart c\n\nCodeplot_data |&gt;\n  group_by(year, broad_group) |&gt;\n  summarise(n = n()) |&gt;\n  mutate(freq = n / sum(n)) |&gt; \n  ggplot(aes(y = freq, x = year, color = broad_group)) + \n    geom_line()\n\n`summarise()` has grouped output by 'year'. You can override using the\n`.groups` argument.",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>ica-reshaping</span>"
    ]
  },
  {
    "objectID": "ica/ica-reshaping.html#footnotes",
    "href": "ica/ica-reshaping.html#footnotes",
    "title": "\n20  ica-reshaping\n",
    "section": "",
    "text": "Gregory Belenky, Nancy J. Wesensten, David R. Thorne, Maria L. Thomas, Helen C. Sing, Daniel P. Redmond, Michael B. Russo and Thomas J. Balkin (2003) Patterns of performance degradation and restoration during sleep restriction and subsequent recovery: a sleep dose-response study. Journal of Sleep Research 12, 1–12.↩︎",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>ica-reshaping</span>"
    ]
  },
  {
    "objectID": "ica/ica-factors.html",
    "href": "ica/ica-factors.html",
    "title": "\n21  Factors\n",
    "section": "",
    "text": "21.1 Review\nWhere are we? Data preparation\nThus far, we’ve learned how to:",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Factors</span>"
    ]
  },
  {
    "objectID": "ica/ica-factors.html#review",
    "href": "ica/ica-factors.html#review",
    "title": "\n21  Factors\n",
    "section": "",
    "text": "do some wrangling:\n\n\narrange() our data in a meaningful order\nsubset the data to only filter() the rows and select() the columns of interest\n\nmutate() existing variables and define new variables\n\nsummarize() various aspects of a variable, both overall and by group (group_by())\n\n\nreshape our data to fit the task at hand (pivot_longer(), pivot_wider())\n\njoin() different datasets into one",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Factors</span>"
    ]
  },
  {
    "objectID": "ica/ica-factors.html#factors",
    "href": "ica/ica-factors.html#factors",
    "title": "\n21  Factors\n",
    "section": "\n21.2 Factors",
    "text": "21.2 Factors\nIn the remaining days of our data preparation unit, we’ll focus on working with special types of “categorical” variables: characters and factors. Variables with these structures often require special tools and considerations.\nWe’ll focus on two common considerations:\n\n\nRegular expressions\nWhen working with character strings, we might want to detect, replace, or extract certain patterns. For example, recall our data on courses:\n\n\n\n    sessionID dept level    sem enroll     iid\n1 session1784    M   100 FA1991     22 inst265\n2 session1785    k   100 FA1991     52 inst458\n3 session1791    J   100 FA1993     22 inst223\n4 session1792    J   300 FA1993     20 inst235\n5 session1794    J   200 FA1993     22 inst234\n6 session1795    J   200 SP1994     26 inst230\n\n\n'data.frame':   1718 obs. of  6 variables:\n $ sessionID: chr  \"session1784\" \"session1785\" \"session1791\" \"session1792\" ...\n $ dept     : chr  \"M\" \"k\" \"J\" \"J\" ...\n $ level    : int  100 100 100 300 200 200 200 100 300 100 ...\n $ sem      : chr  \"FA1991\" \"FA1991\" \"FA1993\" \"FA1993\" ...\n $ enroll   : int  22 52 22 20 22 26 25 38 16 43 ...\n $ iid      : chr  \"inst265\" \"inst458\" \"inst223\" \"inst235\" ...\n\n\nFocusing on just the sem character variable, we might want to…\n\nchange FA to fall_ and SP to spring_\n\nkeep only courses taught in fall\nsplit the variable into 2 new variables: semester (FA or SP) and year\n\n\n\n\nConverting characters to factors (and factors to meaningful factors) (today)\nWhen categorical information is stored as a character variable, the categories of interest might not be labeled or ordered in a meaningful way. We can fix that!\n\nExample 1: Default Order\nRecall our data on presidential election outcomes in each U.S. county (except those in Alaska):\n\nlibrary(tidyverse)\nelections &lt;- read.csv(\"https://mac-stat.github.io/data/election_2020_county.csv\") |&gt; \n  select(state_abbr, historical, county_name, total_votes_20, repub_pct_20, dem_pct_20) |&gt; \n  mutate(dem_support_20 = case_when(\n    (repub_pct_20 - dem_pct_20 &gt;= 5) ~ \"low\",\n    (repub_pct_20 - dem_pct_20 &lt;= -5) ~ \"high\",\n    .default = \"medium\"\n  ))\n\n# Check it out\nhead(elections)  \n\n  state_abbr historical    county_name total_votes_20 repub_pct_20 dem_pct_20\n1         AL        red Autauga County          27770        71.44      27.02\n2         AL        red Baldwin County         109679        76.17      22.41\n3         AL        red Barbour County          10518        53.45      45.79\n4         AL        red    Bibb County           9595        78.43      20.70\n5         AL        red  Blount County          27588        89.57       9.57\n6         AL        red Bullock County           4613        24.84      74.70\n  dem_support_20\n1            low\n2            low\n3            low\n4            low\n5            low\n6           high\n\n\nCheck out the below visual and numerical summaries of dem_support_20:\n\nlow = the Republican won the county by at least 5 percentage points\nmedium = the Republican and Democrat votes were within 5 percentage points\nhigh = the Democrat won the county by at least 5 percentage points\n\n\nggplot(elections, aes(x = dem_support_20)) + \n  geom_bar()\n\n\n\n\n\n\nelections |&gt; \n  count(dem_support_20)\n\n  dem_support_20    n\n1           high  458\n2            low 2494\n3         medium  157\n\n\nFollow-up:\nWhat don’t you like about these results?\nExample 2: Change Order using fct_relevel\n\nThe above categories of dem_support_20 are listed alphabetically, which isn’t particularly meaningful here. This is because dem_support_20 is a character variable and R thinks of character strings as words, not category labels with any meaningful order (other than alphabetical):\n\nstr(elections)\n\n'data.frame':   3109 obs. of  7 variables:\n $ state_abbr    : chr  \"AL\" \"AL\" \"AL\" \"AL\" ...\n $ historical    : chr  \"red\" \"red\" \"red\" \"red\" ...\n $ county_name   : chr  \"Autauga County\" \"Baldwin County\" \"Barbour County\" \"Bibb County\" ...\n $ total_votes_20: int  27770 109679 10518 9595 27588 4613 9488 50983 15284 12301 ...\n $ repub_pct_20  : num  71.4 76.2 53.5 78.4 89.6 ...\n $ dem_pct_20    : num  27.02 22.41 45.79 20.7 9.57 ...\n $ dem_support_20: chr  \"low\" \"low\" \"low\" \"low\" ...\n\n\nWe can fix this by using fct_relevel() to both:\n\nStore dem_support_20 as a factor variable, the levels of which are recognized as specific levels or categories, not just words.\nSpecify a meaningful order for the levels of the factor variable.\n\n\n# Notice that the order of the levels is not alphabetical!\nelections &lt;- elections |&gt; \n  mutate(dem_support_20 = fct_relevel(dem_support_20, c(\"low\", \"medium\", \"high\")))\n\n# Notice the new structure of the dem_support_20 variable\nstr(elections)\n\n'data.frame':   3109 obs. of  7 variables:\n $ state_abbr    : chr  \"AL\" \"AL\" \"AL\" \"AL\" ...\n $ historical    : chr  \"red\" \"red\" \"red\" \"red\" ...\n $ county_name   : chr  \"Autauga County\" \"Baldwin County\" \"Barbour County\" \"Bibb County\" ...\n $ total_votes_20: int  27770 109679 10518 9595 27588 4613 9488 50983 15284 12301 ...\n $ repub_pct_20  : num  71.4 76.2 53.5 78.4 89.6 ...\n $ dem_pct_20    : num  27.02 22.41 45.79 20.7 9.57 ...\n $ dem_support_20: Factor w/ 3 levels \"low\",\"medium\",..: 1 1 1 1 1 3 1 1 1 1 ...\n\n\n\n# And plot dem_support_20\nggplot(elections, aes(x = dem_support_20)) +\n  geom_bar()\n\n\n\n\n\n\n\nExample 3: Change Labels using fct_recode\n\nWe now have a factor variable, dem_support_20, with categories that are ordered in a meaningful way:\n\nelections |&gt; \n  count(dem_support_20)\n\n  dem_support_20    n\n1            low 2494\n2         medium  157\n3           high  458\n\n\nBut maybe we want to change up the category labels. For demo purposes, let’s create a new factor variable, results_20, that’s the same as dem_support_20 but with different category labels:\n\n# We can redefine any number of the category labels.\n# Here we'll relabel all 3 categories:\nelections &lt;- elections |&gt; \n  mutate(results_20 = fct_recode(dem_support_20, \n                                 \"strong republican\" = \"low\",\n                                 \"close race\" = \"medium\",\n                                 \"strong democrat\" = \"high\"))\n\n# Check it out\n# Note that the new category labels are still in a meaningful,\n# not necessarily alphabetical, order!\nelections |&gt; \n  count(results_20)\n\n         results_20    n\n1 strong republican 2494\n2        close race  157\n3   strong democrat  458\n\n\nExample 4: Re-order Levels using fct_relevel\n\nFinally, let’s explore how the Republican vote varied from county to county within each state:\n\n# Note that we're just piping the data into ggplot instead of writing\n# it as the first argument\nelections |&gt; \n  ggplot(aes(x = repub_pct_20, fill = state_abbr)) + \n    geom_density(alpha = 0.5)\n\n\n\n\n\n\n\nThis is too many density plots to put on top of one another. Let’s spread these out while keeping them in the same frame, hence easier to compare, using a joy plot or ridge plot:\n\nlibrary(ggridges)\nelections |&gt; \n  ggplot(aes(x = repub_pct_20, y = state_abbr, fill = historical)) + \n    geom_density_ridges() + \n    scale_fill_manual(values = c(\"blue\", \"purple\", \"red\"))\n\n\n\n\n\n\n\nOK, but this is alphabetical. Suppose we want to reorder the states according to their typical Republican support. Recall that we did something similar in Example 2, using fct_relevel() to specify a meaningful order for the dem_support_20 categories:\nfct_relevel(dem_support_20, c(\"low\", \"medium\", \"high\"))\nWe could use fct_relevel() to reorder the states here, but what would be the drawbacks?\nExample 5: Re-order levels Based on Another Variable using fct_reorder\n\nWhen a meaningful order for the categories of a factor variable can be defined by another variable in our dataset, we can use fct_reorder(). In our joy plot, let’s reorder the states according to their median Republican support:\n\n# Since we might want states to be alphabetical in other parts of our analysis,\n# we'll pipe the data into the ggplot without storing it:\nelections |&gt; \n  mutate(state_abbr = fct_reorder(state_abbr, repub_pct_20, .fun = \"median\")) |&gt; \n  ggplot(aes(x = repub_pct_20, y = state_abbr, fill = historical)) + \n    geom_density_ridges() + \n    scale_fill_manual(values = c(\"blue\", \"purple\", \"red\"))\n\n\n\n\n\n\n\n\n# How did the code change?\n# And the corresponding output?\nelections |&gt; \n  mutate(state_abbr = fct_reorder(state_abbr, repub_pct_20, .fun = \"median\", .desc = TRUE)) |&gt; \n  ggplot(aes(x = repub_pct_20, y = state_abbr, fill = historical)) + \n    geom_density_ridges() + \n    scale_fill_manual(values = c(\"blue\", \"purple\", \"red\"))\n\n\n\n\n\n\n\nSummary\nThe forcats package, part of the tidyverse, includes handy functions for working with categorical variables (for + cats):\n\nHere are just some, few of which we explored above:\n\nfunctions for changing the order of factor levels\n\n\nfct_relevel() = manually reorder levels\n\nfct_reorder() = reorder levels according to values of another variable\n\n\nfct_infreq() = order levels from highest to lowest frequency\n\nfct_rev() = reverse the current order\n\n\nfunctions for changing the labels or values of factor levels\n\n\nfct_recode() = manually change levels\n\nfct_lump() = group together least common levels",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Factors</span>"
    ]
  },
  {
    "objectID": "ica/ica-factors.html#exercises",
    "href": "ica/ica-factors.html#exercises",
    "title": "\n21  Factors\n",
    "section": "\n21.3 Exercises",
    "text": "21.3 Exercises\nThe exercises revisit our grades data:\n\n# Get rid of some duplicate rows!\ngrades &lt;- read.csv(\"https://mac-stat.github.io/data/grades.csv\") |&gt; \n  distinct(sid, sessionID, .keep_all = TRUE)\n\n# Check it out\nhead(grades)\n\n     sid grade   sessionID\n1 S31185    D+ session1784\n2 S31185    B+ session1785\n3 S31185    A- session1791\n4 S31185    B+ session1792\n5 S31185    B- session1794\n6 S31185    C+ session1795\n\n\nWe’ll explore the number of times each grade was assigned:\n\ngrade_distribution &lt;- grades |&gt; \n  count(grade)\n\nhead(grade_distribution)\n\n  grade    n\n1     A 1506\n2    A- 1381\n3    AU   27\n4     B  804\n5    B+ 1003\n6    B-  330\n\n\nExercise 1: Changing Order\nCheck out a column plot of the number of times each grade was assigned during the study period. This is similar to a bar plot, but where we define the height of a bar according to variable in our dataset.\n\ngrade_distribution |&gt; \n  ggplot(aes(x = grade, y = n)) +\n    geom_col()\n\n\n\n\n\n\n\nThe order of the grades is goofy! Construct a new column plot, manually reordering the grades from high (A) to low (NC) with “S” and “AU” at the end:\n\ngrade_distribution |&gt;\n  mutate(grade = fct_relevel(grade, c(\"A\", \"A-\", \"B+\", \"B\", \"B-\", \"C+\", \"C\", \"C-\", \"D+\", \"D\", \"D-\", \"NC\", \"S\", \"AU\"))) |&gt;\n  ggplot(aes(x = grade, y = n)) +\n    geom_col()\n\n\n\n\n\n\n\nConstruct a new column plot, reordering the grades in ascending frequency (i.e. how often the grades were assigned):\n\ngrade_distribution |&gt;\n  mutate(grade = fct_reorder(grade, n)) |&gt;\n  ggplot(aes(x = grade, y = n)) +\n    geom_col()\n\n\n\n\n\n\n\nConstruct a new column plot, reordering the grades in descending frequency (i.e. how often the grades were assigned):\n\ngrade_distribution |&gt;\n  mutate(grade = fct_reorder(grade, n, .desc = TRUE)) |&gt;\n  ggplot(aes(x = grade, y = n)) +\n    geom_col()\n\n\n\n\n\n\n\nExercise 2: Changing Factor Level Labels\nIt may not be clear what “AU” and “S” stand for. Construct a new column plot that renames these levels “Audit” and “Satisfactory”, while keeping the other grade labels the same and in a meaningful order:\n\ngrade_distribution |&gt;\n  mutate(grade = fct_relevel(grade, c(\"A\", \"A-\", \"B+\", \"B\", \"B-\", \"C+\", \"C\", \"C-\", \"D+\", \"D\", \"D-\", \"NC\", \"S\", \"AU\"))) |&gt;\n  mutate(grade = fct_recode(grade, \"Satisfactory\" = \"S\", \"Audit\" = \"AU\")) |&gt;  # Multiple pieces go into the last 2 blanks\n  ggplot(aes(x = grade, y = n)) +\n    geom_col()",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Factors</span>"
    ]
  },
  {
    "objectID": "ica/ica-factors.html#solutions",
    "href": "ica/ica-factors.html#solutions",
    "title": "\n21  Factors\n",
    "section": "\n21.4 Solutions",
    "text": "21.4 Solutions\n\nClick for Solutions\nExample 1: Default Orde\nThe categories are in alphabetical order, which isn’t meaningful here.\nExample 4: Re-ordering Levels using fct_relevel\n\nwe would have to:\n\nCalculate the typical Republican support in each state, e.g. using group_by() and summarize().\nWe’d then have to manually type out a meaningful order for 50 states! That’s a lot of typing and manual bookkeeping.\nExercise 1: Changing Order\n\ngrade_distribution |&gt;\n  mutate(grade = fct_relevel(grade, c(\"A\", \"A-\", \"B+\", \"B\", \"B-\", \"C+\", \"C\", \"C-\", \"D+\", \"D\", \"D-\", \"NC\", \"S\", \"AU\"))) |&gt;\n  ggplot(aes(x = grade, y = n)) +\n    geom_col()\n\n\n\n\n\n\n\n\ngrade_distribution |&gt;\n  mutate(grade = fct_reorder(grade, n)) |&gt;\n  ggplot(aes(x = grade, y = n)) +\n    geom_col()\n\n\n\n\n\n\n\n\ngrade_distribution |&gt;\n  mutate(grade = fct_reorder(grade, n, .desc = TRUE)) |&gt;\n  ggplot(aes(x = grade, y = n)) +\n    geom_col()\n\n\n\n\n\n\n\nExercise 2: Changing Factor Level Labels\n\ngrade_distribution |&gt;\n  mutate(grade = fct_relevel(grade, c(\"A\", \"A-\", \"B+\", \"B\", \"B-\", \"C+\", \"C\", \"C-\", \"D+\", \"D\", \"D-\", \"NC\", \"S\", \"AU\"))) |&gt;\n  mutate(grade = fct_recode(grade, \"Satisfactory\" = \"S\", \"Audit\" = \"AU\")) |&gt;  # Multiple pieces go into the last 2 blanks\n  ggplot(aes(x = grade, y = n)) +\n    geom_col()",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Factors</span>"
    ]
  },
  {
    "objectID": "ica/ica-strings.html",
    "href": "ica/ica-strings.html",
    "title": "\n22  Strings\n",
    "section": "",
    "text": "22.1 Review\nWHERE ARE WE?\nWe’re in the last day of our “data preparation” unit:",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Strings</span>"
    ]
  },
  {
    "objectID": "ica/ica-strings.html#strings",
    "href": "ica/ica-strings.html#strings",
    "title": "\n22  Strings\n",
    "section": "\n22.2 Strings",
    "text": "22.2 Strings\nIn the previous class, we started discussing some considerations in working with special types of “categorical” variables: characters and factors which are:\n\nConverting characters to factors (and factors to meaningful factors)–last time\nWhen categorical information is stored as a character variable, the categories of interest might not be labeled or ordered in a meaningful way. We can fix that!\nStrings–today!\nWhen working with character strings, we might want to detect, replace, or extract certain patterns. For example, recall our data on courses:\n\n\n\n    sessionID dept level    sem enroll     iid\n1 session1784    M   100 FA1991     22 inst265\n2 session1785    k   100 FA1991     52 inst458\n3 session1791    J   100 FA1993     22 inst223\n4 session1792    J   300 FA1993     20 inst235\n5 session1794    J   200 FA1993     22 inst234\n6 session1795    J   200 SP1994     26 inst230\n\n\n'data.frame':   1718 obs. of  6 variables:\n $ sessionID: chr  \"session1784\" \"session1785\" \"session1791\" \"session1792\" ...\n $ dept     : chr  \"M\" \"k\" \"J\" \"J\" ...\n $ level    : int  100 100 100 300 200 200 200 100 300 100 ...\n $ sem      : chr  \"FA1991\" \"FA1991\" \"FA1993\" \"FA1993\" ...\n $ enroll   : int  22 52 22 20 22 26 25 38 16 43 ...\n $ iid      : chr  \"inst265\" \"inst458\" \"inst223\" \"inst235\" ...\n\n\nFocusing on just the sem character variable, we might want to…\n\nchange FA to fall_ and SP to spring_\n\nkeep only courses taught in fall\nsplit the variable into 2 new variables: semester (FA or SP) and year\n\n\n\nMuch more!–maybe in your projects or COMP/STAT 212\nThere are a lot of ways to process character variables. For example, we might have a variable that records the text for a sample of news articles. We might want to analyze things like the articles’ sentiments, word counts, typical word lengths, most common words, etc.\n\nEssential Functions\nThe stringr package within tidyverse contains lots of functions to help process strings. We’ll focus on the most common. Letting x be a string variable…\n\n\nfunction\narguments\nreturns\n\n\n\nstr_replace()\nx, pattern, replacement\na modified string\n\n\nstr_replace_all()\nx, pattern, replacement\na modified string\n\n\nstr_to_lower()\nx\na modified string\n\n\nstr_sub()\nx, start, end\na modified string\n\n\nstr_length()\nx\na number\n\n\nstr_detect()\nx, pattern\nTRUE/FALSE\n\n\nExample 1\nConsider the following data with string variables :\n\nlibrary(tidyverse)\n\nclasses &lt;- data.frame(\n  sem        = c(\"SP2023\", \"FA2023\", \"SP2024\"),\n  area       = c(\"History\", \"Math\", \"Anthro\"),\n  enroll     = c(\"30 - people\", \"20 - people\", \"25 - people\"),\n  instructor = c(\"Ernesto Capello\", \"Lori Ziegelmeier\", \"Arjun Guneratne\")\n)\n\nclasses\n\n     sem    area      enroll       instructor\n1 SP2023 History 30 - people  Ernesto Capello\n2 FA2023    Math 20 - people Lori Ziegelmeier\n3 SP2024  Anthro 25 - people  Arjun Guneratne\n\n\nUsing only your intuition, use our str_ functions to complete the following. NOTE: You might be able to use other wrangling verbs in some cases, but focus on the new functions here.\n\n# Define a new variable \"num\" that adds up the number of characters in the area label\n\n\n# Change the areas to \"history\", \"math\", \"anthro\" instead of \"History\", \"Math\", \"Anthro\"\n\n\n# Create a variable that id's which courses were taught in spring\n\n\n# Change the semester labels to \"fall2023\", \"spring2024\", \"spring2023\"\n\n\n# In the enroll variable, change all e's to 3's (just because?)\n\n\n# Use sem to create 2 new variables, one with only the semester (SP/FA) and 1 with the year\n\nSummary\nHere’s what we learned about each function:\n\nstr_replace(x, pattern, replacement) finds the first part of x that matches the pattern and replaces it with replacement\nstr_replace_all(x, pattern, replacement) finds all instances in x that matches the pattern and replaces it with replacement\nstr_to_lower(x) converts all upper case letters in x to lower case\nstr_sub(x, start, end) only keeps a subset of characters in x, from start (a number indexing the first letter to keep) to end (a number indexing the last letter to keep)\nstr_length(x) records the number of characters in x\nstr_detect(x, pattern) is TRUE if x contains the given pattern and FALSE otherwise\n\n22.2.1 Example 2\nSuppose we only want the spring courses:\n\n# How can we do this after mutating?\nclasses |&gt; \n  mutate(spring = str_detect(sem, \"SP\"))\n\n     sem    area      enroll       instructor spring\n1 SP2023 History 30 - people  Ernesto Capello   TRUE\n2 FA2023    Math 20 - people Lori Ziegelmeier  FALSE\n3 SP2024  Anthro 25 - people  Arjun Guneratne   TRUE\n\n\n\n# We don't have to mutate first!\nclasses |&gt; \n  filter(str_detect(sem, \"SP\"))\n\n     sem    area      enroll      instructor\n1 SP2023 History 30 - people Ernesto Capello\n2 SP2024  Anthro 25 - people Arjun Guneratne\n\n\n\n# Yet another way\nclasses |&gt; \n  filter(!str_detect(sem, \"FA\"))\n\n     sem    area      enroll      instructor\n1 SP2023 History 30 - people Ernesto Capello\n2 SP2024  Anthro 25 - people Arjun Guneratne\n\n\n\n22.2.2 Example 3\nSuppose we wanted to get separate columns for the first and last names of each course instructor in classes. Try doing this using str_sub(). But don’t try too long! Explain what trouble you ran into.\n\n22.2.3 Example 4\nIn general, when we want to split a column into 2+ new columns, we can often use separate():\n\nclasses |&gt; \n  separate(instructor, c(\"first\", \"last\"), sep = \" \")\n\n     sem    area      enroll   first        last\n1 SP2023 History 30 - people Ernesto     Capello\n2 FA2023    Math 20 - people    Lori Ziegelmeier\n3 SP2024  Anthro 25 - people   Arjun   Guneratne\n\n\n\n# Sometimes the function can \"intuit\" how we want to separate the variable\nclasses |&gt; \n  separate(instructor, c(\"first\", \"last\"))\n\n     sem    area      enroll   first        last\n1 SP2023 History 30 - people Ernesto     Capello\n2 FA2023    Math 20 - people    Lori Ziegelmeier\n3 SP2024  Anthro 25 - people   Arjun   Guneratne\n\n\n\nSeparate enroll into 2 separate columns: students and people. (These columns don’t make sense this is just practice).\n\n\n# classes |&gt; \n#   separate(___, c(___, ___), sep = \"___\")\n\n\nWe separated sem into semester and year above using str_sub(). Why would this be hard using separate()?\nWhen we want to split a column into 2+ new columns (or do other types of string processing), but there’s no consistent pattern by which to do this, we can use regular expressions (an optional topic):\n\n\n# (?&lt;=[SP|FA]): any character *before* the split point is a \"SP\" or \"FA\"\n# (?=2): the first character *after* the split point is a 2\nclasses |&gt; \n  separate(sem, \n          c(\"semester\", \"year\"),\n          \"(?&lt;=[SP|FA])(?=2)\")\n\n  semester year    area      enroll       instructor\n1       SP 2023 History 30 - people  Ernesto Capello\n2       FA 2023    Math 20 - people Lori Ziegelmeier\n3       SP 2024  Anthro 25 - people  Arjun Guneratne\n\n\n\n# More general:\n# (?&lt;=[a-zA-Z]): any character *before* the split point is a lower or upper case letter\n# (?=[0-9]): the first character *after* the split point is number\nclasses |&gt; \n  separate(sem, \n          c(\"semester\", \"year\"),\n          \"(?&lt;=[A-Z])(?=[0-9])\")\n\n  semester year    area      enroll       instructor\n1       SP 2023 History 30 - people  Ernesto Capello\n2       FA 2023    Math 20 - people Lori Ziegelmeier\n3       SP 2024  Anthro 25 - people  Arjun Guneratne",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Strings</span>"
    ]
  },
  {
    "objectID": "ica/ica-strings.html#exercises",
    "href": "ica/ica-strings.html#exercises",
    "title": "\n22  Strings\n",
    "section": "\n22.3 Exercises",
    "text": "22.3 Exercises\nExercise 1: Time slots\nThe courses data includes actual data scraped from Mac’s class schedule. (Thanks to Prof Leslie Myint for the scraping code!!)\nIf you want to learn how to scrape data, take COMP/STAT 212, Intermediate Data Science! NOTE: For simplicity, I removed classes that had “TBA” for the days.\n\ncourses &lt;- read.csv(\"https://mac-stat.github.io/data/registrar.csv\")\n\n# Check it out\nhead(courses)\n\n       number   crn                                                name  days\n1 AMST 112-01 10318         Introduction to African American Literature M W F\n2 AMST 194-01 10073              Introduction to Asian American Studies M W F\n3 AMST 194-F1 10072 What’s After White Empire - And Is It Already Here?  T R \n4 AMST 203-01 10646 Politics and Inequality: The American Welfare State M W F\n5 AMST 205-01 10842                         Trans Theories and Politics  T R \n6 AMST 209-01 10474                   Civil Rights in the United States   W  \n             time      room             instructor avail_max\n1 9:40 - 10:40 am  MAIN 009       Daylanne English    3 / 20\n2  1:10 - 2:10 pm MUSIC 219          Jake Nagasawa   -4 / 16\n3  3:00 - 4:30 pm   HUM 214 Karin Aguilar-San Juan    0 / 14\n4 9:40 - 10:40 am  CARN 305          Lesley Lavery    3 / 25\n5  3:00 - 4:30 pm  MAIN 009              Myrl Beam   -2 / 20\n6 7:00 - 10:00 pm  MAIN 010         Walter Greason   -1 / 15\n\n\nUse our more familiar wrangling tools to warm up.\n\n# Construct a table that indicates the number of classes offered in each day/time slot\n# Print only the 6 most popular time slots\ntop_times &lt;- courses %&gt;%\n  group_by(days, time) %&gt;%\n  summarise(n = n(), .groups = \"drop\") %&gt;%\n  arrange(desc(n)) %&gt;%\n  head(6)\n\ntop_times\n\n# A tibble: 6 × 3\n  days    time                 n\n  &lt;chr&gt;   &lt;chr&gt;            &lt;int&gt;\n1 \"M W F\" 10:50 - 11:50 am    76\n2 \" T R \" 9:40 - 11:10 am     71\n3 \"M W F\" 9:40 - 10:40 am     68\n4 \"M W F\" 1:10 - 2:10 pm      66\n5 \" T R \" 3:00 - 4:30 pm      62\n6 \" T R \" 1:20 - 2:50 pm      59\n\n\nExercise 2: Prep the data\nSo that we can analyze it later, we want to wrangle the courses data:\n\nLet’s get some enrollment info:\n\nSplit avail_max into 2 separate variables: avail and max.\nUse avail and max to define a new variable called enroll. HINT: You’ll need as.numeric()\n\n\n\nSplit the course number into 3 separate variables: dept, number, and section. HINT: You can use separate() to split a variable into 3, not just 2 new variables.\n\nStore this as courses_clean so that you can use it later.\n\ncourses_clean &lt;- courses |&gt; \n  separate(avail_max, c(\"avail\", \"max\"), sep = \" / \") |&gt; \n  mutate(enroll = as.numeric(max) - as.numeric(avail)) |&gt; \n  separate(number, c(\"dept\", \"number\", \"section\"))\n\nExercise 3: Courses by department\nUsing courses_clean…\n\n# 1. Identify the 6 departments that offered the most sections\n\n# 2. Identify the 6 departments with the longest average course titles\n\ncourses_clean |&gt; \n  count(dept) |&gt; \n  arrange(desc(n)) |&gt; \n  head()\n\n  dept  n\n1 SPAN 45\n2 BIOL 44\n3 ENVI 38\n4 PSYC 37\n5 CHEM 33\n6 COMP 31\n\ncourses_clean |&gt; \n  mutate(length = str_length(name)) |&gt; \n  group_by(dept) |&gt; \n  summarize(avg_length = mean(length)) |&gt; \n  arrange(desc(avg_length)) |&gt; \n  head()\n\n# A tibble: 6 × 2\n  dept  avg_length\n  &lt;chr&gt;      &lt;dbl&gt;\n1 WGSS        46.3\n2 INTL        41.4\n3 EDUC        39.4\n4 MCST        39.4\n5 POLI        37.4\n6 AMST        37.3\n\n\nExercise 4: STAT courses\nPart a\nGet a subset of courses_clean that only includes courses taught by Alicia Johnson.\n\ncourses_clean |&gt; \n  filter(str_detect(instructor, \"Alicia Johnson\"))\n\n  dept number section   crn                         name  days            time\n1 STAT    253      01 10806 Statistical Machine Learning  T R  9:40 - 11:10 am\n2 STAT    253      02 10807 Statistical Machine Learning  T R   1:20 - 2:50 pm\n3 STAT    253      03 10808 Statistical Machine Learning  T R   3:00 - 4:30 pm\n        room     instructor avail max enroll\n1 THEATR 206 Alicia Johnson    -3  20     23\n2 THEATR 206 Alicia Johnson    -3  20     23\n3 THEATR 206 Alicia Johnson     2  20     18\n\n\nPart b\nCreate a new dataset from courses_clean, named stat, that only includes STAT sections. In this dataset:\n\n\nIn the course names:\n\nRemove “Introduction to” from any name.\nShorten “Statistical” to “Stat” where relevant.\n\n\nDefine a variable that records the start_time for the course.\nKeep only the number, name, start_time, enroll columns.\nThe result should have 19 rows and 4 columns.\n\n\nstat &lt;- courses_clean |&gt; \n  filter(dept == \"STAT\") |&gt; \n  mutate(name = str_replace(name, \"Introduction to \", \"\")) |&gt; \n  mutate(name = str_replace(name, \"Statistical\", \"Stat\")) |&gt; \n  mutate(start_time = str_sub(time, 1, 5)) |&gt; \n  select(number, name, start_time, enroll)\n\nstat\n\n   number                      name start_time enroll\n1     112              Data Science      3:00      27\n2     112              Data Science      9:40      21\n3     112              Data Science      1:20      25\n4     125              Epidemiology      12:00     26\n5     155             Stat Modeling      1:10      32\n6     155             Stat Modeling      9:40      24\n7     155             Stat Modeling      10:50     26\n8     155             Stat Modeling      3:30      25\n9     155             Stat Modeling      1:20      30\n10    155             Stat Modeling      3:00      27\n11    212 Intermediate Data Science      9:40      11\n12    212 Intermediate Data Science      1:20      11\n13    253     Stat Machine Learning      9:40      23\n14    253     Stat Machine Learning      1:20      23\n15    253     Stat Machine Learning      3:00      18\n16    354               Probability      3:00      22\n17    452           Correlated Data      9:40       7\n18    452           Correlated Data      1:20       8\n19    456  Projects in Data Science      9:40      11\n\n\nExercise 5: More cleaning\nIn the next exercises, we’ll dig into enrollments. Let’s get the data ready for that analysis here. Make the following changes to the courses_clean data. Because they have different enrollment structures, and we don’t want to compare apples and oranges, remove the following:\n\nall sections in PE and INTD (interdisciplinary studies courses)\nall music ensembles and dance practicums, i.e. all MUSI and THDA classes with numbers less than 100. HINT: !(dept == \"MUSI\" & as.numeric(number) &lt; 100)\nall lab sections. Be careful which variable you use here. For example, you don’t want to search by “Lab” and accidentally eliminate courses with words such as “Labor”.\n\nSave the results as enrollments (don’t overwrite courses_clean).\n\nenrollments &lt;- courses_clean |&gt; \n  filter(!(dept %in% c(\"PE\", \"INTD\"))) |&gt; \n  filter(!(dept == \"MUSI\" & as.numeric(number) &lt; 100)) |&gt; \n  filter(!(dept == \"THDA\" & as.numeric(number) &lt; 100)) |&gt; \n  filter(!str_starts(section, \"L\"))\n\nenrollments\n\n    dept number section   crn\n1   AMST    112      01 10318\n2   AMST    194      01 10073\n3   AMST    194      F1 10072\n4   AMST    203      01 10646\n5   AMST    205      01 10842\n6   AMST    209      01 10474\n7   AMST    225      01 10476\n8   AMST    237      01 10075\n9   AMST    240      01 10312\n10  AMST    270      01 10077\n11  AMST    271      01 10481\n12  AMST    275      01 10331\n13  AMST    284      01 10078\n14  AMST    291      01 10917\n15  AMST    294      01 10080\n16  AMST    294      02 10719\n17  AMST    294      03 10846\n18  AMST    308      01 10788\n19  AMST    380      01 10338\n20  AMST    387      01 10795\n21  AMST    394      01 10081\n22  AMST    400      01 10082\n23  ANTH    101      01 10886\n24  ANTH    111      01 10083\n25  ANTH    111      02 10084\n26  ANTH    115      01 10085\n27  ANTH    206      01 10536\n28  ANTH    230      01 10086\n29  ANTH    239      01 10087\n30  ANTH    240      01 10088\n31  ANTH    248      01 10887\n32  ANTH    251      F1 10089\n33  ANTH    294      01 10953\n34  ANTH    335      01 10506\n35  ANTH    363      01 10091\n36  ANTH    487      01 10093\n37  ANTH    494      01 10908\n38   ART    130      01 10094\n39   ART    130      02 10095\n40   ART    149      F1 10096\n41   ART    160      01 10097\n42   ART    194      F1 10099\n43   ART    233      01 10100\n44   ART    234      01 10101\n45   ART    235      01 10102\n46   ART    236      01 10103\n47   ART    238      01 10104\n48   ART    239      01 10105\n49   ART    252      01 10106\n50   ART    264      01 10108\n51   ART    267      01 10109\n52   ART    272      01 10110\n53   ART    280      01 10112\n54   ART    294      01 10113\n55   ART    371      01 10116\n56   ART    373      01 10117\n57   ART    380      01 10118\n58   ART    394      01 10912\n59  ASIA    113      01 10708\n60  ASIA    140      01 10465\n61  ASIA    194      01 10074\n62  ASIA    194      02 10471\n63  ASIA    236      01 10715\n64  ASIA    254      F1 10520\n65  ASIA    258      01 10223\n66  ASIA    272      01 10111\n67  ASIA    282      01 10522\n68  ASIA    294      01 10490\n69  ASIA    394      01 10528\n70  BIOL    170      01 10119\n71  BIOL    170      02 10121\n72  BIOL    170      03 10123\n73  BIOL    180      01 10133\n74  BIOL    190      01 10136\n75  BIOL    190      02 10137\n76  BIOL    200      01 10142\n77  BIOL    200      02 10853\n78  BIOL    275      01 10145\n79  BIOL    277      01 10357\n80  BIOL    302      01 10149\n81  BIOL    304      01 10872\n82  BIOL    312      01 10151\n83  BIOL    316      01 10153\n84  BIOL    351      01 10203\n85  BIOL    357      01 10157\n86  BIOL    366      01 10159\n87  BIOL    369      01 10163\n88  BIOL    380      01 10165\n89  BIOL    401      01 10169\n90  BIOL    404      01 10170\n91  BIOL    412      01 10172\n92  BIOL    475      01 10173\n93  CHEM    111      01 10175\n94  CHEM    111      02 10176\n95  CHEM    111      03 10177\n96  CHEM    111      04 10178\n97  CHEM    111      05 10179\n98  CHEM    115      F1 10186\n99  CHEM    211      01 10188\n100 CHEM    211      02 10189\n101 CHEM    211      03 10190\n102 CHEM    300      01 10197\n103 CHEM    311      01 10198\n104 CHEM    351      01 10202\n105 CHEM    394      01 10208\n106 CHEM    411      01 10209\n107 CHIN    101      01 10212\n108 CHIN    101      02 10213\n109 CHIN    203      01 10217\n110 CHIN    203      02 10218\n111 CHIN    258      01 10222\n112 CHIN    305      01 10225\n113 CHIN    407      01 10228\n114 CLAS    111      01 10229\n115 CLAS    113      01 10231\n116 CLAS    115      01 10233\n117 CLAS    122      01 10236\n118 CLAS    129      01 10238\n119 CLAS    194      01 10239\n120 CLAS    202      01 10716\n121 CLAS    220      01 10901\n122 CLAS    231      01 10240\n123 CLAS    237      01 10241\n124 CLAS    241      01 10242\n125 CLAS    260      01 10098\n126 CLAS    294      03 10871\n127 CLAS    482      01 10900\n128 CLAS    485      01 10247\n129 CLAS    490      01 10246\n130 COMP    112      01 10248\n131 COMP    112      02 10250\n132 COMP    112      03 10252\n133 COMP    123      01 10254\n134 COMP    123      02 10255\n135 COMP    123      03 10256\n136 COMP    123      04 10257\n137 COMP    127      01 10259\n138 COMP    127      02 10260\n139 COMP    127      03 10261\n140 COMP    128      01 10265\n141 COMP    128      02 10266\n142 COMP    194      01 10563\n143 COMP    194      F1 10258\n144 COMP    212      01 10267\n145 COMP    212      02 10269\n146 COMP    221      01 10271\n147 COMP    225      01 10272\n148 COMP    225      02 10273\n149 COMP    240      01 10274\n150 COMP    240      02 10275\n151 COMP    272      01 10923\n152 COMP    435      01 10276\n153 COMP    445      01 10277\n154 COMP    446      01 10278\n155 COMP    456      01 10279\n156 COMP    484      01 10881\n157 COMP    494      01 10896\n158 ECON    113      01 10281\n159 ECON    113      02 10282\n160 ECON    117      01 10283\n161 ECON    119      01 10286\n162 ECON    119      02 10287\n163 ECON    119      03 10288\n164 ECON    119      04 10289\n165 ECON    119      F1 10284\n166 ECON    119      F2 10285\n167 ECON    129      01 10290\n168 ECON    221      01 10913\n169 ECON    229      01 10291\n170 ECON    238      01 10862\n171 ECON    294      01 10292\n172 ECON    294      02 10863\n173 ECON    294      03 10864\n174 ECON    361      01 10293\n175 ECON    361      02 10294\n176 ECON    371      01 10295\n177 ECON    381      01 10296\n178 ECON    381      02 10298\n179 ECON    425      01 10302\n180 ECON    426      01 10303\n181 ECON    444      01 10304\n182 ECON    457      01 10305\n183 ECON    473      01 10902\n184 ECON    474      01 10903\n185 EDUC    194      F1 10308\n186 EDUC    220      01 10309\n187 EDUC    220      02 10976\n188 EDUC    240      01 10311\n189 EDUC    250      01 10313\n190 EDUC    275      01 10144\n191 EDUC    315      01 10660\n192 EDUC    380      01 10314\n193 EDUC    390      01 10315\n194 EDUC    394      01 10392\n195 EDUC    460      01 10316\n196 ENGL    105      01 10879\n197 ENGL    112      01 10317\n198 ENGL    115      01 10319\n199 ENGL    137      01 10320\n200 ENGL    150      01 10322\n201 ENGL    150      02 10323\n202 ENGL    150      03 10324\n203 ENGL    150      05 10878\n204 ENGL    150      06 10880\n205 ENGL    150      F1 10321\n206 ENGL    194      F1 10326\n207 ENGL    200      01 10327\n208 ENGL    202      01 10910\n209 ENGL    208      01 10916\n210 ENGL    225      F1 10575\n211 ENGL    230      01 10892\n212 ENGL    262      01 10328\n213 ENGL    275      01 10330\n214 ENGL    280      01 10332\n215 ENGL    284      01 10333\n216 ENGL    285      01 10848\n217 ENGL    285      02 10849\n218 ENGL    294      01 10114\n219 ENGL    294      04 10336\n220 ENGL    294      05 10844\n221 ENGL    380      01 10337\n222 ENGL    394      02 10341\n223 ENGL    394      03 10342\n224 ENGL    400      01 10893\n225 ENGL    406      01 10344\n226 ENVI    130      01 10620\n227 ENVI    150      01 10345\n228 ENVI    160      01 10425\n229 ENVI    170      01 10120\n230 ENVI    170      02 10122\n231 ENVI    170      03 10124\n232 ENVI    202      01 11004\n233 ENVI    203      01 10397\n234 ENVI    215      01 10931\n235 ENVI    232      01 10402\n236 ENVI    234      01 10346\n237 ENVI    237      01 10076\n238 ENVI    240      01 10348\n239 ENVI    252      F1 10350\n240 ENVI    262      01 10329\n241 ENVI    270      01 10353\n242 ENVI    275      01 10143\n243 ENVI    277      01 10356\n244 ENVI    280      01 10355\n245 ENVI    291      01 10891\n246 ENVI    294      01 10358\n247 ENVI    294      02 10454\n248 ENVI    366      01 10160\n249 ENVI    380      01 10166\n250 ENVI    391      01 10890\n251 ENVI    477      01 10420\n252 ENVI    489      01 10360\n253 ENVI    490      01 10361\n254 FREN    101      01 10362\n255 FREN    101      02 10363\n256 FREN    111      01 10371\n257 FREN    194      F1 10374\n258 FREN    203      01 10375\n259 FREN    203      02 10376\n260 FREN    204      01 10381\n261 FREN    305      01 10387\n262 FREN    306      01 10390\n263 FREN    321      01 10884\n264 FREN    371      01 10899\n265 FREN    394      01 10391\n266 GEOG    115      01 10395\n267 GEOG    203      01 10396\n268 GEOG    225      01 10398\n269 GEOG    232      01 10401\n270 GEOG    239      F1 10403\n271 GEOG    241      01 10405\n272 GEOG    242      01 10407\n273 GEOG    242      F1 10406\n274 GEOG    248      F1 10408\n275 GEOG    252      F1 10351\n276 GEOG    256      01 10411\n277 GEOG    261      01 10412\n278 GEOG    368      01 10415\n279 GEOG    372      01 10904\n280 GEOG    394      02 10417\n281 GEOG    477      01 10419\n282 GEOL    101      F1 10851\n283 GEOL    105      01 10422\n284 GEOL    160      01 10424\n285 GEOL    165      F1 10430\n286 GEOL    250      01 10432\n287 GEOL    260      01 10434\n288 GEOL    300      01 10436\n289 GEOL    304      01 10438\n290 GERM    101      01 10440\n291 GERM    110      01 10443\n292 GERM    194      F1 10446\n293 GERM    203      01 10447\n294 GERM    204      01 10450\n295 GERM    278      01 10458\n296 GERM    294      01 10453\n297 GERM    308      01 10455\n298 GERM    364      01 10462\n299 GERM    394      01 10898\n300 HIST    122      01 10237\n301 HIST    137      01 10463\n302 HIST    137      02 10975\n303 HIST    140      01 10464\n304 HIST    154      01 10918\n305 HIST    170      01 10466\n306 HIST    181      F1 10468\n307 HIST    194      01 10470\n308 HIST    194      F1 10472\n309 HIST    209      01 10473\n310 HIST    225      01 10475\n311 HIST    228      01 10477\n312 HIST    234      01 10347\n313 HIST    258      01 10479\n314 HIST    271      01 10480\n315 HIST    282      01 10484\n316 HIST    294      02 10489\n317 HIST    294      03 10868\n318 HIST    294      F1 10874\n319 HIST    367      01 10491\n320 HIST    490      01 10493\n321 HIST    490      02 10924\n322 INTL    110      F1 10497\n323 INTL    113      01 10498\n324 INTL    245      01 10499\n325 INTL    246      01 10501\n326 INTL    250      01 10728\n327 INTL    258      01 10502\n328 INTL    282      01 10500\n329 INTL    335      01 10505\n330 INTL    380      01 10504\n331 INTL    477      01 10421\n332 INTL    485      01 10507\n333 INTL    488      01 10508\n334 JAPA    101      01 10509\n335 JAPA    101      02 10510\n336 JAPA    203      01 10514\n337 JAPA    203      02 10515\n338 JAPA    254      F1 10519\n339 JAPA    282      01 10521\n340 JAPA    305      01 10524\n341 JAPA    394      01 10527\n342 JAPA    407      01 10530\n343 LATI    181      F1 10469\n344 LATI    239      F1 10404\n345 LATI    251      F1 10090\n346 LATI    282      01 10485\n347 LATI    307      01 10786\n348 LATI    308      01 10789\n349 LATI    331      01 10672\n350 LATI    342      01 10664\n351 LATI    387      01 10796\n352 LATI    394      01 10418\n353 LATI    394      02 10897\n354 LING    100      01 10531\n355 LING    104      01 10532\n356 LING    200      01 10533\n357 LING    205      01 10534\n358 LING    206      01 10535\n359 LING    220      01 10537\n360 LING    236      01 10717\n361 LING    282      01 10523\n362 LING    300      01 10539\n363 LING    309      01 10791\n364 LING    394      01 10529\n365 MATH    135      01 10540\n366 MATH    135      02 10541\n367 MATH    135      03 10542\n368 MATH    137      01 10543\n369 MATH    137      02 10544\n370 MATH    137      03 10545\n371 MATH    194      F1 10882\n372 MATH    236      01 10546\n373 MATH    236      02 10547\n374 MATH    237      01 10548\n375 MATH    237      02 10549\n376 MATH    279      01 10550\n377 MATH    279      02 10551\n378 MATH    312      01 10553\n379 MATH    354      01 10554\n380 MATH    377      01 10556\n381 MATH    379      01 10557\n382 MATH    432      01 10558\n383 MATH    437      01 10559\n384 MCST    110      01 10560\n385 MCST    114      01 10564\n386 MCST    128      01 10565\n387 MCST    194      01 10562\n388 MCST    194      F1 10561\n389 MCST    248      01 10566\n390 MCST    258      01 10503\n391 MCST    278      01 10459\n392 MCST    284      01 10079\n393 MCST    294      01 10567\n394 MCST    394      01 10568\n395 MUSI    110      01 10570\n396 MUSI    113      01 10571\n397 MUSI    153      01 10573\n398 MUSI    180      F1 10572\n399 MUSI    220      01 10538\n400 MUSI    225      F1 10574\n401 MUSI    342      01 10576\n402 MUSI    394      01 10578\n403 NSCI    488      01 10852\n404 PHIL    100      01 10606\n405 PHIL    100      02 10607\n406 PHIL    100      F1 10605\n407 PHIL    111      01 10608\n408 PHIL    111      02 10609\n409 PHIL    121      01 10611\n410 PHIL    121      F1 10610\n411 PHIL    213      01 10612\n412 PHIL    310      01 10613\n413 PHIL    321      01 10614\n414 PHIL    489      01 10616\n415 PHIL    489      02 10617\n416 PHYS    126      01 10622\n417 PHYS    130      01 10619\n418 PHYS    194      F1 10621\n419 PHYS    226      01 10625\n420 PHYS    331      01 10631\n421 PHYS    443      01 10634\n422 PHYS    481      01 10636\n423 PHYS    489      01 10638\n424 POLI    100      01 10639\n425 POLI    120      01 10641\n426 POLI    120      F1 10640\n427 POLI    140      01 10642\n428 POLI    160      01 10643\n429 POLI    203      01 10645\n430 POLI    205      01 10644\n431 POLI    207      F1 10647\n432 POLI    215      01 10932\n433 POLI    221      01 10648\n434 POLI    242      01 10649\n435 POLI    252      F1 10856\n436 POLI    261      01 10650\n437 POLI    268      01 10615\n438 POLI    269      01 10652\n439 POLI    278      01 10460\n440 POLI    294      01 10653\n441 POLI    294      03 10656\n442 POLI    294      04 10657\n443 POLI    294      05 10658\n444 POLI    315      01 10659\n445 POLI    324      01 10661\n446 POLI    342      01 10663\n447 POLI    400      01 10665\n448 POLI    400      02 10666\n449 POLI    400      03 10667\n450 POLI    404      01 10668\n451 PORT    111      01 10669\n452 PORT    111      02 10670\n453 PORT    331      01 10671\n454 PSYC    100      01 10674\n455 PSYC    100      02 10675\n456 PSYC    194      F1 10673\n457 PSYC    201      01 10680\n458 PSYC    220      01 10310\n459 PSYC    220      02 10977\n460 PSYC    242      01 10684\n461 PSYC    244      01 10686\n462 PSYC    248      01 10688\n463 PSYC    250      01 10690\n464 PSYC    254      01 10691\n465 PSYC    258      01 10692\n466 PSYC    264      01 10693\n467 PSYC    270      01 10354\n468 PSYC    294      01 10359\n469 PSYC    301      01 10695\n470 PSYC    301      02 10696\n471 PSYC    342      01 10697\n472 PSYC    377      01 10698\n473 PSYC    394      02 10701\n474 PSYC    394      03 10869\n475 PSYC    394      04 10922\n476 PSYC    394      05 11027\n477 PSYC    401      01 10702\n478 PSYC    401      02 10703\n479 PSYC    401      03 11063\n480 PSYC    402      01 10704\n481 RELI    100      01 10705\n482 RELI    110      01 10706\n483 RELI    111      01 10707\n484 RELI    120      01 10709\n485 RELI    121      01 10710\n486 RELI    121      02 11003\n487 RELI    136      01 10711\n488 RELI    226      01 10712\n489 RELI    235      01 10713\n490 RELI    236      01 10714\n491 RELI    278      01 10461\n492 RELI    294      01 10718\n493 RUSS    101      01 10721\n494 RUSS    203      01 10724\n495 RUSS    250      01 10727\n496 RUSS    294      01 10729\n497 RUSS    294      F1 10875\n498 RUSS    394      01 10907\n499 SOCI    110      01 10730\n500 SOCI    150      01 10732\n501 SOCI    150      F1 10731\n502 SOCI    194      01 10733\n503 SOCI    200      01 10885\n504 SOCI    270      01 10735\n505 SOCI    283      01 10736\n506 SOCI    294      02 10738\n507 SOCI    294      03 10870\n508 SOCI    480      01 10740\n509 SPAN    101      01 10741\n510 SPAN    101      02 10742\n511 SPAN    102      01 10748\n512 SPAN    102      02 10749\n513 SPAN    110      01 10755\n514 SPAN    203      01 10756\n515 SPAN    203      02 10757\n516 SPAN    203      03 10758\n517 SPAN    203      04 10759\n518 SPAN    204      01 10769\n519 SPAN    204      02 10770\n520 SPAN    204      03 10771\n521 SPAN    305      01 10780\n522 SPAN    305      02 10781\n523 SPAN    305      03 10782\n524 SPAN    305      04 10783\n525 SPAN    305      F1 10779\n526 SPAN    306      01 10784\n527 SPAN    307      01 10785\n528 SPAN    308      01 10787\n529 SPAN    309      01 10790\n530 SPAN    334      01 10792\n531 SPAN    382      01 10793\n532 SPAN    387      01 10794\n533 SPAN    394      01 10797\n534 SPAN    394      02 10798\n535 STAT    112      01 10249\n536 STAT    112      02 10251\n537 STAT    112      03 10253\n538 STAT    125      01 10799\n539 STAT    155      01 10800\n540 STAT    155      02 10801\n541 STAT    155      03 10802\n542 STAT    155      04 10803\n543 STAT    155      05 10804\n544 STAT    155      06 10805\n545 STAT    212      01 10268\n546 STAT    212      02 10270\n547 STAT    253      01 10806\n548 STAT    253      02 10807\n549 STAT    253      03 10808\n550 STAT    354      01 10555\n551 STAT    452      01 10809\n552 STAT    452      02 10810\n553 STAT    456      01 10280\n554 THDA    105      F1 10811\n555 THDA    105      F2 10812\n556 THDA    112      01 10813\n557 THDA    120      01 10814\n558 THDA    120      02 10815\n559 THDA    125      01 10816\n560 THDA    194      01 10818\n561 THDA    235      01 10819\n562 THDA    242      01 10820\n563 THDA    242      02 10822\n564 THDA    245      01 10824\n565 THDA    292      01 10867\n566 THDA    294      01 10825\n567 THDA    294      02 10826\n568 THDA    350      01 10829\n569 THDA    394      01 10830\n570 THDA    490      01 10854\n571 WGSS    100      F1 10840\n572 WGSS    170      01 10467\n573 WGSS    194      01 11025\n574 WGSS    205      01 10841\n575 WGSS    220      01 10843\n576 WGSS    228      01 10478\n577 WGSS    252      01 10107\n578 WGSS    258      01 10224\n579 WGSS    261      01 10651\n580 WGSS    264      01 10694\n581 WGSS    294      01 10845\n582 WGSS    300      01 10847\n583 WGSS    324      01 10662\n                                                                                                   name\n1                                                           Introduction to African American Literature\n2                                                                Introduction to Asian American Studies\n3                                                   What’s After White Empire - And Is It Already Here?\n4                                                   Politics and Inequality: The American Welfare State\n5                                                                           Trans Theories and Politics\n6                                                                     Civil Rights in the United States\n7                                                                                Native History to 1871\n8                                                                                 Environmental Justice\n9                                                             Race, Culture, and Ethnicity in Education\n10                                                                           Black Public Intellectuals\n11                                                       Uses and Abuses: Drugs, Addiction and Recovery\n12                                                                  African American Literature to 1900\n13                                           Radical Reelism: Indigeneity, Politics, and Visual Culture\n14                                                                             Envi Justice in Practice\n15                                                                            Critical Race Studies 101\n16                                                                               Representing Malcolm X\n17                                           Endarkened Feminist Epistemologies: A Black Feminist Ethic\n18                                                                  Introduction to U.S. Latinx Studies\n19                                        Topics in African American Literature: Black Feminist Thought\n20                                                                                Latinx in the Midwest\n21                                Building Ethical Partnerships: Junior Seminar in Community Engagement\n22                                                                                       Senior Seminar\n23                                                                                 General Anthropology\n24                                                                                Cultural Anthropology\n25                                                                                Cultural Anthropology\n26                                                                              Biological Anthropology\n27                                                                        Endangered/Minority Languages\n28                                                                            Ethnographic Interviewing\n29                                                                                 Medical Anthropology\n30                                                                   Human Osteology and Paleopathology\n31                                                                        Defense Against the Dark Arts\n32                                                                  Politics of Memory in Latin America\n33                                                                                 Anthropology of Work\n34                                                                                    Global Generosity\n35                                                                          Anthropology of Development\n36                                                                               Theory in Anthropology\n37                                                                         Advanced Archaeology Seminar\n38                                                                                            Drawing I\n39                                                                                            Drawing I\n40                                                                       Introduction to Visual Culture\n41                                Introduction to Art History I: From Prehistory to the Medieval Period\n42                                                             Centering: Introduction to Ceramics Arts\n43                                                                                        Photography I\n44                                                                                           Painting I\n45                                                                                          Sculpture I\n46                                                                                        Printmaking I\n47                                                                                  Unconventional Clay\n48                                                                                           2-D Design\n49                                                     Gender, Sexualities, and Feminist Visual Culture\n50                                                                          Contemporary Art and Theory\n51                                                                                           3-D Design\n52                                                                                         Art of China\n53                                                                Art and Architecture of Ancient Egypt\n54                                                                                    The Graphic Novel\n55                                                                                          Painting II\n56                                                                                       Printmaking II\n57                                        Art and Iconoclasm from the Ancient World to Early Modern Era\n58                                                                        Photography: Poetics of Space\n59                                                                             Introduction to Buddhism\n60                                                              Introduction to East Asian Civilization\n61                                                               Introduction to Asian American Studies\n62                                                                         Intro to South Asian History\n63                                                             Sanskrit and Classical Religion in India\n64                                         Japanese Film and Animation: From the Salaryman to the Shojo\n65                                                                        Gender and Sexuality in China\n66                                                                                         Art of China\n67                             Language and Identity in Japanese, Asian American, and Other Communities\n68                                                                        Contemporary India Since 1945\n69                                                                   Japanese Sociocultural Linguistics\n70                                                                          Ecology and the Environment\n71                                                                          Ecology and the Environment\n72                                                                          Ecology and the Environment\n73                                                                           Biodiversity and Evolution\n74                                                                                             Genetics\n75                                                                                             Genetics\n76                                                                                         Cell Biology\n77                                                                                         Cell Biology\n78                                       Outdoor Environmental Education in Theory, Policy and Practice\n79                                                      Sustainable Cities: Urban Environmental Science\n80                                                                        Invertebrate Animal Diversity\n81                                                                                         Neuroanatomy\n82                                                                                         Microbiology\n83                                                                          Cell/Molecular Neuroscience\n84                                                                                       Biochemistry I\n85                                                                                           Immunology\n86                                                                                  Plant Ecophysiology\n87                                                                                Developmental Biology\n88                                                       Animal Behavior: Fundamentals and Applications\n89                                                                                 Mechanisms of Memory\n90                                                                            Seminar in Genome Editing\n91                                                                            Seminar in Cancer Biology\n92                                                                             Research in Neurobiology\n93                                                       General Chemistry I: Structure and Equilibrium\n94                                                       General Chemistry I: Structure and Equilibrium\n95                                                       General Chemistry I: Structure and Equilibrium\n96                                                       General Chemistry I: Structure and Equilibrium\n97                                                       General Chemistry I: Structure and Equilibrium\n98                                                                        Accelerated General Chemistry\n99                                                                                  Organic Chemistry I\n100                                                                                 Organic Chemistry I\n101                                                                                 Organic Chemistry I\n102                                                                                   Chemistry Seminar\n103                                                                         Thermodynamics and Kinetics\n104                                                                                      Biochemistry I\n105                                                                            Scientific Communication\n106                                                                        Advanced Inorganic Chemistry\n107                                                                                First Year Chinese I\n108                                                                                First Year Chinese I\n109                                                                               Second Year Chinese I\n110                                                                               Second Year Chinese I\n111                                                                       Gender and Sexuality in China\n112                                                                                Third Year Chinese I\n113                                                                               Fourth Year Chinese I\n114                                                                                  Elementary Latin I\n115                                                                                 Elementary Arabic I\n116                                                                                  Elementary Greek I\n117                                                                                     The Roman World\n118                                                                                         Greek Myths\n119                                                                                      Ancient Comedy\n120                                                            Sanskrit and Classical Religion in India\n121                                                                        Ancient Healing and Medicine\n122                                                                         Intermediate Latin I: Prose\n123                                                                               Intermediate Hebrew I\n124                                                                               Intermediate Arabic I\n125                               Introduction to Art History I: From Prehistory to the Medieval Period\n126                                                               Art and Architecture of Ancient Egypt\n127                                                                        Advanced Archaeology Seminar\n128                                                                                     Advanced Arabic\n129                                                                                      Senior Seminar\n130                                                                        Introduction to Data Science\n131                                                                        Introduction to Data Science\n132                                                                        Introduction to Data Science\n133                                                                   Core Concepts in Computer Science\n134                                                                   Core Concepts in Computer Science\n135                                                                   Core Concepts in Computer Science\n136                                                                   Core Concepts in Computer Science\n137                                                         Object-Oriented Programming and Abstraction\n138                                                         Object-Oriented Programming and Abstraction\n139                                                         Object-Oriented Programming and Abstraction\n140                                                                                     Data Structures\n141                                                                                     Data Structures\n142                                                              Introduction to Data in the Humanities\n143                                                                          Functional Problem Solving\n144                                                                           Intermediate Data Science\n145                                                                           Intermediate Data Science\n146                                                                       Algorithm Design and Analysis\n147                                                                     Software Design and Development\n148                                                                     Software Design and Development\n149                                                                                    Computer Systems\n150                                                                                    Computer Systems\n151                                                                             Advanced Remote Sensing\n152                                                                                  Data Visualization\n153                                                                 Parallel and Distributed Processing\n154                                                                                  Internet Computing\n155                                                                            Projects in Data Science\n156                                                             Introduction to Artificial Intelligence\n157                                                      Topics in Applied Math: Computational Geometry\n158                                                                                Financial Accounting\n159                                                                                Financial Accounting\n160                                                                               Business Negotiations\n161                                                                             Principles of Economics\n162                                                                             Principles of Economics\n163                                                                             Principles of Economics\n164                                                                             Principles of Economics\n165                                                                             Principles of Economics\n166                                                                             Principles of Economics\n167                                                              Calculus-based Principles of Economics\n168                                                             Introduction to International Economics\n169                                                                              World Economic History\n170                                                                    Introduction to Entrepreneurship\n171                                                                        Economics of the Twin Cities\n172                                                                                  Working in America\n173                                                                                  Working in America\n174                                                                 Intermediate Microeconomic Analysis\n175                                                                 Intermediate Microeconomic Analysis\n176                                                                 Intermediate Macroeconomic Analysis\n177                                                                        Introduction to Econometrics\n178                                                                        Introduction to Econometrics\n179                                                                  International Economic Development\n180                                                         International Economic Development Capstone\n181                                                                                      Honors Seminar\n182                                                                                             Finance\n183                                                                         Open Economy Macroeconomics\n184                                                                Open Economy Macroeconomics Capstone\n185                        We Demand: Student Power, World Building, and Democratizing Higher Education\n186                                                                              Educational Psychology\n187                                                                              Educational Psychology\n188                                                           Race, Culture, and Ethnicity in Education\n189                                                     Building Trust: Education in Global Perspective\n190                                      Outdoor Environmental Education in Theory, Policy and Practice\n191                                         Advanced Topics in Policy: US Education Politics and Policy\n192                                                                       Research Methods for Educ/Adv\n193                                                              Teaching and Learning in Urban Schools\n194                                                                          World Language Methodology\n195                                                                         Education and Social Change\n196                       Identities and Differences in U.S. Literature: LGBTQ2S+ Literature in America\n197                                                         Introduction to African American Literature\n198                                                                                         Shakespeare\n199                                                                                               Novel\n200                                                                    Introduction to Creative Writing\n201                                                                    Introduction to Creative Writing\n202                                                                    Introduction to Creative Writing\n203                                                                    Introduction to Creative Writing\n204                                                                    Introduction to Creative Writing\n205                                                                    Introduction to Creative Writing\n206                                                                                  Movie Medievalisms\n207                                                         Major British Authors: The Self and Society\n208                                                             Great Detectives and Plots of Detection\n209                                                                                 Literary Publishing\n210                                                                                    Musical Fictions\n211                                       19th Century British Literature: Food and Literary Sustenance\n212                                         Studies in Literature and the Natural World: Rural Outcasts\n213                                                                 African American Literature to 1900\n214                                   Crafts of Writing: Poetry - Forms of Attention, Attention to Form\n215                                                                    Crafts of Writing: Screenwriting\n216                                                                                         Playwriting\n217                                                                                         Playwriting\n218                                                                                   The Graphic Novel\n219                                                                Writer’s Sketchbook: Space and Place\n220                                          Feminist Reconstructions: Utopias, Masculinities, and Race\n221                                       Topics in African American Literature: Black Feminist Thought\n222                                                               Disability in the English Renaissance\n223                                                                                         Demonologie\n224                        Special Topics in Literary Studies: Picture This! Literature and Photography\n225                                                             Projects in Creative Writing (Capstone)\n226                                                                         Science of Renewable Energy\n227                                                                                 Climate and Society\n228                                                                         Dynamic Earth/Global Change\n229                                                                         Ecology and the Environment\n230                                                                         Ecology and the Environment\n231                                                                         Ecology and the Environment\n232                                                                       Sustainability and the Campus\n233                                                                       Introduction to Urban Ecology\n234                                                                       Environmental Politics/Policy\n235                                                             People, Agriculture and the Environment\n236                                                                          U.S. Environmental History\n237                                                                               Environmental Justice\n238                                                                          The Earth’s Climate System\n239                                                                                     Water and Power\n240                                         Studies in Literature and the Natural World: Rural Outcasts\n241                                                                  Psychology of Sustainable Behavior\n242                                      Outdoor Environmental Education in Theory, Policy and Practice\n243                                                     Sustainable Cities: Urban Environmental Science\n244                                                                              Environmental Classics\n245                                                                   Environmental Justice in Practice\n246                                                                    Psychology and/of Climate Change\n247                                                      Critical Ecologies: Theory in the Anthropocene\n248                                                                                 Plant Ecophysiology\n249                                                      Animal Behavior: Fundamentals and Applications\n250                                                                       Climate Negotiations at COP28\n251                                                             Comparative Environment and Development\n252                                                                      Environmental Leadership Pract\n253                                                                          Envi St Leadership Seminar\n254                                                                                            French I\n255                                                                                            French I\n256                                                                             Accelerated French I-II\n257                                                                                        18th Century\n258                                                                                          French III\n259                                                                                          French III\n260                                                                                Text, Film and Media\n261                                                            Advanced Expression: Communication Tools\n262                                                                   Introduction to Literary Analysis\n263                                                                       Introduction to French Cinema\n264                                                               French Intellectuals in/And the World\n265                                                                          World Language Methodology\n266                                        Thinking Geographically: The Fundamentals of Human Geography\n267                                                                       Introduction to Urban Ecology\n268                                                      Introduction to Geographic Information Systems\n269                                                             People, Agriculture and the Environment\n270                                                                              Neotropical Landscapes\n271                                                                                     Urban Geography\n272                                                             Regional Geography of the US and Canada\n273                                                             Regional Geography of the US and Canada\n274                                                  The Political Geography of Nations and Nationalism\n275                                                                                     Water and Power\n276                                                                                    Health Geography\n277                                                                     Geography of World Urbanization\n278                                                                                          Health GIS\n279                                                                             Advanced Remote Sensing\n280                                                                      Public Health in Latin America\n281                                                             Comparative Environment and Development\n282                                                                                           Dinosaurs\n283                                                                                          Geohazards\n284                                                                         Dynamic Earth/Global Change\n285                                                                          History/Evolution of Earth\n286                                                                                          Mineralogy\n287                                                                                       Geomorphology\n288                                                                                        Paleobiology\n289                                                                                           Tectonics\n290                                                                                 Elementary German I\n291                                                                       Accelerated Elementary German\n292                                                                              Our Cyborgs, Ourselves\n293                                                                               Intermediate German I\n294                                                                              Intermediate German II\n295                                                               Marx, Religion, and Biopolitical Race\n296                                                      Critical Ecologies: Theory in the Anthropocene\n297                                             German Cultural History I: Uniting and Dividing Germany\n298                                                                             Migration, Then and Now\n299                                                                          World Language Methodology\n300                                                                                     The Roman World\n301                        From Confederation to Confederacy: US History from Independence to Civil War\n302                        From Confederation to Confederacy: US History from Independence to Civil War\n303                                                             Introduction to East Asian Civilization\n304                                                                              African Life Histories\n305                                                                                History of Childhood\n306                                                                       Introduction to Latin America\n307                                                                        Intro to South Asian History\n308                                                                                     Wakanda Forever\n309                                                                   Civil Rights in the United States\n310                                                                              Native History to 1871\n311                                        The Law, Economy, and Family in the Anglo-American Tradition\n312                                                                          U.S. Environmental History\n313                                                                                      Postwar Europe\n314                                                      Uses and Abuses: Drugs, Addiction and Recovery\n315                                                                       Latin America: Art and Nation\n316                                                                       Contemporary India Since 1945\n317                                             The Caucasus: From Ancient Histories to Modern Problems\n318                                                                             Between Europe and Asia\n319                                                                                       The Holocaust\n320                                                                                      Senior Seminar\n321                                                                                      Senior Seminar\n322                         Introduction to Intl Studies: Globalization - Homogeneity and Heterogeneity\n323                         Intro to International Studies: Border-crossing in the Age of Globalization\n324                                                                          Intro to Intl Human Rights\n325                                                    Global Adolescent Sexual and Reproductive Health\n326                                                     Terrorism and Art: The Spectacle of Destruction\n327                                                                        The Middle East Through Film\n328                                                         Introduction to International Public Health\n329                                                                                   Global Generosity\n330                                                                                   Global Leadership\n331                                                             Comparative Environment and Development\n332                                                           Senior Seminar: Confronting Global Hatred\n333                                                           Senior Seminar: Thinking on a World Scale\n334                                                                               First Year Japanese I\n335                                                                               First Year Japanese I\n336                                                                              Second Year Japanese I\n337                                                                              Second Year Japanese I\n338                                        Japanese Film and Animation: From the Salaryman to the Shojo\n339                            Language and Identity in Japanese, Asian American, and Other Communities\n340                                                                               Third Year Japanese I\n341                                                                  Japanese Sociocultural Linguistics\n342                                                                              Fourth Year Japanese I\n343                                                                       Introduction to Latin America\n344                                                                              Neotropical Landscapes\n345                                                                 Politics of Memory in Latin America\n346                                                                       Latin America: Art and Nation\n347                                                      Introduction to the Analysis of Hispanic Texts\n348                                                                 Introduction to U.S. Latinx Studies\n349                                                                             Journeys Through Brazil\n350                                                                     Urban Politics of Latin America\n351                                                                               Latinx in the Midwest\n352                                                                      Public Health in Latin America\n353                       Narratives Against Extermination: Cultural Memory in the Amazon and Patagonia\n354                                                                         Introduction to Linguistics\n355                                                                     Sounds of the World’s Languages\n356                                                                                              Syntax\n357                                                                                           Phonology\n358                                                                       Endangered/Minority Languages\n359                                                                                  Language and Music\n360                                                            Sanskrit and Classical Religion in India\n361                            Language and Identity in Japanese, Asian American, and Other Communities\n362                                                                                 Linguistic Analysis\n363                                                                Introduction to Hispanic Linguistics\n364                                                                  Japanese Sociocultural Linguistics\n365                                                                    Applied Multivariable Calculus I\n366                                                                    Applied Multivariable Calculus I\n367                                                                    Applied Multivariable Calculus I\n368                                                                   Applied Multivariable Calculus II\n369                                                                   Applied Multivariable Calculus II\n370                                                                   Applied Multivariable Calculus II\n371                                                                 Calculus: A Biomedical Introduction\n372                                                                                      Linear Algebra\n373                                                                                      Linear Algebra\n374                                                                  Applied Multivariable Calculus III\n375                                                                  Applied Multivariable Calculus III\n376                                                                                Discrete Mathematics\n377                                                                                Discrete Mathematics\n378                                                                              Differential Equations\n379                                                                                         Probability\n380                                                                                       Real Analysis\n381                                                                                       Combinatorics\n382                                                                               Mathematical Modeling\n383                                                      Topics in Applied Math: Computational Geometry\n384                                          Texts and Power: Foundations of Media and Cultural Studies\n385                                                                          News Reporting and Writing\n386                                                                        Film Analysis/Visual Culture\n387                                                              Introduction to Data in the Humanities\n388                                                             Documentary Cinema: Theory and Practice\n389                                                                           History of Film 1893-1941\n390                                                                        The Middle East Through Film\n391                                                               Marx, Religion, and Biopolitical Race\n392                                          Radical Reelism: Indigeneity, Politics, and Visual Culture\n393                                                      The Horror Film: History, Politics, and Theory\n394                                                  The Transnational Character of Alternative Culture\n395                                                             Introduction to Western Classical Music\n396                                                                                        Musicianship\n397                                                                                    Electronic Music\n398                                                                          Music, Race, and Ethnicity\n399                                                                                  Language and Music\n400                                                                                    Musical Fictions\n401                                    Sacred, Secular, Sublime: Music and Meaning in Europe, 1300-1800\n402                                                                       Music and the Meaning of Life\n403                                                                               Neuroscience Capstone\n404                                                                          Introduction to Philosophy\n405                                                                          Introduction to Philosophy\n406                                                     Introduction to Philosophy: Love and Friendship\n407                                                                      Introduction to Symbolic Logic\n408                                                                      Introduction to Symbolic Logic\n409                                                                              Introduction to Ethics\n410                                                                              Introduction to Ethics\n411                                                                                  Philosophy of Mind\n412                                                                               Philosophy of Science\n413                                                        Contemporary Social and Political Philosophy\n414                                                                                      Senior Seminar\n415                                                                                      Senior Seminar\n416                                                                              Introductory Physics I\n417                                                                         Science of Renewable Energy\n418                                                                         Our Solar System and Beyond\n419                                                                             Principles of Physics I\n420                                                                                      Modern Physics\n421                                                                              Electromagnetic Theory\n422                                                                                   Quantum Mechanics\n423                                                                                     Physics Seminar\n424                                                                          Foundations of US Politics\n425                         Foundations of International Politics: Western and Non-Western Perspectives\n426                         Foundations of International Politics: Western and Non-Western Perspectives\n427                                                                 Foundations of Comparative Politics\n428                                             Foundations of Political Theory: Decolonizing the Canon\n429                                                 Politics and Inequality: The American Welfare State\n430                                                                        US Politics and Policymaking\n431                                                                 US Civil Rights and Civil Liberties\n432                                                                       Environmental Politics/Policy\n433                                                                                   Global Governance\n434                                                                    Political Economy of Development\n435                                                                                     Water and Power\n436                                                                           Feminist Political Theory\n437                                                        Contemporary Social and Political Philosophy\n438                                                                          Empirical Research Methods\n439                                                               Marx, Religion, and Biopolitical Race\n440                                                     A New Cold War? US-China Relations in the 2020s\n441                                                  Modern Political Theory (Machiavelli to Nietzsche)\n442                                                                     Comics and US Political Culture\n443                                        When Justice Goes Blind: The Problem of Wrongful Convictions\n444                                         Advanced Topics in Policy: US Education Politics and Policy\n445                                                                           Women, Peace and Security\n446                                                                     Urban Politics of Latin America\n447                                                             Senior Research Seminar: World Politics\n448                                                                             Senior Research Seminar\n449                                                                             Senior Research Seminar\n450                                                                                   Honors Colloquium\n451                                                                    Accelerated Beginning Portuguese\n452                                                                    Accelerated Beginning Portuguese\n453                                                                             Journeys through Brazil\n454                                                                          Introduction to Psychology\n455                                                                          Introduction to Psychology\n456                                                                                   Applied Cognition\n457                                                                            Research in Psychology I\n458                                                                              Educational Psychology\n459                                                                              Educational Psychology\n460                                                                                Cognitive Psychology\n461                                                                              Cognitive Neuroscience\n462                                                                             Behavioral Neuroscience\n463                                                                            Developmental Psychology\n464                                                                                   Social Psychology\n465                                                                Industrial/Organizational Psychology\n466                                                                            The Psychology of Gender\n467                                                                  Psychology of Sustainable Behavior\n468                                                                    Psychology and/of Climate Change\n469                                                                           Research in Psychology II\n470                                                                           Research in Psychology II\n471                                                                                        Intelligence\n472                                                                                    Moral Psychology\n473                                                                   Indigenous Healing and Well-Being\n474                                                              Neuropsychology of Injury and Recovery\n475                                                                          Trauma/Individuals/Society\n476                                        Applying Developmental Psychology to 21st Century Challenges\n477                                                                     Directed Research in Psychology\n478                                                                     Directed Research in Psychology\n479                                                                     Directed Research in Psychology\n480                                                                           Clinical Science Capstone\n481                                                      Introduction to Islam: Formation and Expansion\n482                                                                                   The Big Questions\n483                                                                            Introduction to Buddhism\n484                                                                                    The Jewish Bible\n485                                         Jesus, Peter, Paul and Mary: The Beginnings of Christianity\n486                                         Jesus, Peter, Paul and Mary: The Beginnings of Christianity\n487                                                       World Religions and World Religions Discourse\n488                                                                              Martyrdom Then and Now\n489                                                                                 Theorizing Religion\n490                                                            Sanskrit and Classical Religion in India\n491                                                               Marx, Religion, and Biopolitical Race\n492                                                                              Representing Malcolm X\n493                                                                                Elementary Russian I\n494                                                                              Intermediate Russian I\n495                                                     Terrorism and Art: The Spectacle of Destruction\n496                                             The Caucasus: From Ancient Histories to Modern Problems\n497                                                                             Between Europe and Asia\n498                                                                          World Language Methodology\n499                                                                           Introduction to Sociology\n500                                                 Prius or Pickup? Political Divides and Social Class\n501                                                 Prius or Pickup? Political Divides and Social Class\n502                                                Children and Childhood in Times of Change and Crisis\n503                                                   The Old Order is Dying and the New Cannot Be Born\n504                                                                        Interpretive Social Research\n505                                                                                  Economic Sociology\n506                                                                                     Suburbanization\n507                                         Unequal Budgets: Taxing, Spending, and States of Inequality\n508                                                                                      Senior Seminar\n509                                                                                Elementary Spanish I\n510                                                                                Elementary Spanish I\n511                                                                               Elementary Spanish II\n512                                                                               Elementary Spanish II\n513                                                                       Accelerated Beginning Spanish\n514                                                                              Intermediate Spanish I\n515                                                                              Intermediate Spanish I\n516                                                                              Intermediate Spanish I\n517                                                                              Intermediate Spanish I\n518                                                                             Intermediate Spanish II\n519                                                                             Intermediate Spanish II\n520                                                                             Intermediate Spanish II\n521                                                                Advanced Oral and Written Expression\n522                                                                Advanced Oral and Written Expression\n523                                                                Advanced Oral and Written Expression\n524                                                                Advanced Oral and Written Expression\n525                        Advanced Oral and Written Expression: Exploring Spanish in the United States\n526                                                                       Spanish for Heritage Speakers\n527                                                      Introduction to the Analysis of Hispanic Texts\n528                                                                 Introduction to U.S. Latinx Studies\n529                                                                Introduction to Hispanic Linguistics\n530                                                                            Spanish in the Workplace\n531                                                                             Constructing the Nation\n532                                                                               Latinx in the Midwest\n533                       Narratives Against Extermination: Cultural Memory in the Amazon and Patagonia\n534 The Veils of Spain: Veiled Characters and the Use of Veils in Medieval/Early Modern Iberian Culture\n535                                                                        Introduction to Data Science\n536                                                                        Introduction to Data Science\n537                                                                        Introduction to Data Science\n538                                                                                        Epidemiology\n539                                                                Introduction to Statistical Modeling\n540                                                                Introduction to Statistical Modeling\n541                                                                Introduction to Statistical Modeling\n542                                                                Introduction to Statistical Modeling\n543                                                                Introduction to Statistical Modeling\n544                                                                Introduction to Statistical Modeling\n545                                                                           Intermediate Data Science\n546                                                                           Intermediate Data Science\n547                                                                        Statistical Machine Learning\n548                                                                        Statistical Machine Learning\n549                                                                        Statistical Machine Learning\n550                                                                                         Probability\n551                                                                                     Correlated Data\n552                                                                                     Correlated Data\n553                                                                            Projects in Data Science\n554                                                               Seeing Performance in the Twin Cities\n555                                                               Seeing Performance in the Twin Cities\n556                                                               Reading Plays: Indigenous Playwrights\n557                                                                                              Acting\n558                                                                                              Acting\n559                                                                         Technologies of Performance\n560                                                                       Drawing for Theater and Dance\n561                                                                         Fundamentals of Scenography\n562                                                                                         Playwriting\n563                                                                                         Playwriting\n564                  Performance Histories and Theories: From the Historical Avant-Garde to the Present\n565                                                                                    Stage Management\n566                                                                         History Fashion and Costume\n567                                                                                        Sound Design\n568                                                                             Directing for the Stage\n569                                                                                  Lighting Design II\n570                                                                         Capstone and Honors Seminar\n571    Intro to Women’s/Gender/Sexuality St: Intersectional Queer Approaches to Pandemics and Uprisings\n572                                                                                History of Childhood\n573                       Identities and Differences in U.S. Literature: LGBTQ2S+ Literature in America\n574                                                                         Trans Theories and Politics\n575                                          Feminist Reconstructions: Utopias, Masculinities, and Race\n576                                         The Law, Economy and Family in the Anglo-American Tradition\n577                                                     Gender, Sexualities and Feminist Visual Culture\n578                                                                       Gender and Sexuality in China\n579                                                                           Feminist Political Theory\n580                                                                            The Psychology of Gender\n581                                          Endarkened Feminist Epistemologies: A Black Feminist Ethic\n582                                           Worlds Upside Down: Revolutions in Theories and Practices\n583                                                                           Women, Peace and Security\n     days              time       room                           instructor\n1   M W F   9:40 - 10:40 am   MAIN 009                     Daylanne English\n2   M W F    1:10 - 2:10 pm  MUSIC 219                        Jake Nagasawa\n3    T R     3:00 - 4:30 pm    HUM 214               Karin Aguilar-San Juan\n4   M W F   9:40 - 10:40 am   CARN 305                        Lesley Lavery\n5    T R     3:00 - 4:30 pm   MAIN 009                            Myrl Beam\n6     W     7:00 - 10:00 pm   MAIN 010                       Walter Greason\n7    T R    9:40 - 11:10 am   MAIN 010                     Katrina Phillips\n8    T R    9:40 - 11:10 am THEATR 001                  Kirisitina Sailiata\n9    T R     1:20 - 2:50 pm  MUSIC 228                       Brian Lozenski\n10    W     7:00 - 10:00 pm    HUM 213                       Duchess Harris\n11  M       7:00 - 10:00 pm THEATR 204                         Amy Sullivan\n12  M W F   12:00 - 1:00 pm   MAIN 111                     Daylanne English\n13  M W      7:00 - 8:30 pm    HUM 215                  Kirisitina Sailiata\n14   T R     3:00 - 4:30 pm MARKIM 201     Samuel WegnerKirisitina Sailiata\n15  M W F    3:30 - 4:30 pm ARTCOM 202                        Jake Nagasawa\n16   T R     3:00 - 4:30 pm   MAIN 011                         William Hart\n17  M W F  10:50 - 11:50 am   MAIN 009                            Ebony Aya\n18  M W F    1:10 - 2:10 pm    HUM 212                         Alicia Muñoz\n19  M W F    2:20 - 3:20 pm   MAIN 111                     Daylanne English\n20  M W F  10:50 - 11:50 am    HUM 217                         Alicia Muñoz\n21  M       7:00 - 10:00 pm    HUM 217               Karin Aguilar-San Juan\n22   T R    9:40 - 11:10 am    HUM 217                       Duchess Harris\n23   T R    9:40 - 11:10 am   CARN 06A                       Jane Holmstrom\n24  M W F   9:40 - 10:40 am   CARN 06A                         Hilary Chart\n25  M W F    1:10 - 2:10 pm   CARN 06A                         Hilary Chart\n26  M W F  10:50 - 11:50 am   CARN 06B                       Jane Holmstrom\n27  M W F    2:20 - 3:20 pm   OLRI 301                    Marianne Milligan\n28  M W F   9:40 - 10:40 am    CARN 05                      Arjun Guneratne\n29   T R     1:20 - 2:50 pm   CARN 06A                          Ron Barrett\n30  M W F    1:10 - 2:10 pm   CARN 06B                       Jane Holmstrom\n31   T R     3:00 - 4:30 pm   CARN 06A                          Ron Barrett\n32   T R     3:00 - 4:30 pm   CARN 204                        Olga González\n33    W     7:00 - 10:00 pm    CARN 05                         Hilary Chart\n34  M W F    3:30 - 4:30 pm   CARN 404                    Jenna Rice Rahaim\n35  M W F    1:10 - 2:10 pm    CARN 05                      Arjun Guneratne\n36   T R    9:40 - 11:10 am THEATR 101                        Olga González\n37   T R    9:40 - 11:10 am   MAIN 011                       Andrew Overman\n38  M W      1:10 - 4:20 pm    ART 302                        Megan Vossler\n39  M W     8:30 - 11:40 am    ART 302                      Nicole Simpkins\n40  M W F   9:40 - 10:40 am ARTCOM 102               Kari Shepherdson-Scott\n41  M W F  10:50 - 11:50 am ARTCOM 102                        Serdar Yalçin\n42   T R    8:00 - 11:10 am    ART 113                 Summer Hills-Bonczyk\n43  M W F   9:40 - 11:40 am    ART 301                           Mara Duvra\n44   T R     1:20 - 4:30 pm    ART 308                        Chris Willcox\n45  M W F   9:40 - 11:40 am    ART 118                          Lela Pierce\n46   T R     1:20 - 4:30 pm    ART 214                     Ruthann Godollei\n47   T R     1:20 - 4:30 pm    ART 113                 Summer Hills-Bonczyk\n48  M W F    1:10 - 3:10 pm    ART 301                           Mara Duvra\n49   T R    9:40 - 11:10 am ARTCOM 102                        Joanna Inglot\n50   T R     3:00 - 4:30 pm ARTCOM 102                        Joanna Inglot\n51  M W F    1:10 - 3:10 pm    ART 118                          Lela Pierce\n52  M W F    1:10 - 2:10 pm ARTCOM 102               Kari Shepherdson-Scott\n53  M W F    2:20 - 3:20 pm ARTCOM 102                        Serdar Yalçin\n54   T R     1:20 - 2:50 pm ARTCOM 202            Matt BurgessMegan Vossler\n55   T R    8:00 - 11:10 am    ART 308                        Chris Willcox\n56   T R                       ART 214                     Ruthann Godollei\n57   T R     1:20 - 2:50 pm ARTCOM 102                        Serdar Yalçin\n58   T R    8:00 - 11:10 am    ART 301                           Mara Duvra\n59  M W F  10:50 - 11:50 am   MAIN 002                           Erik Davis\n60   T R    9:40 - 11:10 am   MAIN 002                         James Coplin\n61  M W F    1:10 - 2:10 pm  MUSIC 219                        Jake Nagasawa\n62  M W F   9:40 - 10:40 am   CARN 105                       Niharika Yadav\n63  M W F   9:40 - 10:40 am   MAIN 001                          James Laine\n64  M W F    1:10 - 2:10 pm    HUM 110                      Arthur Mitchell\n65  M W F    1:10 - 2:10 pm    HUM 111                             Xin Yang\n66  M W F    1:10 - 2:10 pm ARTCOM 102               Kari Shepherdson-Scott\n67   T R    9:40 - 11:10 am    HUM 110                        Satoko Suzuki\n68  M W F    3:30 - 4:30 pm   MAIN 001                       Niharika Yadav\n69   T R     3:00 - 4:30 pm    HUM 110                        Satoko Suzuki\n70  M W F   9:40 - 10:40 am THEATR 203                          Mary Heskel\n71  M W F    1:10 - 2:10 pm THEATR 202                   Stotra Chakrabarti\n72  M W F   12:00 - 1:00 pm THEATR 202                          Anika Bratt\n73  M W F   12:00 - 1:00 pm   OLRI 100                Kristina Curry Rogers\n74  M W F    1:10 - 2:10 pm THEATR 205                        Marc Pisansky\n75  M W F  10:50 - 11:50 am THEATR 206                 Robin Shields-Cutler\n76  M W F    8:30 - 9:30 am THEATR 201                       Phillip Rivera\n77  M W F   9:40 - 10:40 am THEATR 201                       Phillip Rivera\n78  M W      1:10 - 2:10 pm   OLRI 243                         Jerald Dosch\n79   T R     3:00 - 4:30 pm   OLRI 243                          Anika Bratt\n80  M W F   9:40 - 10:40 am   OLRI 270                          Sarah Boyer\n81  M W F   9:40 - 10:40 am   OLRI 370                     Elizabeth Jansen\n82  M W F   9:40 - 10:40 am ARTCOM 202                 Robin Shields-Cutler\n83  M W F  10:50 - 11:50 am   OLRI 301                        Marc Pisansky\n84  M W F    1:10 - 2:10 pm   OLRI 350                        Kathryn Splan\n85  M W F   9:40 - 10:40 am   OLRI 253                           Elena Tonc\n86  M W F  10:50 - 11:50 am   OLRI 284                          Mary Heskel\n87  M W F    1:10 - 2:10 pm   OLRI 273                      Mary Montgomery\n88  M W F  10:50 - 11:50 am   OLRI 250                   Stotra Chakrabarti\n89  M W      8:00 - 9:30 am   OLRI 270                        Michelle Tong\n90  M W F   12:00 - 1:00 pm   OLRI 270                      Mary Montgomery\n91  M W      8:00 - 9:30 am   OLRI 300                           Elena Tonc\n92    W     7:00 - 10:00 pm   OLRI 276                        Michelle Tong\n93  M W F    8:30 - 9:30 am   OLRI 350                         Paul Fischer\n94  M W F   12:00 - 1:00 pm THEATR 200                         Kelsey Boyle\n95  M W F    1:10 - 2:10 pm THEATR 200                         Kelsey Boyle\n96  M W F   9:40 - 10:40 am   OLRI 350                          Susan Green\n97  M W F  10:50 - 11:50 am   OLRI 350                          Susan Green\n98  M W F    1:10 - 2:10 pm   OLRI 301                         Keith Kuwata\n99  M W F    8:30 - 9:30 am   OLRI 100                      Ronald Brisbois\n100 M W F   9:40 - 10:40 am THEATR 205                           Dennis Cao\n101 M W F  10:50 - 11:50 am THEATR 205                           Dennis Cao\n102   W      3:30 - 4:30 pm   OLRI 350                       Thomas Varberg\n103 M W      8:00 - 9:30 am   OLRI 301                       Thomas Varberg\n104 M W F    1:10 - 2:10 pm   OLRI 350                        Kathryn Splan\n105 M       7:00 - 10:00 pm   OLRI 300                           Leah Witus\n106 M W F  10:50 - 11:50 am   OLRI 300                         Paul Fischer\n107 M W F   9:40 - 10:40 am    HUM 111                   Rivi Handler-Spitz\n108 M W F  10:50 - 11:50 am    HUM 111                   Rivi Handler-Spitz\n109 M W F   9:40 - 10:40 am    HUM 112                             Xin Yang\n110 M W F  10:50 - 11:50 am    HUM 112                             Xin Yang\n111 M W F    1:10 - 2:10 pm    HUM 111                             Xin Yang\n112 M W F   9:40 - 10:40 am    HUM 217                    Patricia Anderson\n113 M W F   9:40 - 10:40 am    HUM 404                            Jin Stone\n114 M W F   9:40 - 10:40 am   MAIN 111                    Beth Severy-Hoven\n115 M W F    1:10 - 2:10 pm   MAIN 010                         Kelly Tuttle\n116 M W F    2:20 - 3:20 pm   MAIN 009                      Nanette Goldman\n117  T R    9:40 - 11:10 am   MAIN 111                    Beth Severy-Hoven\n118 M W F    8:30 - 9:30 am   MAIN 010                          Fade Manley\n119 M W F  10:50 - 11:50 am    HUM 228                          Fade Manley\n120 M W F   9:40 - 10:40 am   MAIN 001                          James Laine\n121  T R     1:20 - 2:50 pm    HUM 217                       Andrew Overman\n122 M W F  10:50 - 11:50 am   MAIN 111                    Beth Severy-Hoven\n123 M W F   12:00 - 1:00 pm   MAIN 011                      Nanette Goldman\n124 M W F    2:20 - 3:20 pm   MAIN 010                         Kelly Tuttle\n125 M W F  10:50 - 11:50 am ARTCOM 102                        Serdar Yalçin\n126 M W F    2:20 - 3:20 pm ARTCOM 102                        Serdar Yalçin\n127  T R    9:40 - 11:10 am   MAIN 011                       Andrew Overman\n128 M W F  10:50 - 11:50 am   MAIN 003                         Kelly Tuttle\n129  T R     3:00 - 4:30 pm   MAIN 111                       Andrew Overman\n130  T R     3:00 - 4:30 pm   OLRI 254                    Brianna Heggeseth\n131  T R    9:40 - 11:10 am THEATR 205                        Amin Alhashim\n132  T R     1:20 - 2:50 pm THEATR 205                        Amin Alhashim\n133 M W F   9:40 - 10:40 am   OLRI 258                            Lian Duan\n134 M W F  10:50 - 11:50 am   OLRI 258                            Lian Duan\n135 M W F   12:00 - 1:00 pm   OLRI 258                      Elizabeth Ernst\n136 M W F    1:10 - 2:10 pm   OLRI 258                      Elizabeth Ernst\n137 M W F   9:40 - 10:40 am   OLRI 256                         Bret Jackson\n138 M W F    1:10 - 2:10 pm   OLRI 256                        Joslenne Peña\n139 M W F    2:20 - 3:20 pm   OLRI 256                            Lian Duan\n140 M W F    1:10 - 2:10 pm   OLRI 254                       Suhas Arehalli\n141 M W F    2:20 - 3:20 pm   OLRI 254                       Suhas Arehalli\n142 M W F   9:40 - 10:40 am   CARN 304                      Aisling Quigley\n143 M W F  10:50 - 11:50 am   OLRI 256                           Abby Marsh\n144  T R    9:40 - 11:10 am THEATR 201                         Leslie Myint\n145  T R     1:20 - 2:50 pm THEATR 204                         Leslie Myint\n146 M W F  10:50 - 11:50 am THEATR 201                         Lauren Milne\n147 M W F    1:10 - 2:10 pm THEATR 213                         Lauren Milne\n148 M W F    2:20 - 3:20 pm THEATR 213                         Lauren Milne\n149 M W F   9:40 - 10:40 am   OLRI 245                           Abby Marsh\n150 M W F    1:10 - 2:10 pm   OLRI 245                            Susan Fox\n151  T R    9:40 - 11:10 am   CARN 109                  Xavier Haro-Carrión\n152 M W F    2:20 - 3:20 pm   OLRI 245                         Bret Jackson\n153  T R     3:00 - 4:30 pm   OLRI 245                          Libby Shoop\n154 M W F  10:50 - 11:50 am   OLRI 245                        Joslenne Peña\n155  T R    9:40 - 11:10 am THEATR 202                           Shilad Sen\n156 M W F    3:30 - 4:30 pm   OLRI 258                            Susan Fox\n157  T R    9:40 - 11:10 am   OLRI 245                     Lori Ziegelmeier\n158  T R     8:00 - 9:30 am    HUM 314                       Bridgit Jordan\n159  T R    9:40 - 11:10 am    HUM 314                       Bridgit Jordan\n160  T R     1:20 - 2:50 pm   OLRI 250                          Joyce Minor\n161 M W F    2:20 - 3:20 pm   CARN 304                        Pete Ferderer\n162  T R    9:40 - 11:10 am ARTCOM 202                   Mario Solis-Garcia\n163  T R    9:40 - 11:10 am   CARN 305                           Sarah West\n164 M W F  10:50 - 11:50 am   CARN 305                         Gary Krueger\n165 M W F    1:10 - 2:10 pm   CARN 304                        Pete Ferderer\n166 M W F    1:10 - 2:10 pm   CARN 305                         Gary Krueger\n167 M W F  10:50 - 11:50 am THEATR 204                           Liang Ding\n168  T R     1:20 - 2:50 pm THEATR 002                         Felix Friedt\n169 M W F  10:50 - 11:50 am   CARN 06A                        Pete Ferderer\n170 M       7:00 - 10:00 pm   LIBR 250                         Ali Alizadeh\n171    R     1:20 - 4:30 pm THEATR 203                           Sarah West\n172 M W F    1:10 - 2:10 pm    HUM 400                      Elizabeth Engle\n173 M W F  10:50 - 11:50 am ARTCOM 202                      Elizabeth Engle\n174  T R    9:40 - 11:10 am THEATR 200                         Gabriel Lade\n175  T R     1:20 - 2:50 pm THEATR 200                         Gabriel Lade\n176 M W F  10:50 - 11:50 am THEATR 202                   Mario Solis-Garcia\n177  T R    9:40 - 11:10 am    HUM 212                         Felix Friedt\n178 M W F   9:40 - 10:40 am   CARN 309                         Gary Krueger\n179  T R     1:20 - 2:50 pm   CARN 305                            Amy Damon\n180  T R     1:20 - 2:50 pm   CARN 305                            Amy Damon\n181  T R     3:00 - 4:30 pm   CARN 305                            Amy Damon\n182 M W F    1:10 - 2:10 pm   CARN 206                           Liang Ding\n183 M W F   9:40 - 10:40 am THEATR 202                   Mario Solis-Garcia\n184 M W F   9:40 - 10:40 am THEATR 202                   Mario Solis-Garcia\n185 M W F 9:40 - 10:40 am37        MC                        Gonzalo Guzmán\n186  T R    9:40 - 11:10 am   OLRI 250                           Tina Kruse\n187  T R     3:00 - 4:30 pm THEATR 202                           Tina Kruse\n188  T R     1:20 - 2:50 pm  MUSIC 228                       Brian Lozenski\n189  T R     3:00 - 4:30 pm ARTCOM 202                          Sonia Mehta\n190 M W      1:10 - 2:10 pm   OLRI 243                         Jerald Dosch\n191  T R     3:00 - 4:30 pm   CARN 304                        Lesley Lavery\n192 M W F   12:00 - 1:00 pm ARTCOM 102                       Gonzalo Guzmán\n193 M       7:00 - 10:00 pm ARTCOM 102                       Brian Lozenski\n194 M W F    1:10 - 2:10 pm    HUM 314                      Claude Cassagne\n195 M W F    1:10 - 2:10 pm    HUM 215                       Gonzalo Guzmán\n196  T R    9:40 - 11:10 am    HUM 213                          Rachel Gold\n197 M W F   9:40 - 10:40 am   MAIN 009                     Daylanne English\n198 M W F  10:50 - 11:50 am   MAIN 001                        Penelope Geng\n199  T R     1:20 - 2:50 pm    HUM 212                          James Dawes\n200 M W F   9:40 - 10:40 am   MAIN 011                          James Dawes\n201  T R    9:40 - 11:10 am   MAIN 001                         Matt Burgess\n202  T R     1:20 - 2:50 pm    HUM 409                           Emma Törzs\n203 M W F  10:50 - 11:50 am   MAIN 011                    Cody Klippenstein\n204 M W F    1:10 - 2:10 pm   MAIN 011                    Cody Klippenstein\n205  T R     1:20 - 2:50 pm   MAIN 001                        Michael Prior\n206 M W F  10:50 - 11:50 am    HUM 213                        Coral Lumbley\n207 M W F    1:10 - 2:10 pm    HUM 112                        Coral Lumbley\n208 M W F   9:40 - 10:40 am    HUM 402                  Andrea Kaston Tange\n209 M W F    2:20 - 3:20 pm    HUM 216                      Steven Woodward\n210 M W F   12:00 - 1:00 pm  MUSIC 228                         Mark Mazullo\n211 M W F    3:30 - 4:30 pm   MAIN 009                  Andrea Kaston Tange\n212  T R     1:20 - 2:50 pm   MAIN 111                           Amy Elkins\n213 M W F   12:00 - 1:00 pm   MAIN 111                     Daylanne English\n214  T R     3:00 - 4:30 pm   MAIN 001                        Michael Prior\n215  T R    9:40 - 11:10 am    HUM 111                       Peter Bognanni\n216 M W F    3:30 - 4:30 pm THEATR 213                    Alayna Jacqueline\n217  T R     3:00 - 4:30 pm THEATR 213                   Cristina Luzárraga\n218  T R     1:20 - 2:50 pm ARTCOM 202            Megan VosslerMatt Burgess\n219  T R     3:00 - 4:30 pm    HUM 409                           Emma Törzs\n220  T R    9:40 - 11:10 am   MAIN 009                        Sonita Sarker\n221 M W F    2:20 - 3:20 pm   MAIN 111                     Daylanne English\n222 M W F    1:10 - 2:10 pm   MAIN 001                        Penelope Geng\n223 M W F    3:30 - 4:30 pm   MAIN 111                        Penelope Geng\n224 M       7:00 - 10:00 pm   MAIN 011                           Amy Elkins\n225  T R     1:20 - 2:50 pm   LIBR 250                       Peter Bognanni\n226 M W F    1:10 - 2:10 pm   OLRI 150                          James Doyle\n227 M W F   9:40 - 10:40 am   OLRI 243                   Louisa Bradtmiller\n228 M W F   9:40 - 10:40 am   OLRI 100                      Kelly MacGregor\n229 M W F   9:40 - 10:40 am THEATR 203                          Mary Heskel\n230 M W F    1:10 - 2:10 pm THEATR 202                   Stotra Chakrabarti\n231 M W F   12:00 - 1:00 pm THEATR 202                          Anika Bratt\n232   W      8:00 - 9:30 am   OLRI 243                         Megan Butler\n233  T R     1:20 - 2:50 pm   CARN 107               I-Chun Catherine Chang\n234 M       7:00 - 10:00 pm   CARN 305                         Eric Wojchik\n235  T R    9:40 - 11:10 am   LIBR 250                      William Moseley\n236 M W F    2:20 - 3:20 pm THEATR 205                          Chris Wells\n237  T R    9:40 - 11:10 am THEATR 001                  Kirisitina Sailiata\n238 M W F  10:50 - 11:50 am   OLRI 243                   Louisa Bradtmiller\n239  T R    9:40 - 11:10 am   OLRI 300                       Roopali Phadke\n240  T R     1:20 - 2:50 pm   MAIN 111                           Amy Elkins\n241  T R     1:20 - 2:50 pm   OLRI 243                     Christie Manning\n242 M W      1:10 - 2:10 pm   OLRI 243                         Jerald Dosch\n243  T R     3:00 - 4:30 pm   OLRI 243                          Anika Bratt\n244   W     7:00 - 10:00 pm   OLRI 243                          Chris Wells\n245  T R     3:00 - 4:30 pm MARKIM 201     Samuel WegnerKirisitina Sailiata\n246  T R    9:40 - 11:10 am   OLRI 243                     Christie Manning\n247 M W      7:00 - 8:30 pm    HUM 216                         Ross Shields\n248 M W F  10:50 - 11:50 am   OLRI 284                          Mary Heskel\n249 M W F  10:50 - 11:50 am   OLRI 250                   Stotra Chakrabarti\n250 M W      8:00 - 9:30 am   OLRI 247                       Roopali Phadke\n251  T R     3:00 - 4:30 pm   CARN 105                      William Moseley\n252 M       7:00 - 10:00 pm   OLRI 243                     Christie Manning\n253 M       7:00 - 10:00 pm   OLRI 243                     Christie Manning\n254 M W F    8:30 - 9:30 am    HUM 215                             Lise Hoy\n255 M W F    2:20 - 3:20 pm    HUM 404                        El Hadji Diop\n256 M W F   9:40 - 10:40 am    HUM 216                      Juliette Rogers\n257 M W F    2:20 - 3:20 pm    HUM 314                       Andrew Billing\n258 M W F   9:40 - 10:40 am    HUM 213                      Claude Cassagne\n259 M W F   12:00 - 1:00 pm    HUM 314                      Claude Cassagne\n260 M W F    1:10 - 2:10 pm    HUM 402                             Lise Hoy\n261 M W F   12:00 - 1:00 pm    HUM 402                      Juliette Rogers\n262 M W F    1:10 - 2:10 pm    HUM 404                        El Hadji Diop\n263 M W F   12:00 - 1:00 pm    HUM 212                       Andrew Billing\n264 M W F  10:50 - 11:50 am    HUM 402                      Joëlle Vitiello\n265 M W F    1:10 - 2:10 pm    HUM 314                      Claude Cassagne\n266  T R    9:40 - 11:10 am   CARN 107               I-Chun Catherine Chang\n267  T R     1:20 - 2:50 pm   CARN 107               I-Chun Catherine Chang\n268 M W F   9:40 - 10:40 am   CARN 107                         Holly Barcus\n269  T R    9:40 - 11:10 am   LIBR 250                      William Moseley\n270  T R     3:00 - 4:30 pm    CARN 05                  Xavier Haro-Carrión\n271 M W F    3:30 - 4:30 pm   CARN 06A                       Daniel Trudeau\n272 M W F    2:20 - 3:20 pm   CARN 107                          Laura Smith\n273 M W F  10:50 - 11:50 am   CARN 105                          Laura Smith\n274 M W F   12:00 - 1:00 pm    CARN 05                       Daniel Trudeau\n275  T R    9:40 - 11:10 am   OLRI 300                       Roopali Phadke\n276 M W F  10:50 - 11:50 am    HUM 216                          Eric Carter\n277 M W F    1:10 - 2:10 pm   CARN 107               I-Chun Catherine Chang\n278  T R     1:20 - 2:50 pm   CARN 109                          Eric Carter\n279  T R    9:40 - 11:10 am   CARN 109                  Xavier Haro-Carrión\n280 M       7:00 - 10:00 pm    CARN 05                          Eric Carter\n281  T R     3:00 - 4:30 pm   CARN 105                      William Moseley\n282 M W F    1:10 - 2:10 pm   OLRI 187                Kristina Curry Rogers\n283 M W F  10:50 - 11:50 am   OLRI 100                         Alan Chapman\n284 M W F   9:40 - 10:40 am   OLRI 100                      Kelly MacGregor\n285 M W F    1:10 - 2:10 pm   OLRI 175                       Raymond Rogers\n286 M W F   9:40 - 10:40 am   OLRI 179                          Emily First\n287 M W F  10:50 - 11:50 am   OLRI 175                      Kelly MacGregor\n288 M W F   12:00 - 1:00 pm   OLRI 175                       Raymond Rogers\n289 M W F   12:00 - 1:00 pm   OLRI 179                         Alan Chapman\n290 M W F   12:00 - 1:00 pm    HUM 216                         Ross Shields\n291 M W F   9:40 - 10:40 am    HUM 409                         Ross Shields\n292  T R    9:40 - 11:10 am    HUM 215                         David Martyn\n293 M W F   9:40 - 10:40 am    HUM 214                       Rachael Huener\n294 M W F  10:50 - 11:50 am    HUM 214                       Rachael Huener\n295  T R     3:00 - 4:30 pm    HUM 401                      Kiarina Kordela\n296 M W      7:00 - 8:30 pm    HUM 216                         Ross Shields\n297  T R     1:20 - 2:50 pm    HUM 401                      Kiarina Kordela\n298  T R     3:00 - 4:30 pm    HUM 215                         David Martyn\n299 M W F    1:10 - 2:10 pm    HUM 314                      Claude Cassagne\n300  T R    9:40 - 11:10 am   MAIN 111                    Beth Severy-Hoven\n301 M W F  10:50 - 11:50 am    HUM 110                         Linda Sturtz\n302 M W F    1:10 - 2:10 pm   MAIN 111                         Linda Sturtz\n303  T R    9:40 - 11:10 am   MAIN 002                         James Coplin\n304 M W F  10:50 - 11:50 am   CARN 304                         Tara Hollies\n305  T R     3:00 - 4:30 pm THEATR 200                         Amy Sullivan\n306  T R     3:00 - 4:30 pm   MAIN 010                      Ernesto Capello\n307 M W F   9:40 - 10:40 am   CARN 105                       Niharika Yadav\n308 M       7:00 - 10:00 pm   MAIN 001                       Walter Greason\n309   W     7:00 - 10:00 pm   MAIN 010                       Walter Greason\n310  T R    9:40 - 11:10 am   MAIN 010                     Katrina Phillips\n311 M W F    8:30 - 9:30 am   MAIN 002                         Linda Sturtz\n312 M W F    2:20 - 3:20 pm THEATR 205                          Chris Wells\n313  T R    9:40 - 11:10 am    HUM 228                         Lauren Henry\n314 M       7:00 - 10:00 pm THEATR 204                         Amy Sullivan\n315  T R     1:20 - 2:50 pm   MAIN 010                      Ernesto Capello\n316 M W F    3:30 - 4:30 pm   MAIN 001                       Niharika Yadav\n317  T R     1:20 - 2:50 pm    HUM 314                       Artyom Tonoyan\n318 M W F  10:50 - 11:50 am THEATR 001                       Maria Fedorova\n319  T R     1:20 - 2:50 pm   MAIN 011                         Lauren Henry\n320  T R     8:00 - 9:30 am   MAIN 010                     Katrina Phillips\n321 M W      8:00 - 9:30 am   MAIN 011                          Karin Vélez\n322  T R     3:00 - 4:30 pm   CARN 404                        Ahmed Samatar\n323  T R    9:40 - 11:10 am   CARN 404                       Nadya Nedelsky\n324  T R     1:20 - 2:50 pm   CARN 404                       Nadya Nedelsky\n325 M W F   12:00 - 1:00 pm   CARN 404                       Vanessa Voller\n326 M W F    2:20 - 3:20 pm THEATR 201                        Julia Chadaga\n327 M       7:00 - 10:00 pm   CARN 404                    Jenna Rice Rahaim\n328 M W F    1:10 - 2:10 pm   CARN 404                       Vanessa Voller\n329 M W F    3:30 - 4:30 pm   CARN 404                    Jenna Rice Rahaim\n330  T R     1:20 - 2:50 pm   CARN 411                        Ahmed Samatar\n331  T R     3:00 - 4:30 pm   CARN 105                      William Moseley\n332   W     7:00 - 10:00 pm   CARN 411                       Nadya Nedelsky\n333 M       7:00 - 10:00 pm   CARN 105                          David Moore\n334 M W F   9:40 - 10:40 am    HUM 110                      Arthur Mitchell\n335 M W F  10:50 - 11:50 am    HUM 401                      Arthur Mitchell\n336 M W F    2:20 - 3:20 pm    HUM 110                       Ritsuko Larson\n337 M W F    3:30 - 4:30 pm    HUM 110                       Ritsuko Larson\n338 M W F    1:10 - 2:10 pm    HUM 110                      Arthur Mitchell\n339  T R    9:40 - 11:10 am    HUM 110                        Satoko Suzuki\n340 M W F   12:00 - 1:00 pm    HUM 110                       Ritsuko Larson\n341  T R     3:00 - 4:30 pm    HUM 110                        Satoko Suzuki\n342 M W F  10:50 - 11:50 am    HUM 215                     Kyoshin Sasahara\n343  T R     3:00 - 4:30 pm   MAIN 010                      Ernesto Capello\n344  T R     3:00 - 4:30 pm    CARN 05                  Xavier Haro-Carrión\n345  T R     3:00 - 4:30 pm   CARN 204                        Olga González\n346  T R     1:20 - 2:50 pm   MAIN 010                      Ernesto Capello\n347 M W F  10:50 - 11:50 am    HUM 212                J. Ernesto Ortiz Díaz\n348 M W F    1:10 - 2:10 pm    HUM 212                         Alicia Muñoz\n349 M W F   12:00 - 1:00 pm    HUM 217                J. Ernesto Ortiz Díaz\n350 M W F  10:50 - 11:50 am   LIBR 250                            Paul Dosh\n351 M W F  10:50 - 11:50 am    HUM 217                         Alicia Muñoz\n352 M       7:00 - 10:00 pm    CARN 05                          Eric Carter\n353 M W F    2:20 - 3:20 pm    HUM 215                   Daniel Coral Reyes\n354  T R    9:40 - 11:10 am    HUM 400                   Christina Esposito\n355  T R     8:00 - 9:30 am    HUM 400                   Christina Esposito\n356 M W F   12:00 - 1:00 pm    HUM 400                       Morgan Sleeper\n357  T R     1:20 - 2:50 pm   OLRI 301                    Marianne Milligan\n358 M W F    2:20 - 3:20 pm   OLRI 301                    Marianne Milligan\n359 M W F  10:50 - 11:50 am    HUM 400                       Morgan Sleeper\n360 M W F   9:40 - 10:40 am   MAIN 001                          James Laine\n361  T R    9:40 - 11:10 am    HUM 110                        Satoko Suzuki\n362   W     7:00 - 10:00 pm    HUM 214                       Morgan Sleeper\n363 M W F   9:40 - 10:40 am THEATR 213                     Cynthia Kauffeld\n364  T R     3:00 - 4:30 pm    HUM 110                        Satoko Suzuki\n365 M W F   9:40 - 10:40 am   OLRI 241                         Yariana Diaz\n366 M W F  10:50 - 11:50 am   OLRI 241                         Yariana Diaz\n367 M W F    2:20 - 3:20 pm   OLRI 241                     Andrew Beveridge\n368  T R    9:40 - 11:10 am THEATR 002                          David Ehren\n369 M W F    2:20 - 3:20 pm THEATR 206                        Paul Herstedt\n370 M W F    3:30 - 4:30 pm THEATR 206                        Paul Herstedt\n371  T R    9:40 - 11:10 am   OLRI 241                        Will Mitchell\n372 M W F    1:10 - 2:10 pm THEATR 203                       Kristin Heysse\n373 M W F    2:20 - 3:20 pm THEATR 203                       Kristin Heysse\n374 M W F    2:20 - 3:20 pm THEATR 204                          Taryn Flock\n375 M W F    3:30 - 4:30 pm THEATR 204                          Taryn Flock\n376 M W F   9:40 - 10:40 am THEATR 200                     Andrew Beveridge\n377 M W F  10:50 - 11:50 am THEATR 200                     Andrew Beveridge\n378  T R     3:00 - 4:30 pm THEATR 002                    Daniel O'Loughlin\n379  T R     3:00 - 4:30 pm THEATR 204                          Laura Lyman\n380 M W F   9:40 - 10:40 am THEATR 002                          Taryn Flock\n381 M W F  10:50 - 11:50 am THEATR 203                       Kristin Heysse\n382  T R     1:20 - 2:50 pm   OLRI 245                        Will Mitchell\n383  T R    9:40 - 11:10 am   OLRI 245                     Lori Ziegelmeier\n384 M W F    2:20 - 3:20 pm    HUM 400                      Michael Griffin\n385 M       7:00 - 10:00 pm    HUM 400                        Howard Sinker\n386 M W F   9:40 - 10:40 am    HUM 401                     Bradley Stiffler\n387 M W F   9:40 - 10:40 am   CARN 304                      Aisling Quigley\n388  T R     1:20 - 2:50 pm    HUM 112                       Morgan Adamson\n389  T R    9:40 - 11:10 am    HUM 401                      Michael Griffin\n390 M       7:00 - 10:00 pm   CARN 404                    Jenna Rice Rahaim\n391  T R     3:00 - 4:30 pm    HUM 401                      Kiarina Kordela\n392 M W      7:00 - 8:30 pm    HUM 215                  Kirisitina Sailiata\n393 M W F    3:30 - 4:30 pm    HUM 400                     Bradley Stiffler\n394 M       7:00 - 10:00 pm    HUM 402                     Bradley Stiffler\n395 M W F   12:00 - 1:00 pm  MUSIC 219                          Randy Bauer\n396 M W F   9:40 - 10:40 am  MUSIC 219                          Randy Bauer\n397  T R     3:00 - 4:30 pm  MUSIC 228                          Reid Kruger\n398 M W F  10:50 - 11:50 am  MUSIC 228                      Chuen-Fung Wong\n399 M W F  10:50 - 11:50 am    HUM 400                       Morgan Sleeper\n400 M W F   12:00 - 1:00 pm  MUSIC 228                         Mark Mazullo\n401 M W F    1:10 - 2:10 pm  MUSIC 228                         Mark Mazullo\n402 M W F  10:50 - 11:50 am  MUSIC 219                          Randy Bauer\n403 M       12:00 - 1:00 pm   OLRI 370                        Darcy Burgund\n404  T R     1:20 - 2:50 pm    HUM 213                       Rotem Herrmann\n405  T R     3:00 - 4:30 pm    HUM 213                       Rotem Herrmann\n406  T R     1:20 - 2:50 pm   MAIN 002                      Geoffrey Gorham\n407 M W F   9:40 - 10:40 am   MAIN 010                           Max Dresow\n408 M W F  10:50 - 11:50 am   MAIN 010                           Max Dresow\n409 M W F    2:20 - 3:20 pm   OLRI 300                       Samuel Asarnow\n410 M W F    1:10 - 2:10 pm   OLRI 270                       Samuel Asarnow\n411 M W F   9:40 - 10:40 am   OLRI 300                       Rotem Herrmann\n412 M       7:00 - 10:00 pm   MAIN 002                           Max Dresow\n413  T R     1:20 - 2:50 pm    HUM 215                    Sumeet Patwardhan\n414   W     7:00 - 10:00 pm   MAIN 002                      Geoffrey Gorham\n415   W     7:00 - 10:00 pm   MAIN 003                    Sumeet Patwardhan\n416 M W F   9:40 - 10:40 am   OLRI 250                            Saki Khan\n417 M W F    1:10 - 2:10 pm   OLRI 150                          James Doyle\n418 M W F   12:00 - 1:00 pm   OLRI 101                        Anna Williams\n419 M W F   9:40 - 10:40 am   OLRI 150                  Tonnis ter Veldhuis\n420 M W F    8:30 - 9:30 am   OLRI 101                         James Heyman\n421 M W F    1:10 - 2:10 pm   OLRI 101                         James Heyman\n422 M W F    2:20 - 3:20 pm   OLRI 101                  Tonnis ter Veldhuis\n423   W      3:30 - 4:30 pm   OLRI 150                          James Doyle\n424  T R    9:40 - 11:10 am   CARN 206                      Patrick Schmidt\n425 M W      8:00 - 9:30 am   CARN 305                        Andrew Latham\n426 M W F    1:10 - 2:10 pm ARTCOM 202                        Andrew Latham\n427 M W      8:00 - 9:30 am   LIBR 250                            Paul Dosh\n428  T R     1:20 - 2:50 pm    CARN 05                         Rothin Datta\n429 M W F   9:40 - 10:40 am   CARN 305                        Lesley Lavery\n430 M W F  10:50 - 11:50 am   CARN 206                          Julie Dolan\n431 M W F  10:50 - 11:50 am   CARN 204                      Patrick Schmidt\n432 M       7:00 - 10:00 pm   CARN 305                         Eric Wojchik\n433  T R    9:40 - 11:10 am    HUM 112                          Wendy Weber\n434  T R     3:00 - 4:30 pm   CARN 206                         Lisa Mueller\n435  T R    9:40 - 11:10 am   OLRI 300                       Roopali Phadke\n436  T R     1:20 - 2:50 pm   CARN 206                         Della Zurick\n437  T R     1:20 - 2:50 pm    HUM 215                    Sumeet Patwardhan\n438  T R     1:20 - 2:50 pm   CARN 105                         Lisa Mueller\n439  T R     3:00 - 4:30 pm    HUM 401                      Kiarina Kordela\n440  T R     1:20 - 2:50 pm THEATR 201                        Andrew Latham\n441 M W F    2:20 - 3:20 pm   CARN 305                         Della Zurick\n442 M       7:00 - 10:00 pm   CARN 06A                         Della Zurick\n443   W     7:00 - 10:00 pm   CARN 06A                        David Schultz\n444  T R     3:00 - 4:30 pm   CARN 304                        Lesley Lavery\n445 M W F  10:50 - 11:50 am   CARN 404                          Wendy Weber\n446 M W F  10:50 - 11:50 am   LIBR 250                            Paul Dosh\n447 M W F    1:10 - 2:10 pm   CARN 204                          Wendy Weber\n448 M W F   12:00 - 1:00 pm   CARN 204                          Julie Dolan\n449 M W F   9:40 - 10:40 am   LIBR 250                            Paul Dosh\n450   W F    8:45 - 9:30 am   CARN 204                        Lesley Lavery\n451 M W F  10:50 - 11:50 am    HUM 409           Fernanda Bartolomei-Merlin\n452 M W F    2:20 - 3:20 pm    HUM 217                J. Ernesto Ortiz Díaz\n453 M W F   12:00 - 1:00 pm    HUM 217                J. Ernesto Ortiz Díaz\n454 M W F   9:40 - 10:40 am THEATR 206                   Cari Gillen-O'Neel\n455 M W F    8:30 - 9:30 am THEATR 206                   Cari Gillen-O'Neel\n456 M W F  10:50 - 11:50 am THEATR 213                           Brooke Lea\n457 M W F    2:20 - 3:20 pm THEATR 202                           Brooke Lea\n458  T R    9:40 - 11:10 am   OLRI 250                           Tina Kruse\n459  T R     3:00 - 4:30 pm THEATR 202                           Tina Kruse\n460 M W F    8:30 - 9:30 am   OLRI 352                          Ariel James\n461 M W F   9:40 - 10:40 am THEATR 204                        Darcy Burgund\n462 M W F    2:20 - 3:20 pm   OLRI 352                    Jean-Marie Maddux\n463 M W F    1:10 - 2:10 pm THEATR 204                        Annie Pezalla\n464  T R     1:20 - 2:50 pm THEATR 202                        Varnica Arora\n465 M       7:00 - 10:00 pm THEATR 202           Keith HalperinKaren Grabow\n466  T R     3:00 - 4:30 pm THEATR 201                       Rebecca Bigler\n467  T R     1:20 - 2:50 pm   OLRI 243                     Christie Manning\n468  T R    9:40 - 11:10 am   OLRI 243                     Christie Manning\n469  T R     1:20 - 2:50 pm THEATR 213                      Steve Guglielmo\n470  T R    9:40 - 11:10 am   OLRI 301                        Annie Pezalla\n471  T R     3:00 - 4:30 pm   OLRI 300                          Ariel James\n472  T R    9:40 - 11:10 am   OLRI 370                      Steve Guglielmo\n473  T R     1:20 - 2:50 pm   OLRI 370                            Jill Fish\n474 M W      7:00 - 8:30 pm   OLRI 352             Alexander RomanAnne Rial\n475   W     7:00 - 10:00 pm   OLRI 370                       Tracey Wilkins\n476  T R     8:00 - 9:30 am   OLRI 370                      Sarah Gillespie\n477 M W F  10:50 - 11:50 am   OLRI 352                  Piercarlo Valdesolo\n478 M W F  10:50 - 11:50 am   OLRI 370                        Annie Pezalla\n479 M W F  10:50 - 11:50 am   OLRI 349                   Cari Gillen-O'Neel\n480 M W F  10:50 - 11:50 am   OLRI 270                        Jaine Strauss\n481 M W F    2:20 - 3:20 pm   CARN 404                    Jenna Rice Rahaim\n482  T R    9:40 - 11:10 am    CARN 05                         William Hart\n483 M W F  10:50 - 11:50 am   MAIN 002                           Erik Davis\n484  T R     3:00 - 4:30 pm   MAIN 003                     Nicholas Schaser\n485 M W F   9:40 - 10:40 am   CARN 204                        Susanna Drake\n486 M W F  10:50 - 11:50 am    HUM 314                        Susanna Drake\n487 M W F    1:10 - 2:10 pm   MAIN 002                          James Laine\n488  T R    9:40 - 11:10 am    HUM 402                        Susanna Drake\n489  T R     1:20 - 2:50 pm   MAIN 003                           Erik Davis\n490 M W F   9:40 - 10:40 am   MAIN 001                          James Laine\n491  T R     3:00 - 4:30 pm    HUM 401                      Kiarina Kordela\n492  T R     3:00 - 4:30 pm   MAIN 011                         William Hart\n493 M W F   9:40 - 10:40 am THEATR 001                       Maria Fedorova\n494 M W F   9:40 - 10:40 am    HUM 314                        Julia Chadaga\n495 M W F    2:20 - 3:20 pm THEATR 201                        Julia Chadaga\n496  T R     1:20 - 2:50 pm    HUM 314                       Artyom Tonoyan\n497 M W F  10:50 - 11:50 am THEATR 001                       Maria Fedorova\n498 M W F    1:10 - 2:10 pm    HUM 314                      Claude Cassagne\n499 M W F    3:30 - 4:30 pm   MAIN 010                     Christina Hughes\n500   W     7:00 - 10:00 pm   CARN 204                      Khaldoun Samman\n501  T R     1:20 - 2:50 pm   CARN 204                      Khaldoun Samman\n502 M W F   12:00 - 1:00 pm   CARN 06A                           Aspen Chen\n503 M       7:00 - 10:00 pm   CARN 204                      Khaldoun Samman\n504  T R    9:40 - 11:10 am   CARN 304                 Erika Busse-Cárdenas\n505 M W F  10:50 - 11:50 am    CARN 05                          Erik Larson\n506  T R     3:00 - 4:30 pm    HUM 112                     Christina Hughes\n507 M W F    1:10 - 2:10 pm   LIBR 250                          Erik Larson\n508  T R     1:20 - 2:50 pm   CARN 304 Christina HughesErika Busse-Cárdenas\n509 M W F    1:10 - 2:10 pm    HUM 214           Fernanda Bartolomei-Merlin\n510 M W F    2:20 - 3:20 pm    HUM 214           Fernanda Bartolomei-Merlin\n511 M W F    8:30 - 9:30 am    HUM 400                    Rosa Rull-Montoya\n512 M W F   9:40 - 10:40 am    HUM 400                    Rosa Rull-Montoya\n513 M W F    1:10 - 2:10 pm    HUM 217               Susana Blanco-Iglesias\n514 M W F   9:40 - 10:40 am    HUM 212                   Fernando Contreras\n515 M W F    8:30 - 9:30 am    HUM 212                         Magaly Ortiz\n516 M W F   12:00 - 1:00 pm THEATR 204                    Rosa Rull-Montoya\n517 M W F    2:20 - 3:20 pm    HUM 212                           Toni Dorca\n518 M W F   12:00 - 1:00 pm THEATR 205               Blanca Gimeno Escudero\n519 M W F    1:10 - 2:10 pm    HUM 213                     Claudia Giannini\n520 M W F    2:20 - 3:20 pm    HUM 213                     Claudia Giannini\n521  T R     1:20 - 2:50 pm    HUM 402               Blanca Gimeno Escudero\n522  T R     3:00 - 4:30 pm    HUM 402               Blanca Gimeno Escudero\n523 M W F  10:50 - 11:50 am    HUM 113                     Claudia Giannini\n524 M W F   12:00 - 1:00 pm    HUM 112                   Daniel Coral Reyes\n525 M W F   12:00 - 1:00 pm THEATR 213                     Cynthia Kauffeld\n526 M W F   9:40 - 10:40 am    HUM 215                         Alicia Muñoz\n527 M W F  10:50 - 11:50 am    HUM 212                J. Ernesto Ortiz Díaz\n528 M W F    1:10 - 2:10 pm    HUM 212                         Alicia Muñoz\n529 M W F   9:40 - 10:40 am THEATR 213                     Cynthia Kauffeld\n530 M W F   12:00 - 1:00 pm    HUM 214               Susana Blanco-Iglesias\n531 M W F   12:00 - 1:00 pm    HUM 213                           Toni Dorca\n532 M W F  10:50 - 11:50 am    HUM 217                         Alicia Muñoz\n533 M W F    2:20 - 3:20 pm    HUM 215                   Daniel Coral Reyes\n534  T R    9:40 - 11:10 am    HUM 216                         Magaly Ortiz\n535  T R     3:00 - 4:30 pm   OLRI 254                    Brianna Heggeseth\n536  T R    9:40 - 11:10 am THEATR 205                        Amin Alhashim\n537  T R     1:20 - 2:50 pm THEATR 205                        Amin Alhashim\n538 M W F   12:00 - 1:00 pm THEATR 206                      Vittorio Addona\n539 M W F    1:10 - 2:10 pm THEATR 206                      Vittorio Addona\n540 M W F   9:40 - 10:40 am   OLRI 254                        Taylor Okonek\n541 M W F  10:50 - 11:50 am   OLRI 254                        Taylor Okonek\n542 M W F    3:30 - 4:30 pm   OLRI 254                          Laura Lyman\n543  T R     1:20 - 2:50 pm   OLRI 241                     Lori Ziegelmeier\n544  T R     3:00 - 4:30 pm   OLRI 241                     Lori Ziegelmeier\n545  T R    9:40 - 11:10 am THEATR 201                         Leslie Myint\n546  T R     1:20 - 2:50 pm THEATR 204                         Leslie Myint\n547  T R    9:40 - 11:10 am THEATR 206                       Alicia Johnson\n548  T R     1:20 - 2:50 pm THEATR 206                       Alicia Johnson\n549  T R     3:00 - 4:30 pm THEATR 206                       Alicia Johnson\n550  T R     3:00 - 4:30 pm THEATR 204                          Laura Lyman\n551  T R    9:40 - 11:10 am   OLRI 254                    Brianna Heggeseth\n552  T R     1:20 - 2:50 pm   OLRI 254                    Brianna Heggeseth\n553  T R    9:40 - 11:10 am THEATR 202                           Shilad Sen\n554  T R    9:40 - 11:10 am THEATR 203                          Wynn Fricke\n555  T R    9:40 - 11:10 am THEATR 204                          Randy Reyes\n556  T R    9:40 - 11:10 am THEATR 213                         Sam Mitchell\n557 M W F    2:20 - 3:20 pm THEATR 120                         Robert Rosen\n558 M W F    3:30 - 4:30 pm THEATR 120                         Robert Rosen\n559 M W F  10:50 - 11:50 am THEATR 006                       Thomas Barrett\n560 M W F   8:30 - 10:40 am THEATR 006                     Brandon Chambers\n561  T R     1:20 - 2:50 pm THEATR 006                         Wu Chen Khoo\n562 M W F    3:30 - 4:30 pm THEATR 213                    Alayna Jacqueline\n563  T R     3:00 - 4:30 pm THEATR 213                   Cristina Luzárraga\n564  T R     3:00 - 4:30 pm THEATR 205           Cláudia Tatinge Nascimento\n565 M        7:00 - 8:30 pm THEATR 213                      Kenji Shoemaker\n566 M W F   12:00 - 1:00 pm THEATR 101                      MaryBeth Gagner\n567 M W F    1:10 - 2:10 pm THEATR 006                   Katharine Horowitz\n568 M W F    2:20 - 4:30 pm THEATR 002                          Randy Reyes\n569  T R    9:40 - 11:10 am THEATR 006                        Kathy Maxwell\n570 M        4:45 - 6:30 pm THEATR 213           Cláudia Tatinge Nascimento\n571 M W F    1:10 - 2:10 pm   MAIN 009                            Myrl Beam\n572  T R     3:00 - 4:30 pm THEATR 200                         Amy Sullivan\n573  T R    9:40 - 11:10 am    HUM 213                          Rachel Gold\n574  T R     3:00 - 4:30 pm   MAIN 009                            Myrl Beam\n575  T R    9:40 - 11:10 am   MAIN 009                        Sonita Sarker\n576 M W F    8:30 - 9:30 am   MAIN 002                         Linda Sturtz\n577  T R    9:40 - 11:10 am ARTCOM 102                        Joanna Inglot\n578 M W F    1:10 - 2:10 pm    HUM 111                             Xin Yang\n579  T R     1:20 - 2:50 pm   CARN 206                         Della Zurick\n580  T R     3:00 - 4:30 pm THEATR 201                       Rebecca Bigler\n581 M W F  10:50 - 11:50 am   MAIN 009                            Ebony Aya\n582  T R     1:20 - 2:50 pm   MAIN 009                        Sonita Sarker\n583 M W F  10:50 - 11:50 am   CARN 404                          Wendy Weber\n    avail max enroll\n1       3  20     17\n2      -4  16     20\n3       0  14     14\n4       3  25     22\n5      -2  20     22\n6      -1  15     16\n7       2  20     18\n8       8  25     17\n9      -4  24     28\n10      3  25     22\n11     -5  20     25\n12      0  20     20\n13     11  25     14\n14      2  12     10\n15      2  16     14\n16      6  15      9\n17      9  20     11\n18      2  15     13\n19      7  20     13\n20      8  15      7\n21      5  16     11\n22      4  16     12\n23      0  24     24\n24      1  24     23\n25      1  24     23\n26     -1  24     25\n27      0  20     20\n28     11  16      5\n29     10  30     20\n30      6  16     10\n31      4  30     26\n32      0  14     14\n33     11  20      9\n34      2  20     18\n35     16  20      4\n36      7  16      9\n37      8  12      4\n38      0  15     15\n39      0  15     15\n40      0  13     13\n41     -3  30     33\n42      0  12     12\n43      1  16     15\n44      0  14     14\n45      0  15     15\n46      1  15     14\n47      0  12     12\n48      0  16     16\n49      0  20     20\n50      5  20     15\n51      1  15     14\n52      2  20     18\n53      3  20     17\n54      0  16     16\n55      0   8      8\n56      9  15      6\n57      8  15      7\n58     11  16      5\n59     -1  16     17\n60      7  20     13\n61     -4  16     20\n62      4  20     16\n63      7  20     13\n64      0  14     14\n65      2  20     18\n66      2  20     18\n67      6  20     14\n68      0  15     15\n69     12  20      8\n70     -1  32     33\n71      1  28     27\n72      0  28     28\n73      0  36     36\n74      7  36     29\n75      0  36     36\n76      1  24     23\n77      0  24     24\n78      2  15     13\n79      3  16     13\n80      3  12      9\n81     -3  14     17\n82     -2  12     14\n83      0  18     18\n84      0  32     32\n85      0  16     16\n86     -1  12     13\n87      2  12     10\n88      0  16     16\n89      6  18     12\n90      0  12     12\n91      5  16     11\n92      2   6      4\n93     20  33     13\n94      0  32     32\n95      0  32     32\n96      0  32     32\n97      0  32     32\n98      0  16     16\n99      8  24     16\n100    -1  24     25\n101    -4  32     36\n102    28  70     42\n103     4  24     20\n104     0  32     32\n105     2  12     10\n106     6  12      6\n107    12  20      8\n108     9  20     11\n109    12  20      8\n110    13  20      7\n111     2  20     18\n112     9  20     11\n113    11  20      9\n114    10  20     10\n115     7  20     13\n116     3  20     17\n117     5  25     20\n118     0  20     20\n119     9  20     11\n120     7  20     13\n121     4  16     12\n122    13  25     12\n123    12  20      8\n124    11  20      9\n125    -3  30     33\n126     3  20     17\n127     8  12      4\n128    15  20      5\n129     8  20     12\n130     1  28     27\n131     3  24     21\n132     1  26     25\n133     2  25     23\n134    -6  25     31\n135     5  25     20\n136     2  25     23\n137     0  16     16\n138     0  16     16\n139    -1  16     17\n140     1  20     19\n141    -1  22     23\n142    -1  16     17\n143     0  16     16\n144     9  20     11\n145     9  20     11\n146    13  25     12\n147     2  16     14\n148     0  16     16\n149     3  20     17\n150     1  22     21\n151     4  12      8\n152     2  20     18\n153     7  14      7\n154     0  20     20\n155     9  20     11\n156     0  33     33\n157     0  16     16\n158    -1  25     26\n159     1  25     24\n160     6  25     19\n161     4  25     21\n162    -1  25     26\n163     7  25     18\n164    11  25     14\n165     0  16     16\n166     1  15     14\n167    16  25      9\n168    11  25     14\n169     8  25     17\n170    -2  20     22\n171     5  25     20\n172     5  25     20\n173     9  25     16\n174     0  25     25\n175     0  25     25\n176    15  35     20\n177    -2  22     24\n178     4  22     18\n179    -3  20     23\n180    -3  20     23\n181     9  15      6\n182    -5  20     25\n183     0  20     20\n184     0  20     20\n185     0  13     13\n186     0  30     30\n187   -10  18     28\n188    -4  24     28\n189     4  24     20\n190     2  15     13\n191     5  20     15\n192    -2  18     20\n193    11  18      7\n194    11  20      9\n195     6  12      6\n196     2  20     18\n197     3  20     17\n198     1  20     19\n199     3  20     17\n200     3  16     13\n201     0  16     16\n202     2  16     14\n203     0  16     16\n204     0  16     16\n205     0  15     15\n206     0  15     15\n207     1  20     19\n208     0  20     20\n209     5  20     15\n210     0  13     13\n211     4  20     16\n212     7  20     13\n213     0  20     20\n214     1  16     15\n215     0  16     16\n216     3  12      9\n217     7  12      5\n218     0  16     16\n219     0  16     16\n220     4  15     11\n221     7  20     13\n222     1  16     15\n223    -2  20     22\n224     7  12      5\n225     2  14     12\n226     2  48     46\n227     3  24     21\n228    28  48     20\n229    -1  32     33\n230     1  28     27\n231     0  28     28\n232    15  20      5\n233     6  24     18\n234     1  25     24\n235    10  30     20\n236    -2  40     42\n237     8  25     17\n238     5  24     19\n239     0  16     16\n240     7  20     13\n241     3  22     19\n242     2  15     13\n243     3  16     13\n244     4  18     14\n245     2  12     10\n246     1  22     21\n247    19  25      6\n248    -1  12     13\n249     0  16     16\n250    -1   8      9\n251     6  16     10\n252     2  20     18\n253     2  20     18\n254     1  15     14\n255     0  15     15\n256     3  20     17\n257     1  10      9\n258     3  23     20\n259     1  23     22\n260     8  20     12\n261     7  15      8\n262    10  15      5\n263    -1  15     16\n264     5  15     10\n265    11  20      9\n266     6  20     14\n267     6  24     18\n268     6  30     24\n269    10  30     20\n270     0  13     13\n271    -2  30     32\n272     0  25     25\n273     0  14     14\n274     0  15     15\n275     0  16     16\n276     7  25     18\n277     7  24     17\n278     7  12      5\n279     4  12      8\n280    12  20      8\n281     6  16     10\n282     0  14     14\n283     0  48     48\n284    28  48     20\n285     0  13     13\n286     9  18      9\n287    -3  18     21\n288     6  18     12\n289    12  18      6\n290    12  20      8\n291     4  15     11\n292     0  14     14\n293     9  20     11\n294     6  20     14\n295     2  30     28\n296    19  25      6\n297     1  15     14\n298     0  18     18\n299    11  20      9\n300     5  25     20\n301     1  20     19\n302     4  18     14\n303     7  20     13\n304     0  20     20\n305    -1  18     19\n306     0  10     10\n307     4  20     16\n308     0  14     14\n309    -1  15     16\n310     2  20     18\n311     8  20     12\n312    -2  40     42\n313     3  20     17\n314    -5  20     25\n315     1  20     19\n316     0  15     15\n317     7  20     13\n318     0  10     10\n319     8  15      7\n320     1  15     14\n321     6  15      9\n322     1  12     11\n323     1  25     24\n324    -1  20     21\n325    -2  20     22\n326     1  20     19\n327    -3  20     23\n328     0  20     20\n329     2  20     18\n330     7  15      8\n331     6  16     10\n332    -1  14     15\n333     0  14     14\n334     2  22     20\n335    -3  22     25\n336     5  20     15\n337     9  20     11\n338     0  14     14\n339     6  20     14\n340     4  20     16\n341    12  20      8\n342    17  20      3\n343     0  10     10\n344     0  13     13\n345     0  14     14\n346     1  20     19\n347     1  15     14\n348     2  15     13\n349     9  15      6\n350     5  20     15\n351     8  15      7\n352    12  20      8\n353     0  15     15\n354    -1  30     31\n355     0  20     20\n356     5  20     15\n357     5  20     15\n358     0  20     20\n359     1  20     19\n360     7  20     13\n361     6  20     14\n362    11  20      9\n363    -1  15     16\n364    12  20      8\n365     3  24     21\n366     0  24     24\n367    -9  24     33\n368     9  24     15\n369     6  24     18\n370     2  24     22\n371     1  15     14\n372    -5  24     29\n373    -3  24     27\n374     1  20     19\n375     5  20     15\n376    -9  20     29\n377    -6  20     26\n378     7  20     13\n379    -2  20     22\n380    -4  20     24\n381    -2  20     22\n382     1  16     15\n383     0  16     16\n384     2  16     14\n385    -1  18     19\n386     2  24     22\n387    -1  16     17\n388     0  12     12\n389    19  24      5\n390    -3  20     23\n391     2  30     28\n392    11  25     14\n393     0  24     24\n394     9  20     11\n395     5  25     20\n396     3  25     22\n397    -1  12     13\n398     0   8      8\n399     1  20     19\n400     0  13     13\n401    10  20     10\n402     6  20     14\n403     4  16     12\n404     0  20     20\n405     2  20     18\n406     1  16     15\n407     0  20     20\n408     3  20     17\n409     0  20     20\n410     0  16     16\n411     1  16     15\n412    10  16      6\n413     5  16     11\n414     3  10      7\n415     9  10      1\n416    22  36     14\n417     2  48     46\n418     1  15     14\n419    14  54     40\n420    12  24     12\n421     8  24     16\n422    14  24     10\n423    51  63     12\n424     4  25     21\n425     3  25     22\n426    -1  15     16\n427     1  20     19\n428     2  20     18\n429     3  25     22\n430     1  25     24\n431     0  16     16\n432     1  25     24\n433     7  25     18\n434    -1  20     21\n435     0  16     16\n436     5  25     20\n437     5  16     11\n438     5  25     20\n439     2  30     28\n440     3  25     22\n441     9  25     16\n442     8  25     17\n443    -2  25     27\n444     5  20     15\n445    11  20      9\n446     5  20     15\n447     5  15     10\n448     5  15     10\n449     9  15      6\n450     3  16     13\n451     1  15     14\n452     2  15     13\n453     9  15      6\n454     3  36     33\n455     3  36     33\n456     0  16     16\n457     0  32     32\n458     0  30     30\n459   -10  18     28\n460     0  24     24\n461    -1  30     31\n462     0  24     24\n463    -2  28     30\n464    -1  28     29\n465     3  24     21\n466     2  28     26\n467     3  22     19\n468     1  22     21\n469    12  24     12\n470     2  24     22\n471     0  16     16\n472     0  16     16\n473     0  15     15\n474     0  16     16\n475     0  16     16\n476     0  16     16\n477     0   9      9\n478     0   4      4\n479     0   9      9\n480     0  14     14\n481     0  15     15\n482     2  15     13\n483    -1  16     17\n484     7  15      8\n485     5  20     15\n486    10  20     10\n487     1  12     11\n488     2  15     13\n489     4  12      8\n490     7  20     13\n491     2  30     28\n492     6  15      9\n493     6  20     14\n494    10  20     10\n495     1  20     19\n496     7  20     13\n497     0  10     10\n498    11  20      9\n499     2  25     23\n500     7  20     13\n501     1  14     13\n502     9  25     16\n503     2  20     18\n504    -2  20     22\n505     9  20     11\n506     3  20     17\n507    19  25      6\n508     3  19     16\n509     0  20     20\n510    -1  20     21\n511    -1  20     21\n512     1  20     19\n513     0  15     15\n514     0  20     20\n515     9  20     11\n516     1  20     19\n517     5  20     15\n518     4  20     16\n519     6  20     14\n520     4  20     16\n521     4  15     11\n522     6  15      9\n523     3  15     12\n524     6  15      9\n525     9  16      7\n526     0  15     15\n527     1  15     14\n528     2  15     13\n529    -1  15     16\n530    -1  15     16\n531     1  15     14\n532     8  15      7\n533     0  15     15\n534     8  15      7\n535     1  28     27\n536     3  24     21\n537     1  26     25\n538    -2  24     26\n539    -8  24     32\n540     0  24     24\n541    -2  24     26\n542    -1  24     25\n543    -6  24     30\n544    -3  24     27\n545     9  20     11\n546     9  20     11\n547    -3  20     23\n548    -3  20     23\n549     2  20     18\n550    -2  20     22\n551     7  14      7\n552     6  14      8\n553     9  20     11\n554     1  10      9\n555     0   9      9\n556     8  14      6\n557     4  14     10\n558     0  14     14\n559     6  12      6\n560     7  12      5\n561     2  12     10\n562     3  12      9\n563     7  12      5\n564    11  16      5\n565     4  14     10\n566     2  16     14\n567     5  12      7\n568    -1   8      9\n569     8  12      4\n570     6  12      6\n571     0  15     15\n572    -1  18     19\n573     2  20     18\n574    -2  20     22\n575     4  15     11\n576     8  20     12\n577     0  20     20\n578     2  20     18\n579     5  25     20\n580     2  28     26\n581     9  20     11\n582    10  15      5\n583    11  20      9\n\n\nExercise 6: Enrollment & departments\nExplore enrollments by department. You decide what research questions to focus on. Use both visual and numerical summaries.\n\nenrollments |&gt; \n  group_by(dept) |&gt; \n  summarize(avg_enroll = mean(enroll), \n            total_enroll = sum(enroll), \n            n_courses = n()) |&gt; \n  arrange(desc(total_enroll)) |&gt; \n  head(10)\n\n# A tibble: 10 × 4\n   dept  avg_enroll total_enroll n_courses\n   &lt;chr&gt;      &lt;dbl&gt;        &lt;dbl&gt;     &lt;int&gt;\n 1 PSYC        20.8          562        27\n 2 ENVI        18.9          529        28\n 3 ECON        19.4          525        27\n 4 COMP        18.2          509        28\n 5 POLI        17.6          476        27\n 6 ENGL        14.9          447        30\n 7 BIOL        19.4          446        23\n 8 MATH        21.3          404        19\n 9 STAT        20.9          397        19\n10 SPAN        14.3          372        26\n\nenrollments |&gt; \n  group_by(dept) |&gt; \n  summarize(avg_enroll = mean(enroll)) |&gt; \n  ggplot(aes(x = reorder(dept, avg_enroll), y = avg_enroll)) +\n  geom_col() +\n  coord_flip()\n\n\n\n\n\n\n\nExercise 7: Enrollment & faculty\nLet’s now explore enrollments by instructor. In doing so, we have to be cautious of cross-listed courses that are listed under multiple different departments. Uncomment the code lines in the chunk below for an example.\n\n\n\n\n\n\nCommenting/Uncommenting Code\n\n\n\nTo comment/uncomment several lines of code at once, highlight them then click ctrl/cmd+shift+c.\n\n\n\nenrollments |&gt;\n  filter(dept %in% c(\"STAT\", \"COMP\"), number == 112, section == \"01\")\n\n  dept number section   crn                         name  days           time\n1 COMP    112      01 10248 Introduction to Data Science  T R  3:00 - 4:30 pm\n2 STAT    112      01 10249 Introduction to Data Science  T R  3:00 - 4:30 pm\n      room        instructor avail max enroll\n1 OLRI 254 Brianna Heggeseth     1  28     27\n2 OLRI 254 Brianna Heggeseth     1  28     27\n\n\nNotice that these are the exact same section! In order to not double count an instructor’s enrollments, we can keep only the courses that have distinct() combinations of days, time, instructor values. Uncomment the code lines in the chunk below.\n\nenrollments_2 &lt;- enrollments |&gt; \n  distinct(days, time, instructor, .keep_all = TRUE)\n\n# NOTE: By default this keeps the first department alphabetically\n# That's fine because we won't use this to analyze department enrollments!\nenrollments_2 |&gt;\n  filter(instructor == \"Brianna Heggeseth\", name == \"Introduction to Data Science\")\n\n  dept number section   crn                         name  days           time\n1 COMP    112      01 10248 Introduction to Data Science  T R  3:00 - 4:30 pm\n      room        instructor avail max enroll\n1 OLRI 254 Brianna Heggeseth     1  28     27\n\nenrollments_2 |&gt; \n  group_by(instructor) |&gt; \n  summarize(total_enroll = sum(enroll), avg_enroll = mean(enroll), n_courses = n()) |&gt; \n  arrange(desc(total_enroll)) |&gt; \n  slice_head(n = 10)\n\n# A tibble: 10 × 4\n   instructor         total_enroll avg_enroll n_courses\n   &lt;chr&gt;                     &lt;dbl&gt;      &lt;dbl&gt;     &lt;int&gt;\n 1 Andrew Beveridge             88       29.3         3\n 2 Kristin Heysse               78       26           3\n 3 Cari Gillen-O'Neel           75       25           3\n 4 Lori Ziegelmeier             73       24.3         3\n 5 Lian Duan                    71       23.7         3\n 6 Mario Solis-Garcia           66       22           3\n 7 Alicia Johnson               64       21.3         3\n 8 Kelsey Boyle                 64       32           2\n 9 Susan Green                  64       32           2\n10 Thomas Varberg               62       31           2\n\n\nNow, explore enrollments by instructor. You decide what research questions to focus on. Use both visual and numerical summaries.\nCAVEAT: The above code doesn’t deal with co-taught courses that have more than one instructor. Thus instructors that co-taught are recorded as a pair, and their co-taught enrollments aren’t added to their total enrollments. This is tough to get around with how the data were scraped as the instructor names are smushed together, not separated by a comma!\nQuestion: which instructor has largest number of mean enrollment?\n\nenrollments_2 |&gt; \n  group_by(instructor) |&gt; \n  summarize(total_enroll = sum(enroll)) |&gt; \n  arrange(desc(total_enroll)) |&gt; \n  slice_head(n = 15) |&gt; \n  ggplot(aes(x = reorder(instructor, total_enroll), y = total_enroll)) +\n  geom_col() +\n  coord_flip()\n\n\n\n\n\n\n\nOptional extra practice\n\n# Make a bar plot showing the number of night courses by day of the week\n# Use courses_clean\ncourses_clean |&gt; \n  filter(str_detect(time, \"^7:\")) |&gt; \n  ggplot(aes(x = days)) +\n  geom_bar()\n\n\n\n\n\n\n\nDig Deeper: regex\nExample 4 gave 1 small example of a regular expression.\nThese are handy when we want process a string variable, but there’s no consistent pattern by which to do this. You must think about the structure of the string and how you can use regular expressions to capture the patterns you want (and exclude the patterns you don’t want).\nFor example, how would you describe the pattern of a 10-digit phone number? Limit yourself to just a US phone number for now.\n\nThe first 3 digits are the area code.\nThe next 3 digits are the exchange code.\nThe last 4 digits are the subscriber number.\n\nThus, a regular expression for a US phone number could be:\n\n\n[:digit:]{3}-[:digit:]{3}-[:digit:]{4} which limits you to XXX-XXX-XXXX pattern or\n\n\\\\([:digit:]{3}\\\\) [:digit:]{3}-[:digit:]{4} which limits you to (XXX) XXX-XXXX pattern or\n\n[:digit:]{3}\\\\.[:digit:]{3}\\\\.[:digit:]{4} which limits you to XXX.XXX.XXXX pattern\n\nThe following would include the three patterns above in addition to the XXXXXXXXXX pattern (no dashes or periods): - [\\\\(]*[:digit:]{3}[-.\\\\)]*[:digit:]{3}[-.]*[:digit:]{4}\nIn order to write a regular expression, you first need to consider what patterns you want to include and exclude.\nWork through the following examples, and the tutorial after them to learn about the syntax.\nEXAMPLES\n\n# Define some strings to play around with\nexample &lt;- \"The quick brown fox jumps over the lazy dog.\"\n\n\nstr_replace(example, \"quick\", \"really quick\")\n\n[1] \"The really quick brown fox jumps over the lazy dog.\"\n\n\n\nstr_replace_all(example, \"(fox|dog)\", \"****\") # | reads as OR\n\n[1] \"The quick brown **** jumps over the lazy ****.\"\n\n\n\nstr_replace_all(example, \"(fox|dog).\", \"****\") # \".\" for any character\n\n[1] \"The quick brown ****jumps over the lazy ****\"\n\n\n\nstr_replace_all(example, \"(fox|dog)\\\\.$\", \"****\") # at end of sentence only, \"\\\\.\" only for a period\n\n[1] \"The quick brown fox jumps over the lazy ****\"\n\n\n\nstr_replace_all(example, \"the\", \"a\") # case-sensitive only matches one\n\n[1] \"The quick brown fox jumps over a lazy dog.\"\n\n\n\nstr_replace_all(example, \"[Tt]he\", \"a\") # # will match either t or T; could also make \"a\" conditional on capitalization of t\n\n[1] \"a quick brown fox jumps over a lazy dog.\"\n\n\n\nstr_replace_all(example, \"[Tt]he\", \"a\") # first match only\n\n[1] \"a quick brown fox jumps over a lazy dog.\"\n\n\n\n# More examples\nexample2 &lt;- \"Two roads diverged in a yellow wood, / And sorry I could not travel both / And be one traveler, long I stood / And looked down one as far as I could\"\nexample3 &lt;- \"This is a test\"\n\n# Store the examples in 1 place\nexamples &lt;- c(example, example2, example3)\n\n\npat &lt;- \"[^aeiouAEIOU ]{3}\" # Regular expression for three straight consonants. Note that I've excluded spaces as well\n\nstr_detect(examples, pat) # TRUE/FALSE if it detects pattern\n\n[1]  TRUE  TRUE FALSE\n\n\n\nstr_subset(examples, pat) # Pulls out those that detects pattern\n\n[1] \"The quick brown fox jumps over the lazy dog.\"                                                                                                        \n[2] \"Two roads diverged in a yellow wood, / And sorry I could not travel both / And be one traveler, long I stood / And looked down one as far as I could\"\n\n\n\npat2 &lt;- \"[^aeiouAEIOU ][aeiouAEIOU]{2}[^aeiouAEIOU ]{1}\" # consonant followed by two vowels followed by a consonant\n\nstr_extract(example2, pat2) # extract first match\n\n[1] \"road\"\n\n\n\nstr_extract_all(example2, pat2, simplify = TRUE) # extract all matches\n\n     [,1]   [,2]   [,3]   [,4]   [,5]   [,6]  \n[1,] \"road\" \"wood\" \"coul\" \"tood\" \"look\" \"coul\"\n\n\nTUTORIAL\nTry out this interactive tutorial. Note that neither the tutorial nor regular expressions more generally are specific to R, but it still illustrates the main ideas of regular expressions.",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Strings</span>"
    ]
  },
  {
    "objectID": "ica/ica-strings.html#solutions",
    "href": "ica/ica-strings.html#solutions",
    "title": "\n22  Strings\n",
    "section": "\n22.4 Solutions",
    "text": "22.4 Solutions\n\nClick for Solutions\n\n22.4.1 Example 1\n\n# Define a new variable \"num\" that adds up the number of characters in the area label\nclasses |&gt; \n  mutate(num = str_length(area))\n\n     sem    area      enroll       instructor num\n1 SP2023 History 30 - people  Ernesto Capello   7\n2 FA2023    Math 20 - people Lori Ziegelmeier   4\n3 SP2024  Anthro 25 - people  Arjun Guneratne   6\n\n# Change the areas to \"history\", \"math\", \"anthro\"\nclasses |&gt; \n  mutate(area = str_to_lower(area))\n\n     sem    area      enroll       instructor\n1 SP2023 history 30 - people  Ernesto Capello\n2 FA2023    math 20 - people Lori Ziegelmeier\n3 SP2024  anthro 25 - people  Arjun Guneratne\n\n# Create a variable that id's which courses were taught in spring \nclasses |&gt; \n  mutate(spring = str_detect(sem, \"SP\"))\n\n     sem    area      enroll       instructor spring\n1 SP2023 History 30 - people  Ernesto Capello   TRUE\n2 FA2023    Math 20 - people Lori Ziegelmeier  FALSE\n3 SP2024  Anthro 25 - people  Arjun Guneratne   TRUE\n\n# Change the semester labels to \"fall2023\", \"spring2024\", \"spring2023\"\nclasses |&gt; \n  mutate(sem = str_replace(sem, \"SP\", \"spring\")) |&gt; \n  mutate(sem = str_replace(sem, \"FA\", \"fall\"))\n\n         sem    area      enroll       instructor\n1 spring2023 History 30 - people  Ernesto Capello\n2   fall2023    Math 20 - people Lori Ziegelmeier\n3 spring2024  Anthro 25 - people  Arjun Guneratne\n\n# In the enroll variable, change all e's to 3's (just because?)\nclasses |&gt; \n  mutate(enroll = str_replace_all(enroll, \"e\", \"3\"))\n\n     sem    area      enroll       instructor\n1 SP2023 History 30 - p3opl3  Ernesto Capello\n2 FA2023    Math 20 - p3opl3 Lori Ziegelmeier\n3 SP2024  Anthro 25 - p3opl3  Arjun Guneratne\n\n# Use sem to create 2 new variables, one with only the semester (SP/FA) and 1 with the year\nclasses |&gt; \n  mutate(semester = str_sub(sem, 1, 2),\n         year = str_sub(sem, 3, 6))\n\n     sem    area      enroll       instructor semester year\n1 SP2023 History 30 - people  Ernesto Capello       SP 2023\n2 FA2023    Math 20 - people Lori Ziegelmeier       FA 2023\n3 SP2024  Anthro 25 - people  Arjun Guneratne       SP 2024\n\n\n\n22.4.2 Example 2\n\n# How can we do this after mutating?\nclasses |&gt; \n  mutate(spring = str_detect(sem, \"SP\")) |&gt; \n  filter(spring == TRUE)\n\n     sem    area      enroll      instructor spring\n1 SP2023 History 30 - people Ernesto Capello   TRUE\n2 SP2024  Anthro 25 - people Arjun Guneratne   TRUE\n\n\nExercise 1: Popular time slots\n\n# Construct a table that indicates the number of classes offered in each day/time slot\n# Print only the 6 most popular time slots\ncourses |&gt; \n  count(days, time) |&gt; \n  arrange(desc(n)) |&gt; \n  head()\n\n   days             time  n\n1 M W F 10:50 - 11:50 am 76\n2  T R   9:40 - 11:10 am 71\n3 M W F  9:40 - 10:40 am 68\n4 M W F   1:10 - 2:10 pm 66\n5  T R    3:00 - 4:30 pm 62\n6  T R    1:20 - 2:50 pm 59\n\n\nExercise 2: Prep the data\n\ncourses_clean &lt;- courses |&gt; \n  separate(avail_max, c(\"avail\", \"max\"), sep = \" / \") |&gt; \n  mutate(enroll = as.numeric(max) - as.numeric(avail)) |&gt; \n  separate(number, c(\"dept\", \"number\", \"section\"))\n  \nhead(courses_clean)\n\n  dept number section   crn                                                name\n1 AMST    112      01 10318         Introduction to African American Literature\n2 AMST    194      01 10073              Introduction to Asian American Studies\n3 AMST    194      F1 10072 What’s After White Empire - And Is It Already Here?\n4 AMST    203      01 10646 Politics and Inequality: The American Welfare State\n5 AMST    205      01 10842                         Trans Theories and Politics\n6 AMST    209      01 10474                   Civil Rights in the United States\n   days            time      room             instructor avail max enroll\n1 M W F 9:40 - 10:40 am  MAIN 009       Daylanne English     3  20     17\n2 M W F  1:10 - 2:10 pm MUSIC 219          Jake Nagasawa    -4  16     20\n3  T R   3:00 - 4:30 pm   HUM 214 Karin Aguilar-San Juan     0  14     14\n4 M W F 9:40 - 10:40 am  CARN 305          Lesley Lavery     3  25     22\n5  T R   3:00 - 4:30 pm  MAIN 009              Myrl Beam    -2  20     22\n6   W   7:00 - 10:00 pm  MAIN 010         Walter Greason    -1  15     16\n\n\nExercise 3: Courses offered by department\n\n# Identify the 6 departments that offered the most sections\ncourses_clean |&gt; \n  count(dept) |&gt; \n  arrange(desc(n)) |&gt; \n  head()\n\n  dept  n\n1 SPAN 45\n2 BIOL 44\n3 ENVI 38\n4 PSYC 37\n5 CHEM 33\n6 COMP 31\n\n# Identify the 6 departments with the longest average course titles\ncourses_clean |&gt; \n  mutate(length = str_length(name)) |&gt; \n  group_by(dept) |&gt; \n  summarize(avg_length = mean(length)) |&gt; \n  arrange(desc(avg_length)) |&gt; \n  head()\n\n# A tibble: 6 × 2\n  dept  avg_length\n  &lt;chr&gt;      &lt;dbl&gt;\n1 WGSS        46.3\n2 INTL        41.4\n3 EDUC        39.4\n4 MCST        39.4\n5 POLI        37.4\n6 AMST        37.3\n\n\nExercise 4: STAT courses\nPart a\n\ncourses_clean |&gt; \n  filter(str_detect(instructor, \"Alicia Johnson\")) \n\n  dept number section   crn                         name  days            time\n1 STAT    253      01 10806 Statistical Machine Learning  T R  9:40 - 11:10 am\n2 STAT    253      02 10807 Statistical Machine Learning  T R   1:20 - 2:50 pm\n3 STAT    253      03 10808 Statistical Machine Learning  T R   3:00 - 4:30 pm\n        room     instructor avail max enroll\n1 THEATR 206 Alicia Johnson    -3  20     23\n2 THEATR 206 Alicia Johnson    -3  20     23\n3 THEATR 206 Alicia Johnson     2  20     18\n\n\nPart b\n\nstat &lt;- courses_clean |&gt; \n  filter(dept == \"STAT\") |&gt; \n  mutate(name = str_replace(name, \"Introduction to \", \"\")) |&gt;\n  mutate(name = str_replace(name, \"Statistical\", \"Stat\")) |&gt; \n  mutate(start_time = str_sub(time, 1, 5)) |&gt; \n  select(number, name, start_time, enroll)\n\nstat\n\n   number                      name start_time enroll\n1     112              Data Science      3:00      27\n2     112              Data Science      9:40      21\n3     112              Data Science      1:20      25\n4     125              Epidemiology      12:00     26\n5     155             Stat Modeling      1:10      32\n6     155             Stat Modeling      9:40      24\n7     155             Stat Modeling      10:50     26\n8     155             Stat Modeling      3:30      25\n9     155             Stat Modeling      1:20      30\n10    155             Stat Modeling      3:00      27\n11    212 Intermediate Data Science      9:40      11\n12    212 Intermediate Data Science      1:20      11\n13    253     Stat Machine Learning      9:40      23\n14    253     Stat Machine Learning      1:20      23\n15    253     Stat Machine Learning      3:00      18\n16    354               Probability      3:00      22\n17    452           Correlated Data      9:40       7\n18    452           Correlated Data      1:20       8\n19    456  Projects in Data Science      9:40      11\n\ndim(stat)\n\n[1] 19  4\n\n\nExercise 5: More cleaning\n\nenrollments &lt;- courses_clean |&gt; \n  filter(dept != \"PE\", dept != \"INTD\") |&gt; \n  filter(!(dept == \"MUSI\" & as.numeric(number) &lt; 100)) |&gt; \n  filter(!(dept == \"THDA\" & as.numeric(number) &lt; 100)) |&gt; \n  filter(!str_detect(section, \"L\"))\n  \nhead(enrollments)\n\n  dept number section   crn                                                name\n1 AMST    112      01 10318         Introduction to African American Literature\n2 AMST    194      01 10073              Introduction to Asian American Studies\n3 AMST    194      F1 10072 What’s After White Empire - And Is It Already Here?\n4 AMST    203      01 10646 Politics and Inequality: The American Welfare State\n5 AMST    205      01 10842                         Trans Theories and Politics\n6 AMST    209      01 10474                   Civil Rights in the United States\n   days            time      room             instructor avail max enroll\n1 M W F 9:40 - 10:40 am  MAIN 009       Daylanne English     3  20     17\n2 M W F  1:10 - 2:10 pm MUSIC 219          Jake Nagasawa    -4  16     20\n3  T R   3:00 - 4:30 pm   HUM 214 Karin Aguilar-San Juan     0  14     14\n4 M W F 9:40 - 10:40 am  CARN 305          Lesley Lavery     3  25     22\n5  T R   3:00 - 4:30 pm  MAIN 009              Myrl Beam    -2  20     22\n6   W   7:00 - 10:00 pm  MAIN 010         Walter Greason    -1  15     16\n\n\nOptional extra practice\n\n# Make a bar plot showing the number of night courses by day of the week.\ncourses_clean |&gt; \n  filter(str_detect(time, \"7:00\")) |&gt; \n  ggplot(aes(x = days)) + \n    geom_bar()",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Strings</span>"
    ]
  },
  {
    "objectID": "ica/ica-data import.html",
    "href": "ica/ica-data import.html",
    "title": "\n23  Data Import\n",
    "section": "",
    "text": "23.1 Review\nWHERE ARE WE?\nWe’ve thus far focused on data preparation and visualization:\nWhat’s coming up?",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Data Import</span>"
    ]
  },
  {
    "objectID": "ica/ica-data import.html#review",
    "href": "ica/ica-data import.html#review",
    "title": "\n23  Data Import\n",
    "section": "",
    "text": "In the last few weeks we’ll focus on data storytelling through the completion of a course project.\nThis week we’ll address the other gaps in the workflow: data collection and analysis. We’ll do so in the context of starting a data project…",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Data Import</span>"
    ]
  },
  {
    "objectID": "ica/ica-data import.html#data-import",
    "href": "ica/ica-data import.html#data-import",
    "title": "\n23  Data Import\n",
    "section": "\n23.2 Data Import",
    "text": "23.2 Data Import\nStarting Data Project\nAny data science project consists of two phases:\n\n\nData Collection\nA data project starts with data! Thus far, you’ve either been given data, or used TidyTuesday data. In this unit:\n\nWe WILL explore how to find data, save data, import this data into RStudio, and do some preliminary data cleaning.\nWe will NOT discuss how to collect data from scratch (e.g. via experiment, observational study, or survey).\n\n\n\nData Analysis\nOnce we have data, we need to do some analysis. In this unit…\n\nWe WILL bring together our wrangling & visualization tools to discuss exploratory data analysis (EDA). EDA is the process of getting to know our data, obtaining insights from data, and using these insights to formulate, refine, and explore research questions.\nWe will NOT explore other types of analysis, such as modeling & prediction–if interested, take STAT 155 and 253 to learn more about these topics.\n\n\n\nNOTE: These skills are best learned through practice. We’ll just scratch the surface here.\nFile Formats\nBefore exploring how to find, store, import, check, and clean data, it’s important to recognize that data can be stored in various formats. We’ve been working with .csv files. In the background, these have “comma-separated values” (csv):\n\nBut there are many other common file types. For example, the following are compatible with R:\n\nExcel files: .xls, .xlsx\n\nR “data serialization” files: .rds\n\nfiles with tab-separated values: .tsv\n\n\n\nSTEP 1: Find Data\nCheck the Datasets page for information about how to find a dataset that fits your needs.\nSTEP 2: Save Data Locally\nUnless we’re just doing a quick, one-off data analysis, it’s important to store a local copy of a data file, i.e. save the data file to our own machine.\nMainly, we shouldn’t rely on another person / institution to store a data file online, in the same place, forever!\n\n\n\n\n\n\nRecommendations when Saving Data\n\n\n\nWhen saving your data, make sure they are\n\nin a nice format, eg, a csv file format\nwhere you’ll be able to find it again\nideally, within a folder that’s dedicated to the related project / assignment\nalongside the qmd file(s) where you’ll record your analysis of the data\n\n\n\nSTEP 3: Import Data to RStudio\nOnce we have a local copy of our data file, we need to get it into RStudio! This process depends on 2 things: (1) the file type (e.g. .csv); and (2) the file location, i.e. where it’s stored on your computer.\n1. FILE TYPE\nThe file type indicates which function we’ll need to import it. The table below lists some common import functions and when to use them.\n\n\n\n\n\n\nFunction\nData file type\n\n\n\nread_csv()\n.csv - you can save Excel files and Google Sheets as .csv\n\n\nread_delim()\nother delimited formats (tab, space, etc.)\n\n\nread_sheet()\nGoogle Sheet\n\n\nst_read()\nspatial data shapefile\n\n\n\nNOTE: In comparison to read.csv, read_csv is faster when importing large data files and can more easily parse complicated datasets, eg, with dates, times, percentages.\n2. FILE LOCATION\nTo import the data, we apply the appropriate function from above to the file path.\nA file path is an address to where the file is stored on our computer or the web.\nConsider “1600 Grand Ave, St. Paul, MN 55105”. Think about how different parts of the address give increasingly more specific information about the location. “St. Paul, MN 55105” tells us the city and smaller region within the city, “Grand Ave” tells us the street, and “1600” tells us the specific location on the street.\nIn the example below, the file path is absolute where it tells us the location giving more and more specific information as you read it from left to right.\n\n“~”, on an Apple computer, tells you that you are looking in the user’s home directory.\n“Desktop” tells you to go to the Desktop within that home directory.\n“112” tells you that you are looking in the 112 folder on the Desktop.\n“data” tells you to next go in the data folder in the 112 folder.\n“my_data.csv” tells you that you are looking for a file called my_data.csv location within the data folder.\n\n\nlibrary(tidyverse)\nmy_data &lt;- read_csv(\"~/Desktop/112/data/my_data.csv\")\n\nAbsolute file paths should only be used when reading files hosted on the web. Otherwise, relative file paths should be used. Relative file paths as the name suggest is relative to the file where the data file is read.\n\n# assume the code containing this script is under a folder called /src which\n# is at the same level of the /data folder\n\nlibrary(tidyverse)\nmy_data &lt;- read_csv(\"../data/my_data.csv\")\n\nSTEP 4: Check & Clean Data\nOnce the data is loaded, ask yourself a few questions:\nWhat’s the structure of the data?\n\nUse str() to learn about the numbers of variables and observations as well as the classes or types of variables (date, location, string, factor, number, boolean, etc.)\nUse head() to view the top of the data table\nUse View() to view the data in a spreadsheet-like viewer, use this command in the Console but don’t include it in your qmd files since it will prevent your project from rendering.\n\nIs there anything goofy that we need to clean before we can analyze the data?\n\nIs it in a tidy format?\nHow many rows are there? What does the row mean? What is an observation?\nIs there consistent formatting for categorical variables?\nIs there missing data that needs to be addressed?\nSTEP 5: Understand Data\nStart by understanding the data that is available to you. If you have a codebook, you have struck gold! If not (the more common case), you’ll need to do some detective work that often involves talking to people.\nAt this stage, ask yourself:\n\nWhere does my data come from? How was it collected?1\n\nIs there a codebook? If not, how can I learn more about it?\nAre there people I can reach out to who have experience with this data?",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Data Import</span>"
    ]
  },
  {
    "objectID": "ica/ica-data import.html#exercises",
    "href": "ica/ica-data import.html#exercises",
    "title": "\n23  Data Import\n",
    "section": "\n23.3 Exercises",
    "text": "23.3 Exercises\nSuppose our goal is to work with data on movie reviews, and that we’ve already gone through the work to find a dataset. The imdb_5000_messy.csv file is posted on Moodle. Let’s work with it!\nExercise 1: Save Data Locally\nPart a\nOn your laptop:\n\nDownload the “imdb_5000_messy.csv” file from Moodle\nMove it to the data folder in your portfolio repository\nPart b\nHot tip: After saving your data file, it’s important to record appropriate citations and info in either a new qmd (eg: “imdb_5000_messy_README.qmd”) or in the qmd where you’ll analyze the data. These citations should include:\n\nthe data source, i.e. where you found the data\nthe data creator, i.e. who / what group collected the original data\npossibly a data codebook, i.e. descriptions of the data variables\n\nTo this end, check out where we originally got our IMDB data:\nhttps://www.kaggle.com/datasets/tmdb/tmdb-movie-metadata\nAfter visiting that website, take some quick notes here on the data source and creator.\nExercise 2: Import Data to RStudio\nNow that we have a local copy of our data file, let’s get it into RStudio! Remember that this process depends on 2 things: the file type and location. Since our file type is a csv, we can import it using read_csv(). But we have to supply the file location through a file path. To this end, we can either use an absolute file path or a relative file path.\nPart a\nAn absolute file path describes the location of a file starting from the root or home directory. How we refer to the user root directory depends upon your machine:\n\nOn a Mac: ~\n\nOn Windows: typically C:\\\n\n\nThen the complete file path to the IMDB data file in the data folder, depending on your machine an where you created your portfolio project, can be:\n\nOn a Mac: ~/Desktop/portfolio/data/imdb_5000_messy.csv\n\nOn Windows: C:\\Desktop\\portfolio\\data\\imdb_5000_messy.csv or C:\\\\Desktop\\\\portfolio\\\\data\\\\imdb_5000_messy.csv\n\n\nPutting this together, use read_csv() with the appropriate absolute file path to import your data into RStudio. Save this as imdb_messy.\n\nlibrary(tidyverse)\nimdb_messy &lt;- read_csv(\"/Users/piipan/Documents/GitHub/comp112/portfolio-PiiPan/data/imdb_5000_messy.csv\")\n\nPart b\nAbsolute file paths can get really long, depending upon our number of sub-folders, and they should not be used when sharing code with other and instead relative file paths should be used. A relative file path describes the location of a file from the current “working directory”, i.e. where RStudio would currently look for on your computer. Check what your working directory is inside this qmd:\n\n# This should be the folder where you stored this qmd!\ngetwd()\n\n[1] \"/Users/piipan/Documents/GitHub/comp112/portfolio-PiiPan/ica\"\n\n\nNext, check what the working directory is for the console by typing getwd() in the console. This is probably different, meaning that the relative file paths that will work in your qmd won’t work in the console! You can either exclusively work inside your qmd, or change the working directory in your console, by navigating to the following in the upper toolbar: Session &gt; Set Working Directory &gt; To Source File location.\nPart c\nAs a good practice, we created a data folder and saved our data file (imdb_5000_messy.csv) into.\nSince our .qmd analysis and .csv data live in the same project, we don’t have to write out absolute file paths that go all the way to the root directory. We can use relative file paths that start from where our code file exists to where the data file exist:\n\nOn a Mac: ../data/imdb_5000_messy.csv\n\nOn Windows: ..\\data\\imdb_5000_messy.csv or ..\\\\data\\\\imdb_5000_messy.csv\n\n\nNOTE: .. means go up one level in the file hierarchy, ie, go to the parent folder/directory.\nPutting this together, use read_csv() with the appropriate relative file path to import your data into RStudio. Save this as imdb_temp (temp for “temporary”). Convince yourself that this worked, i.e. you get the same dataset as imdb_messy.\n\nimdb_temp &lt;- read_csv(\"../data/imdb_5000_messy.csv\")\n\n\n\n\n\n\n\nFile Paths\n\n\n\nAbsolute file paths should be used when referring to files hosed on the web, eg, https://mac-stat.github.io/data/kiva_partners2.csv. In all other instances, relative file paths should be used.\n\n\nPart d: OPTIONAL\nSometimes, we don’t want to import the entire dataset. For example, we might want to…\n\nskips some rows, eg, if they’re just “filler”\nonly import the first “n” rows, eg, if the dataset is really large\nonly import a random subset of “n” rows, eg, if the dataset is really large\n\nThe “data import cheat sheet” at the top of this qmd, or Google, are handy resources here. As one example…\n\n# Try importing only the first 5 rows\n# read_csv(\"___\", n_max = ___)\n\n\n\n\n\n\n\nCommenting/Uncommenting Code\n\n\n\nTo comment/uncomment several lines of code at once, highlight them then click ctrl/cmd+shift+c.\n\n\nExercise 3: Check Data\nAfter importing new data into RStudio, you MUST do some quick checks of the data. Here are two first steps that are especially useful.\nPart a\nOpen imdb_messy in the spreadsheet-like viewer by typing View(imdb_messy) in the console. Sort this “spreadsheet” by different variables by clicking on the arrows next to the variable names. Do you notice anything unexpected?\nPart b\nDo a quick summary() of each variable in the dataset. One way to do this is below:\n\nimdb_messy |&gt;\n  mutate(across(where(is.character), as.factor)) |&gt;  # convert characters to factors in order to summarize\n  summary()\n\n      ...1                  color               director_name \n Min.   :   1   B&W            :  10   Steven Spielberg:  26  \n 1st Qu.:1262   Black and White: 199   Woody Allen     :  22  \n Median :2522   color          :  30   Clint Eastwood  :  20  \n Mean   :2522   Color          :4755   Martin Scorsese :  20  \n 3rd Qu.:3782   COLOR          :  30   Ridley Scott    :  17  \n Max.   :5043   NA's           :  19   (Other)         :4834  \n                                       NA's            : 104  \n num_critic_for_reviews    duration     director_facebook_likes\n Min.   :  1.0          Min.   :  7.0   Min.   :    0.0        \n 1st Qu.: 50.0          1st Qu.: 93.0   1st Qu.:    7.0        \n Median :110.0          Median :103.0   Median :   49.0        \n Mean   :140.2          Mean   :107.2   Mean   :  686.5        \n 3rd Qu.:195.0          3rd Qu.:118.0   3rd Qu.:  194.5        \n Max.   :813.0          Max.   :511.0   Max.   :23000.0        \n NA's   :50             NA's   :15      NA's   :104            \n actor_3_facebook_likes          actor_2_name  actor_1_facebook_likes\n Min.   :    0.0        Morgan Freeman :  20   Min.   :     0        \n 1st Qu.:  133.0        Charlize Theron:  15   1st Qu.:   614        \n Median :  371.5        Brad Pitt      :  14   Median :   988        \n Mean   :  645.0        James Franco   :  11   Mean   :  6560        \n 3rd Qu.:  636.0        Meryl Streep   :  11   3rd Qu.: 11000        \n Max.   :23000.0        (Other)        :4959   Max.   :640000        \n NA's   :23             NA's           :  13   NA's   :7             \n     gross                            genres             actor_1_name \n Min.   :      162   Drama               : 236   Robert De Niro:  49  \n 1st Qu.:  5340988   Comedy              : 209   Johnny Depp   :  41  \n Median : 25517500   Comedy|Drama        : 191   Nicolas Cage  :  33  \n Mean   : 48468408   Comedy|Drama|Romance: 187   J.K. Simmons  :  31  \n 3rd Qu.: 62309438   Comedy|Romance      : 158   Bruce Willis  :  30  \n Max.   :760505847   Drama|Romance       : 152   (Other)       :4852  \n NA's   :884         (Other)             :3910   NA's          :   7  \n                    movie_title   num_voted_users   cast_total_facebook_likes\n Ben-Hur                  :   3   Min.   :      5   Min.   :     0           \n Halloween                :   3   1st Qu.:   8594   1st Qu.:  1411           \n Home                     :   3   Median :  34359   Median :  3090           \n King Kong                :   3   Mean   :  83668   Mean   :  9699           \n Pan                      :   3   3rd Qu.:  96309   3rd Qu.: 13756           \n The Fast and the Furious :   3   Max.   :1689764   Max.   :656730           \n (Other)                  :5025                                              \n         actor_3_name  facenumber_in_poster\n Ben Mendelsohn:   8   Min.   : 0.000      \n John Heard    :   8   1st Qu.: 0.000      \n Steve Coogan  :   8   Median : 1.000      \n Anne Hathaway :   7   Mean   : 1.371      \n Jon Gries     :   7   3rd Qu.: 2.000      \n (Other)       :4982   Max.   :43.000      \n NA's          :  23   NA's   :13          \n                                                                           plot_keywords \n based on novel                                                                   :   4  \n 1940s|child hero|fantasy world|orphan|reference to peter pan                     :   3  \n alien friendship|alien invasion|australia|flying car|mother daughter relationship:   3  \n animal name in title|ape abducts a woman|gorilla|island|king kong                :   3  \n assistant|experiment|frankenstein|medical student|scientist                      :   3  \n (Other)                                                                          :4874  \n NA's                                                                             : 153  \n                                             movie_imdb_link\n http://www.imdb.com/title/tt0077651/?ref_=fn_tt_tt_1:   3  \n http://www.imdb.com/title/tt0232500/?ref_=fn_tt_tt_1:   3  \n http://www.imdb.com/title/tt0360717/?ref_=fn_tt_tt_1:   3  \n http://www.imdb.com/title/tt1976009/?ref_=fn_tt_tt_1:   3  \n http://www.imdb.com/title/tt2224026/?ref_=fn_tt_tt_1:   3  \n http://www.imdb.com/title/tt2638144/?ref_=fn_tt_tt_1:   3  \n (Other)                                             :5025  \n num_user_for_reviews     language       country       content_rating\n Min.   :   1.0       English :4704   USA    :3807   R        :2118  \n 1st Qu.:  65.0       French  :  73   UK     : 448   PG-13    :1461  \n Median : 156.0       Spanish :  40   France : 154   PG       : 701  \n Mean   : 272.8       Hindi   :  28   Canada : 126   Not Rated: 116  \n 3rd Qu.: 326.0       Mandarin:  26   Germany:  97   G        : 112  \n Max.   :5060.0       (Other) : 160   (Other): 406   (Other)  : 232  \n NA's   :21           NA's    :  12   NA's   :   5   NA's     : 303  \n     budget            title_year   actor_2_facebook_likes   imdb_score   \n Min.   :2.180e+02   Min.   :1916   Min.   :     0         Min.   :1.600  \n 1st Qu.:6.000e+06   1st Qu.:1999   1st Qu.:   281         1st Qu.:5.800  \n Median :2.000e+07   Median :2005   Median :   595         Median :6.600  \n Mean   :3.975e+07   Mean   :2002   Mean   :  1652         Mean   :6.442  \n 3rd Qu.:4.500e+07   3rd Qu.:2011   3rd Qu.:   918         3rd Qu.:7.200  \n Max.   :1.222e+10   Max.   :2016   Max.   :137000         Max.   :9.500  \n NA's   :492         NA's   :108    NA's   :13                            \n  aspect_ratio   movie_facebook_likes\n Min.   : 1.18   Min.   :     0      \n 1st Qu.: 1.85   1st Qu.:     0      \n Median : 2.35   Median :   166      \n Mean   : 2.22   Mean   :  7526      \n 3rd Qu.: 2.35   3rd Qu.:  3000      \n Max.   :16.00   Max.   :349000      \n NA's   :329                         \n\n\nFollow-up:\n\nWhat type of info is provided on quantitative variables?\nWhat type of info is provided on categorical variables?\nWhat stands out to you in these summaries? Is there anything you’d need to clean before using this data?\nExercise 4: Clean Data: Factor Variables 1\nIf you didn’t already in Exercise 3, check out the color variable in the imdb_messy dataset.\n\nWhat’s goofy about this / what do we need to fix?\nMore specifically, what different categories does the color variable take, and how many movies fall into each of these categories?\n\n\nimdb_messy |&gt;\n  count(color)\n\n# A tibble: 6 × 2\n  color               n\n  &lt;chr&gt;           &lt;int&gt;\n1 B&W                10\n2 Black and White   199\n3 COLOR              30\n4 Color            4755\n5 color              30\n6 &lt;NA&gt;               19\n\n\nExercise 5: Clean Data: Factor Variables 2\nWhen working with categorical variables like color, the categories must be “clean”, i.e. consistent and in the correct format. Let’s make that happen.\nPart a\nWe could open the .csv file in, say, Excel or Google sheets, clean up the color variable, save a clean copy, and then reimport that into RStudio. BUT that would be the wrong thing to do. Why is it important to use R code, which we then save inside this qmd, to clean our data?\nPart b\nLet’s use R code to change the color variable so that it appropriately combines the various categories into only 2: Color and Black_White. We’ve learned a couple sets of string-related tools that could be handy here. First, starting with the imdb_messy data, change the color variable using one of the functions we learned in the Factors lesson.\nfct_relevel(), fct_recode(), fct_reorder()\nStore your results in imdb_temp (don’t overwrite imdb_messy). To check your work, print out a count() table of the color variable in imdb_temp.\n\nimdb_temp &lt;- imdb_messy |&gt;\n  mutate(color = fct_recode(\n    color,\n    \"Color\" = \"Color\",\n    \"Black_White\" = \"Black and White\",\n    \"Black_White\" = \"Black and white\",\n    \"Black_White\" = \" Black and White\",\n    \"Black_White\" = \"black and white\"\n  ))\n\nimdb_temp |&gt; count(color)\n\n# A tibble: 6 × 2\n  color           n\n  &lt;fct&gt;       &lt;int&gt;\n1 B&W            10\n2 Black_White   199\n3 color          30\n4 Color        4755\n5 COLOR          30\n6 &lt;NA&gt;           19\n\n\nPart c\nRepeat Part b using one of our string functions from the String lesson:\nstr_replace(), str_replace_all(), str_to_lower(), str_sub(), str_length(), str_detect()\n\nimdb_temp2 &lt;- imdb_messy |&gt;\n  mutate(color = str_replace_all(\n    str_to_lower(str_trim(color)),\n    \"color\", \"Color\"\n  )) |&gt;\n  mutate(color = if_else(str_detect(color, \"black\"), \"Black_White\", color))\n\nimdb_temp2 |&gt; count(color)\n\n# A tibble: 4 × 2\n  color           n\n  &lt;chr&gt;       &lt;int&gt;\n1 Black_White   199\n2 Color        4815\n3 b&w            10\n4 &lt;NA&gt;           19\n\n\nExercise 6: Clean Data: Missing Data 1\nThroughout these exercises, you’ve probably noticed that there’s a bunch of missing data. This is encoded as NA (not available) in R. There are a few questions to address about missing data:\n\n\nHow many values are missing data? What’s the volume of the missingness?\n\nWhy are some values missing?\n\nWhat should we do about the missing values?\n\nLet’s consider the first 2 questions in this exercise.\nPart a\nAs a first step, let’s simply understand the volume of NAs. Specifically:\n\n# Count the total number of rows in imdb_messy\nnrow(imdb_messy)\n\n[1] 5043\n\n# Then count the number of NAs in each column\ncolSums(is.na(imdb_messy))\n\n                     ...1                     color             director_name \n                        0                        19                       104 \n   num_critic_for_reviews                  duration   director_facebook_likes \n                       50                        15                       104 \n   actor_3_facebook_likes              actor_2_name    actor_1_facebook_likes \n                       23                        13                         7 \n                    gross                    genres              actor_1_name \n                      884                         0                         7 \n              movie_title           num_voted_users cast_total_facebook_likes \n                        0                         0                         0 \n             actor_3_name      facenumber_in_poster             plot_keywords \n                       23                        13                       153 \n          movie_imdb_link      num_user_for_reviews                  language \n                        0                        21                        12 \n                  country            content_rating                    budget \n                        5                       303                       492 \n               title_year    actor_2_facebook_likes                imdb_score \n                      108                        13                         0 \n             aspect_ratio      movie_facebook_likes \n                      329                         0 \n\n# Then count the number of NAs in a specific column\nimdb_messy |&gt; \n  summarize(across(everything(), ~sum(is.na(.))))\n\n# A tibble: 1 × 29\n   ...1 color director_name num_critic_for_reviews duration\n  &lt;int&gt; &lt;int&gt;         &lt;int&gt;                  &lt;int&gt;    &lt;int&gt;\n1     0    19           104                     50       15\n# ℹ 24 more variables: director_facebook_likes &lt;int&gt;,\n#   actor_3_facebook_likes &lt;int&gt;, actor_2_name &lt;int&gt;,\n#   actor_1_facebook_likes &lt;int&gt;, gross &lt;int&gt;, genres &lt;int&gt;,\n#   actor_1_name &lt;int&gt;, movie_title &lt;int&gt;, num_voted_users &lt;int&gt;,\n#   cast_total_facebook_likes &lt;int&gt;, actor_3_name &lt;int&gt;,\n#   facenumber_in_poster &lt;int&gt;, plot_keywords &lt;int&gt;, movie_imdb_link &lt;int&gt;,\n#   num_user_for_reviews &lt;int&gt;, language &lt;int&gt;, country &lt;int&gt;, …\n\n\nPart b\nAs a second step, let’s think about why some values are missing. Study the individual observations with NAs carefully. Why do you think they are missing? Are certain films more likely to have more NAs than others?\nPart c\nConsider a more specific example. Obtain a dataset of movies that are missing data on actor_1_facebook_likes. Then explain why you think there are NAs. HINT: is.na(___)\n\nimdb_messy |&gt;\n  filter(is.na(actor_1_facebook_likes))\n\n# A tibble: 7 × 29\n   ...1 color director_name     num_critic_for_reviews duration\n  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;                              &lt;dbl&gt;    &lt;dbl&gt;\n1  4503 Color Léa Pool                              23       97\n2  4520 Color Harry Gantz                           12      105\n3  4721 Color U. Roberto Romano                      3       80\n4  4838 Color Pan Nalin                             15      102\n5  4946 Color Amal Al-Agroobi                       NA       62\n6  4947 Color Andrew Berends                        12       90\n7  4991 Color Jem Cohen                             12      111\n# ℹ 24 more variables: director_facebook_likes &lt;dbl&gt;,\n#   actor_3_facebook_likes &lt;dbl&gt;, actor_2_name &lt;chr&gt;,\n#   actor_1_facebook_likes &lt;dbl&gt;, gross &lt;dbl&gt;, genres &lt;chr&gt;,\n#   actor_1_name &lt;chr&gt;, movie_title &lt;chr&gt;, num_voted_users &lt;dbl&gt;,\n#   cast_total_facebook_likes &lt;dbl&gt;, actor_3_name &lt;chr&gt;,\n#   facenumber_in_poster &lt;dbl&gt;, plot_keywords &lt;chr&gt;, movie_imdb_link &lt;chr&gt;,\n#   num_user_for_reviews &lt;dbl&gt;, language &lt;chr&gt;, country &lt;chr&gt;, …\n\n\nExercise 7: Clean Data: Missing Data 2\nNext, let’s think about what to do about the missing values. There is no perfect or universal approach here. Rather, we must think carefully about…\n\nWhy the values are missing?\nWhat we want to do with our data?\nWhat is the impact of removing or replacing missing data on our work / conclusions?\n\nPart a\nCalculate the average duration of a film. THINK: How can we deal with the NA’s?\n\nmean(imdb_messy$duration, na.rm = TRUE)\n\n[1] 107.2011\n\n\nFollow-up:\nHow are the NAs dealt with here? Did we have to create and save a new dataset in order to do this analysis?\nPart b\nTry out the drop_na() function:\n\nimdb_temp &lt;- drop_na(imdb_messy)\n\nFollow-up questions:\n\nWhat did drop_na() do? How many data points are left?\nIn what situations might this function be a good idea?\nIn what situations might this function be a bad idea?\nPart c\ndrop_na() removes data points that have any NA values, even if we don’t care about the variable(s) for which data is missing. This can result in losing a lot of data points that do have data on the variables we actually care about! For example, suppose we only want to explore the relationship between film duration and whether it’s in color. Check out a plot:\n\nggplot(imdb_messy, aes(x = duration, fill = color)) +\n  geom_density()\n\n\n\n\n\n\n\nFollow-up:\n\nCreate a new dataset with only and all movies that have complete info on duration and color. HINT: You could use !is.na(___) or drop_na() (differently than above)\nUse this new dataset to create a new and improved plot.\nHow many movies remain in your new dataset? Hence why this is better than using the dataset from part b?\n\n\nimdb_duration_color &lt;- imdb_messy |&gt;\n  filter(!is.na(duration), !is.na(color))\n\nnrow(imdb_duration_color)\n\n[1] 5010\n\nggplot(imdb_duration_color, aes(x = duration, fill = color)) +\n  geom_density()\n\n\n\n\n\n\n\nPart d\nIn some cases, missing data is more non-data than unknown data. For example, the films with NAs for actor_1_facebook_likes actually have 0 Facebook likes–they don’t even have actors! In these cases, we can replace the NAs with a 0. Use the replace_na() function to create a new dataset (imdb_temp) that replaces the NAs in actor_1_facebook_likes with 0. You’ll have to check out the help file for this function.\n\nimdb_temp &lt;- imdb_messy |&gt;\n  mutate(actor_1_facebook_likes = replace_na(actor_1_facebook_likes, 0))\n\nExercise 8: New Data + Projects\nLet’s practice the above ideas while also planting some seeds for the course project. Each group will pick and analyze their own dataset. The people you’re sitting with today aren’t necessarily your project groups! BUT do some brainstorming together:\n\nShare with each other: What are some personal hobbies or passions or things you’ve been thinking about or things you’d like to learn more about? Don’t think too hard about this! Just share what’s at the top of mind today.\nEach individual: Find a dataset online that’s related to one of the topics you shared in the above prompt.\nDiscuss what data you found with your group!\nLoad the data into RStudio, perform some basic checks, and perform some preliminary cleaning, as necessary.",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Data Import</span>"
    ]
  },
  {
    "objectID": "ica/ica-data import.html#footnotes",
    "href": "ica/ica-data import.html#footnotes",
    "title": "\n23  Data Import\n",
    "section": "",
    "text": "Particularly important questions about how it was collected include who: Is it a sample of a larger data set? If so, how the sampling was done? Randomly? All cases during a specific time frame? All data for a selected set of users?, when: Is this current data? Historical? What events may have had an impact?, what: What variables were measured? How was it measured? Self-reported through a questionnaire? Directly?, why: Who funded the data collection? What was the purposes? To whose benefit was the data collected? Answers to such questions strongly impact the conclusions you will be able to draw from the data.↩︎",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Data Import</span>"
    ]
  },
  {
    "objectID": "ica/ica-EDA.html",
    "href": "ica/ica-EDA.html",
    "title": "24  EDA",
    "section": "",
    "text": "24.1 Review\nWHERE ARE WE?!? Starting a data project\nThis final, short unit will help prepare us as we launch into course projects. In order to even start these projects, we need some sense of the following:",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>EDA</span>"
    ]
  },
  {
    "objectID": "ica/ica-EDA.html#review",
    "href": "ica/ica-EDA.html#review",
    "title": "24  EDA",
    "section": "",
    "text": "data import: how to find data, store data, load data into RStudio, and do some preliminary data checks & cleaning\nexploratory data analysis (EDA)",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>EDA</span>"
    ]
  },
  {
    "objectID": "ica/ica-EDA.html#eda",
    "href": "ica/ica-EDA.html#eda",
    "title": "24  EDA",
    "section": "24.2 EDA",
    "text": "24.2 EDA\n\nWhat is EDA?!\nEDA is a preliminary, exploratory, and iterative analysis of our data relative to our general research questions of interest.\n\n\n\nDifferece from Before\nHow is this different than what we’ve been doing? We’ve been focusing on various tools needed for various steps within an EDA. Now we’ll bring them all together in a more cohesive process.\n\n\nExample\nIn his book Exploratory Data Analysis with R, Dr. Roger D. Peng included an EDA case study about Changes in Fine Particle Air Pollution in the U.S.. Note that the link to the datasets used by Peng in the chapter is currently broken. Can you figure out the new location?1\n\n\nEDA Essentials\n\nStart small.\nWe often start with lots of data – some of it useful, some of it not. To start:\n\nFocus on just a small set of variables of interest.\nBreak down your research question into smaller pieces.\nObtain the most simple numerical & visual summaries that are relevant to your research questions.\n\nAsk questions.\nWe typically start a data analysis with at least some general research questions in mind. In obtaining numerical and graphical summaries that provide insight into these questions, we must ask:\n\nwhat questions do these summaries answer?\nwhat questions don’t these summaries answer?\nwhat’s surprising or interesting here?\nwhat follow-up questions do these summaries provoke?\n\nPlay! Be creative. Don’t lock yourself into a rigid idea of what should happen.\nRepeat.\nRepeat this iterative questioning and analysis process as necessary, letting our reflections on the previous questions inspire our next steps.",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>EDA</span>"
    ]
  },
  {
    "objectID": "ica/ica-EDA.html#exercises",
    "href": "ica/ica-EDA.html#exercises",
    "title": "24  EDA",
    "section": "24.3 Exercises",
    "text": "24.3 Exercises\nWork on Homework 7",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>EDA</span>"
    ]
  },
  {
    "objectID": "ica/ica-EDA.html#footnotes",
    "href": "ica/ica-EDA.html#footnotes",
    "title": "24  EDA",
    "section": "",
    "text": "It might have been moved to https://aqs.epa.gov/aqsweb/airdata/download_files.html↩︎",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>EDA</span>"
    ]
  }
]